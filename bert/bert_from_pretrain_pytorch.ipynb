{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawQuery(Dataset):\n",
    "    def __init__(self, raw_path='data'): \n",
    "        # read provided data\n",
    "        raw_training_data = pd.read_csv(os.path.join(raw_path, 'TD.csv'))\n",
    "        news_urls = pd.read_csv(os.path.join(raw_path, 'NC_1.csv'))\n",
    "        contents = pd.read_json(os.path.join(raw_path, 'url2content.json'), typ=pd.Series)\n",
    "        \n",
    "        # proccess data\n",
    "        merged_training = pd.merge(raw_training_data, news_urls, on=['News_Index'], how='left')\n",
    "        contents = contents.apply(lambda x: ' [CLS] ' + re.sub('[\\n，。、]+', ' [SEP] ', x))\n",
    "        \n",
    "        # construct dataset\n",
    "        self.queries = merged_training['Query']\n",
    "        self.contents = contents[merged_training['News_URL']]\n",
    "        self.targets = merged_training['Relevance']\n",
    "        \n",
    "        self.size = len(merged_training)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        padding_size = 512\n",
    "        tokenized_text = tokenizer.tokenize(self.contents[i])\n",
    "\n",
    "        if len(tokenized_text) > padding_size:\n",
    "            tokenized_text = tokenized_text[:padding_size]\n",
    "        else:\n",
    "            tokenized_text = tokenized_text + ['[PAD]'] * (padding_size - len(tokenized_text))\n",
    "\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "        segments_tensors = torch.zeros(padding_size).long()\n",
    "        tokens_tensor = torch.tensor(indexed_tokens)\n",
    "        \n",
    "        return self.queries[i], tokens_tensor, segments_tensors, self.targets[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "bert = BertModel.from_pretrained('bert-base-chinese')\n",
    "bert = bert.cuda()\n",
    "bert.eval();\n",
    "\n",
    "\n",
    "NumberCPU = multiprocessing.cpu_count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = RawQuery()\n",
    "loader = DataLoader(\n",
    "    dataset, batch_size=70, shuffle=False, num_workers=NumberCPU\n",
    ")\n",
    "\n",
    "encoded_text = []\n",
    "with torch.no_grad():\n",
    "    for q, t, s, y in loader:\n",
    "        _, encoded_batch = bert(t.cuda(), s.cuda())\n",
    "        encoded_text.append(encoded_batch.cpu().data.numpy())\n",
    "\n",
    "np.save('data/embedded/encoded_content.npy', np.concatenate(encoded_text, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_training_data = dataset.queries\n",
    "\n",
    "queries = list(set(raw_training_data))\n",
    "encoded_text = []\n",
    "with torch.no_grad():\n",
    "    for q in queries:\n",
    "        tokenized_text = tokenizer.tokenize(q)\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        segments_tensors = torch.zeros(len(indexed_tokens)).long().cuda()\n",
    "        tokens_tensor = torch.tensor([indexed_tokens]).cuda()\n",
    "        _, encoded = bert(tokens_tensor, segments_tensors)\n",
    "        encoded_text.append(encoded.cpu().data.numpy())\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Query'] = queries\n",
    "df['code'] = encoded_text\n",
    "encoded_query = pd.merge(raw_training_data, df, on=['Query'], how='left')['code']\n",
    "np.save('data/embedded/encoded_query.npy', np.concatenate(encoded_query, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_training_data = dataset.queries\n",
    "\n",
    "queries = pd.unique(raw_training_data)\n",
    "encoded_text = []\n",
    "with torch.no_grad():\n",
    "    for q in queries:\n",
    "        tokenized_text = tokenizer.tokenize(q)\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        segments_tensors = torch.zeros(len(indexed_tokens)).long().cuda()\n",
    "        tokens_tensor = torch.tensor([indexed_tokens]).cuda()\n",
    "        _, encoded = bert(tokens_tensor, segments_tensors)\n",
    "        encoded_text.append(encoded.cpu().data.numpy())\n",
    "\n",
    "np.save('data/embedded/ref_training_uniq_query.npy', queries)\n",
    "np.save('data/embedded/encoded_training_uniq_query.npy', np.concatenate(encoded_text, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsContentSet(Dataset):\n",
    "    def __init__(self, raw_path='data'): \n",
    "        # read provided data\n",
    "        news_urls = pd.read_csv(os.path.join(raw_path, 'NC_1.csv'))\n",
    "        contents = pd.read_json(os.path.join(raw_path, 'url2content.json'), typ=pd.Series)\n",
    "        \n",
    "        # proccess data\n",
    "        contents = contents[news_urls['News_URL']]\n",
    "        contents = contents.apply(lambda x: ' [CLS] ' + re.sub('[\\n，。、]+', ' [SEP] ', x))\n",
    "        \n",
    "        # construct dataset\n",
    "        self.contents = contents        \n",
    "        self.size = len(contents)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        padding_size = 512\n",
    "        tokenized_text = tokenizer.tokenize(self.contents[i])\n",
    "\n",
    "        if len(tokenized_text) > padding_size:\n",
    "            tokenized_text = tokenized_text[:padding_size]\n",
    "        else:\n",
    "            tokenized_text = tokenized_text + ['[PAD]'] * (padding_size - len(tokenized_text))\n",
    "\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "        segments_tensors = torch.zeros(padding_size).long()\n",
    "        tokens_tensor = torch.tensor(indexed_tokens)\n",
    "        \n",
    "        return tokens_tensor, segments_tensors\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = pd.read_csv(os.path.join('data', 'QS_1.csv'))['Query']\n",
    "encoded_text = []\n",
    "with torch.no_grad():\n",
    "    for q in queries:\n",
    "        tokenized_text = tokenizer.tokenize(q)\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        segments_tensors = torch.zeros(len(indexed_tokens)).long().cuda()\n",
    "        tokens_tensor = torch.tensor([indexed_tokens]).cuda()\n",
    "        _, encoded = bert(tokens_tensor, segments_tensors)\n",
    "        encoded_text.append(encoded.cpu().data.numpy())\n",
    "\n",
    "\n",
    "np.save('data/embedded/encoded_test_query.npy', np.concatenate(encoded_text, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    NewsContentSet(), batch_size=70, shuffle=False, num_workers=NumberCPU\n",
    ")\n",
    "\n",
    "encoded_text = []\n",
    "with torch.no_grad():\n",
    "    for t, s in loader:\n",
    "        _, encoded_batch = bert(t.cuda(), s.cuda())\n",
    "        encoded_text.append(encoded_batch.cpu().data.numpy())\n",
    "\n",
    "np.save('data/embedded/encoded_all_content.npy', np.concatenate(encoded_text, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "同等重視，讓獄政基本人權邁一大步。1031230(中央社)\n",
      "表示，這支重型狙擊步槍可有效對抗輕型裝甲車。(中時電子報)\n",
      "一份新的診斷報告，而是放下藍綠、尊重人權的決心。(旺報 )\n",
      "、台中市廖姓台商及屏東縣前國民黨縣議長等勢力的申請案。()\n",
      "，並一起向現場冉冉上升的國旗致敬。1040101(中央社)\n",
      "權威，不會提升台灣國際形象，只會深化政治對立，加重社會傷痕。\n",
      "程出訪歐洲前，還特別去看過陳水扁。1030124(中央社)\n",
      "府同意讓健康惡化的陳水扁保外就醫資料來源：《蘋果》採訪整理\n",
      "樣的做法，否則阿扁情況一定會更糟。1020419(中央社)\n",
      "縮社會運動陳情抗議空間，對台灣公民社會發展也將造成嚴重阻礙。\n",
      "期的規定，法務部必須依法行政，目前須讓扁在醫療專區繼續照顧。\n",
      "明蝦和紅蟳通通都有，地方親友還是滿心祝賀，這對新人百年好合。\n",
      "支持後，相信馬政府「頑石亦能點頭」，終會同意保外就醫。()\n",
      "黨漸失人心時，民進黨卻仍無法得利，獲得人民信任與支持。()\n",
      "立法院，要等到公式過關、台電回饋，遙遙無期。(中時電子報)\n",
      "，才出面聲援阿扁保外就醫，這些動作純粹是基於人道考量。()\n",
      "很清楚，馬應立即實踐諾言，讓扁保外就醫，實現醫療人權的承諾。\n",
      "有當召集人不是那麼重要，現在剩下政治上怎麼解決。(旺報 )\n",
      "由巷」揭牌儀式時，呼籲讓扁保外就醫資料來源：《蘋果》資料室\n",
      "郝出來開了第一槍，但他不覺得郝是藍營內部唯一贊成的人。()\n",
      "法核准保外醫治，讓陳水扁回家療養。1031210(中央社)\n",
      "這些社團要立委們一個一個用書面表態，「我們能不簽嗎？」()\n",
      "望集結社會各界的力量，能讓阿扁在過年前，達成保外就醫的願望。\n",
      "：輕度狹心症、高血脂症★膚色莫名變深資料來源：扁醫療小組\n",
      "醫」，扁上警備車後，還有支持者跳上車頭阻擋，隨即被警員拉下。\n",
      "治和諧考量，應由馬總統特赦阿扁，這是處理阿扁問題的最好方法。\n",
      "並起，南台灣無人要迎戰的的政局，也開始要讓馬英九頭疼。()\n",
      "及吳榮義明確表達支持赦扁；吳允諾當選成立「救扁小組」。()\n",
      "告確已收到，至於是否保外就醫，由矯正署會同台中監獄審慎處理。\n",
      "音，讓世界看見台灣民主和平發展，更尊重人道，重視人權。()\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    print(dataset[i][1][-30:].replace('\\n', ''))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2019",
   "language": "python",
   "name": "ml2019"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
