{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from procDataSet import TrainingQuery\n",
    "from bert_serving.client import BertClient\n",
    "from multiprocessing import Pool\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim  as optim\n",
    "import argparse\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.functional import F\n",
    "\n",
    "import os\n",
    "import jieba\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "jieba.load_userdict(os.path.join('data', 'dict.txt.big'))\n",
    "[jieba.add_word(i, freq=None, tag=None) for i in ['不支持','文林苑', '都更案','十八趴','證所稅','前瞻建設']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_num = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/stop_word_zhtw.txt') as file:\n",
    "    data = file.read()\n",
    "    \n",
    "stop_words = data.split('\\n')\n",
    "stop_words += ['「', '」', '，', '\\n', '）', '（', ')', '(']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "folder = 'data/'\n",
    "raw_training_data = pd.read_csv(os.path.join(folder,'TD.csv'))\n",
    "news_urls = pd.read_csv(os.path.join(folder,'NC_1.csv'))\n",
    "contents = pd.read_json(os.path.join(folder,'url2content.json'), typ=pd.Series)\n",
    "\n",
    "## sort the contents by index\n",
    "keys, content_list = contents.keys(), contents.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = np.array(pd.read_csv('./data/QS_1.csv').Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['通姦', '在', '刑法', '上', '應該', '除罪化']\n",
      "['應該', '取消', '機車', '強制', '二段式', '左轉', '(', '待轉', ')']\n",
      "['支持', '博弈', '特區', '在', '台灣', '合法化']\n",
      "['中華', '航空', '空服員', '罷工', '是', '合理', '的']\n",
      "['性交易', '應該', '合法化']\n",
      "['ECFA', '早', '收清單', '可', '（', '有', '）', '達到', '其', '預期', '成效']\n",
      "['應該', '減免', '證所稅']\n",
      "['贊成', '中油', '在', '觀塘', '興建', '第三', '天然氣', '接收站']\n",
      "['支持', '中國', '學生', '納入', '健保']\n",
      "['支持', '臺灣', '中小學', '（', '含', '高職', '、', '專科', '）', '服儀', '規定', '（', '含髮', '、', '襪', '、', '鞋', '）', '給予', '學生', '自主']\n",
      "['不支持', '使用', '加密', '貨幣']\n",
      "['不支持', '學雜費', '調漲']\n",
      "['同意', '政府', '舉債', '發展', '前瞻建設', '計畫']\n",
      "['支持', '電競', '列入', '體育競技']\n",
      "['反對', '台鐵', '東移', '徵收案']\n",
      "['支持', '陳前總統', '保外就醫']\n",
      "['年金', '改革', '應', '取消', '或', '應', '調降', '軍公教', '月', '退之', '優存', '利率', '十八趴']\n",
      "['同意', '動物', '實驗']\n",
      "['油價', '應該', '凍漲', '或', '緩漲']\n",
      "['反對', '旺旺', '中時', '併購', '中嘉']\n"
     ]
    }
   ],
   "source": [
    "for test_id , text_q in enumerate(test_query):\n",
    "    print(jieba.lcut(text_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "NumberCPU = multiprocessing.cpu_count()\n",
    "jieba.initialize()\n",
    "\n",
    "def jbcut(x):\n",
    "    if x is not None:\n",
    "        sen = jieba.lcut(x, cut_all=False)\n",
    "        sen = [i for i in sen if i not in stop_words]\n",
    "        return sen\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def psegcut(x):\n",
    "    if x is not None:\n",
    "        sen = pseg.lcut(x)\n",
    "        sen = [i for i in sen if i not in stop_words]\n",
    "        return sen\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "pool = multiprocessing.Pool(processes=NumberCPU)\n",
    "sentence_arr = pool.map(jbcut,content_list)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_arr = np.load('arr.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## okapi model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import bm25\n",
    "bm25Model = bm25.BM25(sentence_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "stands = ['支持', '應該', '反對', '贊成', '不'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_query = list(set(raw_training_data.Query))\n",
    "\n",
    "y_train = []\n",
    "y_index = {}\n",
    "for i  in train_query:\n",
    "    index = np.where(raw_training_data.Query==i)\n",
    "    data = raw_training_data.iloc[index]\n",
    "    y = dict(zip(data.News_Index,data.Relevance))\n",
    "    y_idx = [ (int(idx.split('_')[1])-1, rel )  for idx, rel in zip(data.News_Index, data.Relevance)]\n",
    "    y_train.append(y)\n",
    "    y_index[i] = [y_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_query = set(train_query) & set(test_query)\n",
    "common_query = list(common_query)\n",
    "common = [ train_query.index(i) for i in common_query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[139, 230, 230, 40, 95]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ len(list(y_train[i].values())) for i in common]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6933333333333334,\n",
       " 1.0533333333333332,\n",
       " 0.21333333333333335,\n",
       " 0.7066666666666667,\n",
       " 1.3,\n",
       " 2.7866666666666666,\n",
       " 2.1933333333333334,\n",
       " 0.77,\n",
       " 2.296666666666667,\n",
       " 1.4466666666666668,\n",
       " 0.5766666666666667,\n",
       " 2.6466666666666665,\n",
       " 1.3733333333333333,\n",
       " 1.35,\n",
       " 0.2833333333333333,\n",
       " 0.9433333333333334,\n",
       " 1.3233333333333333,\n",
       " 0.14,\n",
       " 0.23,\n",
       " 0.24333333333333335]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ np.sum(list(y_train[i].values())) / 300 for i in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 反對旺旺中時併購中嘉 ['反對', '旺旺', '中時', '併購', '中嘉']\n",
      "top query num 100\n",
      "162 / 208 \n",
      "1 核四應該啟用 ['核四', '應該', '啟用']\n",
      "top query num 100\n",
      "43 / 316 \n",
      "2 反對無圍牆校園 ['反對', '圍牆', '校園']\n",
      "top query num 100\n",
      "59 / 64 \n",
      "3 支持正名「臺灣」參與國際運動賽事 ['支持', '正名', '臺灣', '參與', '國際', '運動', '賽事']\n",
      "top query num 100\n",
      "154 / 212 \n",
      "4 反對二代健保規定 ['反對', '二代', '健保', '規定']\n",
      "top query num 100\n",
      "231 / 390 \n",
      "5 國際賽事會場內應該可以持中華民國國旗 ['國際賽', '事', '會場', '應該', '持', '中華民國', '國旗']\n",
      "top query num 100\n",
      "133 / 836 \n",
      "6 堅決反對政府舉債發展前瞻建設計畫 ['堅決', '反對', '政府', '舉債', '發展', '前瞻建設', '計畫']\n",
      "top query num 100\n",
      "304 / 658 \n",
      "7 另立專法保障同婚是正確的 ['另立', '專法', '保障', '同婚', '正確']\n",
      "top query num 100\n",
      "104 / 231 \n",
      "8 贊同課綱微調 ['贊同', '課綱', '微調']\n",
      "top query num 100\n",
      "142 / 689 \n",
      "9 贊成流浪動物零撲殺 ['贊成', '流浪', '動物', '零', '撲殺']\n",
      "top query num 100\n",
      "297 / 434 \n",
      "10 十二年國教高中職「免學費補助」適用對象增加是不對的 ['十二年', '國教', '高中職', '免', '學費', '補助', '適用', '對象', '增加', '不對']\n",
      "top query num 100\n",
      "114 / 173 \n",
      "11 應該提高酒駕罰責以有效遏制酒駕 ['應該', '提高', '酒駕', '罰責', '有效', '遏制', '酒駕']\n",
      "top query num 100\n",
      "383 / 794 \n",
      "12 臺灣應開放含瘦肉精(萊克多巴胺)之美國牛肉進口 ['臺灣', '應', '開放', '含', '瘦肉精', '萊克', '多巴胺', '美國', '牛肉', '進口']\n",
      "top query num 100\n",
      "103 / 412 \n",
      "13 支持陳前總統保外就醫 ['支持', '陳前總統', '保外就醫']\n",
      "top query num 100\n",
      "135 / 405 \n",
      "14 贊成文林苑都更案可依法拆除王家 ['贊成', '文林苑', '都更案', '依法', '拆除', '王家']\n",
      "top query num 100\n",
      "63 / 85 \n",
      "15 年金改革應取消或應調降軍公教月退之優存利率十八趴 ['年金', '改革', '應', '取消', '應', '調降', '軍公教', '月', '退之', '優存', '利率', '十八趴']\n",
      "top query num 100\n",
      "100 / 283 \n",
      "16 遠雄大巨蛋工程應停工或拆除 ['遠雄', '大巨蛋', '工程', '應', '停工', '拆除']\n",
      "top query num 100\n",
      "229 / 397 \n",
      "17 同意動物實驗 ['同意', '動物', '實驗']\n",
      "top query num 100\n",
      "42 / 42 \n",
      "18 油價應該凍漲或緩漲 ['油價', '應該', '凍漲', '緩漲']\n",
      "top query num 100\n",
      "45 / 69 \n",
      "19 拒絕公投通過門檻下修 ['拒絕', '公投', '門檻', '下修']\n",
      "top query num 100\n",
      "26 / 73 \n"
     ]
    }
   ],
   "source": [
    "total_scores = list()\n",
    "for test_id , text_q in enumerate(train_query):\n",
    "    text = jieba.lcut(text_q)\n",
    "    text = [ t for t in text if t not in stop_words]\n",
    "    \n",
    "    scores = np.array(bm25Model.get_scores(text))\n",
    "    \n",
    "    top_query = np.argsort(scores)[::-1][:100]\n",
    "    irrelevant_query = np.argsort(scores)[:100]\n",
    "    \n",
    "    print(test_id ,text_q, text)\n",
    "    print(\"top query num {}\".format(len(top_query)))\n",
    "    \n",
    "    for query in top_query:\n",
    "        all_words = [ (sentence_arr[query][cnt], bm25Model.get_score(sentence_arr[query], cnt))\n",
    "                     for cnt, i in enumerate(sentence_arr[query])]\n",
    "\n",
    "        all_words = sorted(all_words,key=lambda x:(x[1]))[::-1]\n",
    "        top_words = all_words[:20]\n",
    "        top_words = [x[0] for x in top_words]\n",
    "        text += top_words\n",
    "    scores += 0.5 * np.array(bm25Model.get_scores(text))\n",
    "\n",
    "    irrelevant = []\n",
    "    for query in irrelevant_query:\n",
    "        all_words = [ (sentence_arr[query][cnt], bm25Model.get_score(sentence_arr[query], cnt))\n",
    "                     for cnt, i in enumerate(sentence_arr[query])]\n",
    "\n",
    "        all_words = sorted(all_words,key=lambda x:(x[1]))[::-1]\n",
    "        top_words = all_words[:20]\n",
    "        top_words = [x[0] for x in top_words]\n",
    "        irrelevant += top_words\n",
    "\n",
    "    scores -= 0.5 * np.array(bm25Model.get_scores(irrelevant))\n",
    "#     for idx, rel in (y_index[text_q][0]):\n",
    "#         scores[idx] += np.mean(scores) * idx\n",
    "    \n",
    "    top_num = 300\n",
    "    keys = pd.DataFrame(np.argsort(scores)[::-1][:top_num])\n",
    "    ans = keys[0].apply(lambda x: 'news_{:06d}'.format(x+1))\n",
    "    \n",
    "    validation_score = 0\n",
    "    for a in ans:\n",
    "        if a in y_train[test_id].keys():\n",
    "            validation_score += y_train[test_id][a]\n",
    "            \n",
    "    total_scores.append(validation_score/top_num)\n",
    "    print(\"{} / {} \".format(validation_score,(np.sum(list(y_train[test_id].values())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Building dictionary...')\n",
    "from gensim import corpora\n",
    "data = sentence_arr\n",
    "dictionary = corpora.Dictionary(data)\n",
    "corpus = [dictionary.doc2bow(text) for text in data]\n",
    "corpora.MmCorpus.serialize('corpus.mm', corpus) \n",
    "\n",
    "from gensim import models\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "corpora.MmCorpus.serialize('corpus.mm', corpus_tfidf) \n",
    "tfidf.save(\"my_model.tfidf\")\n",
    "tfidf = models.TfidfModel.load(\"my_model.tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building LsiModel...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-67bd2924a090>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Building LsiModel...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcorpus_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMmCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corpus.mm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlsi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLsiModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Building MatrixSimilarity...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.6/site-packages/gensim/models/lsimodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, chunksize, decay, distributed, onepass, power_iters, extra_samples, dtype)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.6/site-packages/gensim/models/lsimodel.py\u001b[0m in \u001b[0;36madd_documents\u001b[0;34m(self, corpus, chunksize, decay)\u001b[0m\n\u001b[1;32m    510\u001b[0m                         update = Projection(\n\u001b[1;32m    511\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m                             \u001b[0mpower_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m                         )\n\u001b[1;32m    514\u001b[0m                         \u001b[0;32mdel\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.6/site-packages/gensim/models/lsimodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, m, k, docs, use_svdlibc, power_iters, extra_dims, dtype)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0mnum_terms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower_iters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                     extra_dims=self.extra_dims, dtype=dtype)\n\u001b[0m\u001b[1;32m    200\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.6/site-packages/gensim/models/lsimodel.py\u001b[0m in \u001b[0;36mstochastic_svd\u001b[0;34m(corpus, rank, num_terms, chunksize, extra_dims, power_iters, dtype, eps)\u001b[0m\n\u001b[1;32m    917\u001b[0m     \u001b[0;31m# build Y in blocks of `chunksize` documents (much faster than going one-by-one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[0;31m# and more memory friendly than processing all documents at once)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_terms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1st phase: constructing %s action matrix\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Building LsiModel...')\n",
    "corpus_tfidf = corpora.MmCorpus('corpus.mm')\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=100)\n",
    "\n",
    "print('Building MatrixSimilarity...')\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "index = MatrixSimilarity(lsi[corpus_tfidf])\n",
    "\n",
    "index.save('deerwester.index')\n",
    "index = MatrixSimilarity.load('deerwester.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "0 拒絕公投通過門檻下修 ['反對', '旺旺', '中時', '併購', '中嘉']\n",
      "top query num 100\n",
      "105 / 208 \n",
      "1 拒絕公投通過門檻下修 ['核四', '應該', '啟用']\n",
      "top query num 100\n"
     ]
    }
   ],
   "source": [
    "total_scores = []\n",
    "print('Testing...')\n",
    "result = np.zeros((20,300)).astype('str')\n",
    "\n",
    "j = 0\n",
    "for test_id, doc in enumerate(train_query):\n",
    "    tokens = jieba.lcut(doc)\n",
    "    text = tokens\n",
    "    text = [ t for t in text if t not in stop_words]\n",
    "    \n",
    "    scores = np.array(bm25Model.get_scores(text))\n",
    "    top_query = np.argsort(scores)[::-1][:100]\n",
    "    irrelevant_query = np.argsort(scores)[:100]\n",
    "    \n",
    "    score = scores\n",
    "    score = (score-min(score))/(max(score)-min(score))\n",
    "    score *= 1.2\n",
    "    print(test_id ,text_q, text)\n",
    "    print(\"top query num {}\".format(len(top_query)))\n",
    "    \n",
    "    for query in top_query:\n",
    "        all_words = [ (sentence_arr[query][cnt], bm25Model.get_score(sentence_arr[query], cnt))\n",
    "                     for cnt, i in enumerate(sentence_arr[query])]\n",
    "\n",
    "        all_words = sorted(all_words,key=lambda x:(x[1]))[::-1]\n",
    "        top_words = all_words[:20]\n",
    "        top_words = [x[0] for x in top_words]\n",
    "        text += top_words\n",
    "    tmp = np.array(bm25Model.get_scores(text))\n",
    "    tmp = (tmp-min(tmp))/(max(tmp)-min(tmp))\n",
    "    scores += 0.2 * tmp\n",
    "    \n",
    "    irrelevant = []\n",
    "    for query in irrelevant_query:\n",
    "        all_words = [ (sentence_arr[query][cnt], bm25Model.get_score(sentence_arr[query], cnt))\n",
    "                     for cnt, i in enumerate(sentence_arr[query])]\n",
    "\n",
    "        all_words = sorted(all_words,key=lambda x:(x[1]))[::-1]\n",
    "        top_words = all_words[:20]\n",
    "        top_words = [x[0] for x in top_words]\n",
    "        irrelevant += top_words\n",
    "        \n",
    "    tmp = np.array(bm25Model.get_scores(irrelevant))\n",
    "    tmp = (tmp-min(tmp))/(max(tmp)-min(tmp))\n",
    "    scores -= 0.2 * tmp\n",
    "    \n",
    "    vec_bow = dictionary.doc2bow(tokens)\n",
    "    vec_lsi = lsi[vec_bow]\n",
    "    sims = index[vec_lsi]\n",
    "    tmp = sims\n",
    "    tmp = (tmp-min(tmp))/(max(tmp)-min(tmp))\n",
    "    scores += tmp\n",
    "        \n",
    "    vec_bow = dictionary.doc2bow(text)\n",
    "    vec_lsi = lsi[vec_bow]\n",
    "    sims = index[vec_lsi]\n",
    "    tmp = sims\n",
    "    tmp = (tmp-min(tmp))/(max(tmp)-min(tmp))\n",
    "    scores += 0.2 * tmp\n",
    "\n",
    "    vec_bow = dictionary.doc2bow(irrelevant)\n",
    "    vec_lsi = lsi[vec_bow]\n",
    "    sims = index[vec_lsi]\n",
    "    tmp = sims\n",
    "    tmp = (tmp-min(tmp))/(max(tmp)-min(tmp))\n",
    "    scores -= 0.2 * tmp\n",
    "\n",
    "    top_num = 300\n",
    "    keys = pd.DataFrame(np.argsort(scores)[::-1][:top_num])\n",
    "    ans = keys[0].apply(lambda x: 'news_{:06d}'.format(x+1))\n",
    "    \n",
    "    validation_score = 0\n",
    "    for a in ans:\n",
    "        if a in y_train[test_id].keys():\n",
    "            validation_score += y_train[test_id][a]\n",
    "    \n",
    "    total_scores.append(validation_score/top_num)\n",
    "    print(\"{} / {} \".format(validation_score,(np.sum(list(y_train[test_id].values())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4445"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(total_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix, vstack, csc_matrix\n",
    "pfeature  = vstack([pos, neg])\n",
    "plabel =  [1] * pos.shape[0] + [0] * neg.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "indices = np.arange(pfeature.shape[0]) #gets the number of rows \n",
    "\n",
    "combined = (list(zip(indices, plabel)))\n",
    "random.shuffle(combined)\n",
    "indices, plabel = zip(*combined)\n",
    "\n",
    "pfeature = pfeature[list(indices)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(pfeature, plabel, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.5, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model =  SVC(C=0.5)\n",
    "model.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4975"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(testY == pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 反對旺旺中時併購中嘉 ['反對', '旺旺', '中時', '併購', '中嘉']\n",
      "top query num 1000\n"
     ]
    }
   ],
   "source": [
    "total_scores = list()\n",
    "for test_id , text_q in enumerate(train_query):\n",
    "    text = jieba.lcut(text_q)\n",
    "    text = [ t for t in text if t not in stop_words]\n",
    "    \n",
    "    scores = np.array(bm25Model.get_scores(text))\n",
    "    \n",
    "    top_query = np.argsort(scores)[::-1][:1000]\n",
    "    irrelevant_query = np.argsort(scores)[:1000]\n",
    "\n",
    "    print(test_id ,text_q, text)\n",
    "    print(\"top query num {}\".format(len(top_query)))\n",
    "\n",
    "    ## pseudo relevant feedback\n",
    "    pos = X[top_query]\n",
    "    neg = X[irrelevant_query]\n",
    "    break\n",
    "    plabel = [1] * pos.shape[0] + [0] * neg.shape[0]\n",
    "    pfeature  = vstack([pos, neg])\n",
    "    \n",
    "    indices = np.arange(pfeature.shape[0])\n",
    "    combined = (list(zip(indices, plabel)))\n",
    "    random.shuffle(combined)\n",
    "    indices, plabel = zip(*combined)\n",
    "    pfeature = pfeature[list(indices)]\n",
    "    \n",
    "    model = GradientBoostingClassifier(n_estimators=100)\n",
    "    model.fit(pfeature, plabel)\n",
    "    scores += 0.5 * np.max(scores) * model.predict_proba(X)[:,1]\n",
    "\n",
    "    top_num = 300\n",
    "    keys = pd.DataFrame(np.argsort(scores)[::-1][:top_num])\n",
    "    ans = keys[0].apply(lambda x: 'news_{:06d}'.format(x+1))\n",
    "    \n",
    "    validation_score = 0\n",
    "    for a in ans:\n",
    "        if a in y_train[test_id].keys():\n",
    "            validation_score += y_train[test_id][a]\n",
    "            \n",
    "    total_scores.append(validation_score / top_num)\n",
    "    print(\"{} / {} \".format(validation_score,(np.sum(list(y_train[test_id].values())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${\\displaystyle k_{1}\\in [1.2,2.0]}, {\\displaystyle b=0.75}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 反對旺旺中時併購中嘉 ['反對', '旺旺', '中時', '併購', '中嘉']\n",
      "top query num 1000\n",
      "168 / 208 \n",
      "1 核四應該啟用 ['核四', '應該', '啟用']\n",
      "top query num 1000\n"
     ]
    }
   ],
   "source": [
    "total_scores = list()\n",
    "for test_id , text_q in enumerate(train_query):\n",
    "    text = jieba.lcut(text_q)\n",
    "    text = [ t for t in text if t not in stop_words]\n",
    "    \n",
    "    scores = np.array(bm25Model.get_scores(text))\n",
    "    \n",
    "    top_query = np.argsort(scores)[::-1][:1000]\n",
    "    irrelevant_query = np.argsort(scores)[:1000]\n",
    "    \n",
    "    print(test_id ,text_q, text)\n",
    "    print(\"top query num {}\".format(len(top_query)))\n",
    "    \n",
    "    for query in top_query:\n",
    "        all_words = [ (sentence_arr[query][cnt], bm25Model.get_score(sentence_arr[query], cnt))\n",
    "                     for cnt, i in enumerate(sentence_arr[query])]\n",
    "\n",
    "        all_words = sorted(all_words,key=lambda x:(x[1]))[::-1]\n",
    "        top_words = all_words[:20]\n",
    "        top_words = [x[0] for x in top_words]\n",
    "        text += top_words\n",
    "    scores += 0.5 * np.array(bm25Model.get_scores(text))\n",
    "\n",
    "    irrelevant = []\n",
    "    for query in irrelevant_query:\n",
    "        all_words = [ (sentence_arr[query][cnt], bm25Model.get_score(sentence_arr[query], cnt))\n",
    "                     for cnt, i in enumerate(sentence_arr[query])]\n",
    "\n",
    "        all_words = sorted(all_words,key=lambda x:(x[1]))[::-1]\n",
    "        top_words = all_words[:20]\n",
    "        top_words = [x[0] for x in top_words]\n",
    "        irrelevant += top_words\n",
    "\n",
    "    scores -= 0.5 * np.array(bm25Model.get_scores(irrelevant))\n",
    "#     for idx, rel in (y_index[text_q][0]):\n",
    "#         scores[idx] += np.mean(scores) * idx\n",
    "    \n",
    "    top_num = 300\n",
    "    keys = pd.DataFrame(np.argsort(scores)[::-1][:top_num])\n",
    "    ans = keys[0].apply(lambda x: 'news_{:06d}'.format(x+1))\n",
    "    \n",
    "    validation_score = 0\n",
    "    for a in ans:\n",
    "        if a in y_train[test_id].keys():\n",
    "            validation_score += y_train[test_id][a]\n",
    "            \n",
    "    total_scores.append(validation_score/top_num)\n",
    "    print(\"{} / {} \".format(validation_score,(np.sum(list(y_train[test_id].values())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(total_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_num = 300\n",
    "search_result = np.zeros((20,top_num))\n",
    "for cnt,i in enumerate(total_scores):\n",
    "    keys = np.argsort(i)[::-1][:top_num]\n",
    "    search_result[cnt] += keys\n",
    "    \n",
    "search_result = search_result.astype(np.int)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Query_Index'] = ['q_{:02d}'.format(i+1) for i in range(20)]\n",
    "\n",
    "for i in range(top_num):\n",
    "    df['Rank_{:03d}'.format(i+1)] = search_result[:, i] + 1\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    df.iloc[i, 1:] = df.iloc[i, 1:].apply(lambda x: 'news_{:06d}'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'simple27.csv'\n",
    "assert fname not in (os.listdir('output/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output/' + fname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# search_result[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
