{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from procDataSet import TrainingQuery\n",
    "from bert_serving.client import BertClient\n",
    "from multiprocessing import Pool\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim  as optim\n",
    "import argparse\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.functional import F\n",
    "import os\n",
    "import jieba\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from IPython.display import clear_output\n",
    "\n",
    "jieba.load_userdict(os.path.join('data', 'dict.txt.big'))\n",
    "[jieba.add_word(i, freq=None, tag=None) for i in ['不支持','文林苑', '都更案','十八趴','證所稅','前瞻建設']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_num = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/stop_word_zhtw.txt') as file:\n",
    "    data = file.read()\n",
    "    \n",
    "stop_words = data.split('\\n')\n",
    "stop_words += ['「', '」', '，', '\\n', '）', '（', ')', '(']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "folder = 'data/'\n",
    "raw_training_data = pd.read_csv(os.path.join(folder,'TD.csv'))\n",
    "news_urls = pd.read_csv(os.path.join(folder,'NC_1.csv'))\n",
    "contents = pd.read_json(os.path.join(folder,'url2content.json'), typ=pd.Series)\n",
    "\n",
    "## sort the contents by index\n",
    "keys, content_list = contents.keys(), contents.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = np.array(pd.read_csv('./data/QS_1.csv').Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_id , text_q in enumerate(test_query):\n",
    "    print(jieba.lcut_for_search(text_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "NumberCPU = multiprocessing.cpu_count()\n",
    "jieba.initialize()\n",
    "\n",
    "def jbcut(x):\n",
    "    if x is not None:\n",
    "        sen = jieba.lcut(x, cut_all=False)\n",
    "        sen = [i for i in sen if i not in stop_words]\n",
    "        return sen\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "import jieba.posseg as pseg\n",
    "def psegcut(x):\n",
    "    if x is not None:\n",
    "        sen = pseg.lcut(x)\n",
    "        sen = [i for i in sen if i not in stop_words]\n",
    "        return sen\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "pool = multiprocessing.Pool(processes=NumberCPU)\n",
    "sentenece_arr = pool.map(psegcut,content_list)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## okapi model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import bm25\n",
    "bm25Model = bm25.BM25(sentenece_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stands = ['支持', '應該', '反對', '贊成', '不'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_query = list(set(raw_training_data.Query))\n",
    "\n",
    "y_train = []\n",
    "y_index = {}\n",
    "for i  in train_query:\n",
    "    index = np.where(raw_training_data.Query==i)\n",
    "    data = raw_training_data.iloc[index]\n",
    "    y = dict(zip(data.News_Index,data.Relevance))\n",
    "    y_idx = [ (int(idx.split('_')[1])-1, rel )  for idx, rel in zip(data.News_Index, data.Relevance)]\n",
    "    y_train.append(y)\n",
    "    y_index[i] = [y_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_query = set(train_query) & set(test_query)\n",
    "common_query = list(common_query)\n",
    "common = [ train_query.index(i) for i in common_query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[230, 95, 139, 40, 230]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ len(list(y_train[i].values())) for i in common]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.1933333333333334,\n",
       " 1.0533333333333332,\n",
       " 0.5766666666666667,\n",
       " 1.3233333333333333,\n",
       " 1.4466666666666668,\n",
       " 0.9433333333333334,\n",
       " 0.2833333333333333,\n",
       " 0.23,\n",
       " 0.21333333333333335,\n",
       " 2.296666666666667,\n",
       " 0.6933333333333334,\n",
       " 0.77,\n",
       " 0.7066666666666667,\n",
       " 1.3,\n",
       " 1.3733333333333333,\n",
       " 0.24333333333333335,\n",
       " 0.14,\n",
       " 1.35,\n",
       " 2.6466666666666665,\n",
       " 2.7866666666666666]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ np.sum(list(y_train[i].values())) / 300 for i in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4789999999999999"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(total_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9433333333333334, 0.23, 0.6933333333333334, 0.14, 1.35]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ np.sum(list(y_train[i].values())) / 300 for i in common]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 堅決反對政府舉債發展前瞻建設計畫 ['堅決', '反對', '政府', '舉債', '發展', '前瞻建設', '計畫']\n",
      "top query num 100\n",
      "304 / 658 \n",
      "1 核四應該啟用 ['核四', '應該', '啟用']\n",
      "top query num 100\n",
      "45 / 316 \n",
      "2 十二年國教高中職「免學費補助」適用對象增加是不對的 ['十二年', '國教', '高中職', '免', '學費', '補助', '適用', '對象', '增加', '不對']\n",
      "top query num 100\n",
      "114 / 173 \n",
      "3 遠雄大巨蛋工程應停工或拆除 ['遠雄', '大巨蛋', '工程', '應', '停工', '拆除']\n",
      "top query num 100\n",
      "229 / 397 \n",
      "4 贊成流浪動物零撲殺 ['贊成', '流浪', '動物', '零', '撲殺']\n",
      "top query num 100\n",
      "299 / 434 \n",
      "5 年金改革應取消或應調降軍公教月退之優存利率十八趴 ['年金', '改革', '應', '取消', '應', '調降', '軍公教', '月', '退之', '優存', '利率', '十八趴']\n",
      "top query num 100\n",
      "101 / 283 \n",
      "6 贊成文林苑都更案可依法拆除王家 ['贊成', '文林苑', '都更案', '依法', '拆除', '王家']\n",
      "top query num 100\n",
      "63 / 85 \n",
      "7 油價應該凍漲或緩漲 ['油價', '應該', '凍漲', '緩漲']\n",
      "top query num 100\n"
     ]
    }
   ],
   "source": [
    "total_scores = list()\n",
    "for test_id , text_q in enumerate(train_query):\n",
    "    text = jieba.lcut(text_q)\n",
    "    text = [ t for t in text if t not in stop_words]\n",
    "    scores = np.array(bm25Model.get_scores(text))\n",
    "    top_query = np.argsort(scores)[::-1][:100]\n",
    "    irrelevant_query = np.argsort(scores)[:100]\n",
    "    \n",
    "    print(test_id ,text_q, text)\n",
    "    print(\"top query num {}\".format(len(top_query)))\n",
    "    \n",
    "    \n",
    "    for query in top_query:\n",
    "        all_words = [ (sentenece_arr[query][cnt], bm25Model.get_score(sentenece_arr[query], cnt))\n",
    "                     for cnt, i in enumerate(sentenece_arr[query])]\n",
    "\n",
    "        all_words = sorted(all_words,key=lambda x:(x[1]))[::-1]\n",
    "        top_words = all_words[:20]\n",
    "        top_words = [x[0] for x in top_words]\n",
    "        text += top_words\n",
    "    scores += 0.3 * np.array(bm25Model.get_scores(text))\n",
    "    \n",
    "    irrelevant = []\n",
    "    for query in irrelevant_query:\n",
    "        all_words = [ (sentenece_arr[query][cnt], bm25Model.get_score(sentenece_arr[query], cnt))\n",
    "                     for cnt, i in enumerate(sentenece_arr[query])]\n",
    "\n",
    "        all_words = sorted(all_words,key=lambda x:(x[1]))[::-1]\n",
    "        top_words = all_words[:20]\n",
    "        top_words = [x[0] for x in top_words]\n",
    "        irrelevant += top_words\n",
    "    \n",
    "    scores -= 0.3 * np.array(bm25Model.get_scores(irrelevant))\n",
    "#     for idx, rel in (y_index[text_q][0]):\n",
    "#         scores[idx] += np.mean(scores) * idx\n",
    "    \n",
    "    top_num = 300\n",
    "    keys = pd.DataFrame(np.argsort(scores)[::-1][:top_num])\n",
    "    ans = keys[0].apply(lambda x: 'news_{:06d}'.format(x+1))\n",
    "    \n",
    "    validation_score = 0\n",
    "    for a in ans:\n",
    "        if a in y_train[test_id].keys():\n",
    "            validation_score += y_train[test_id][a]\n",
    "            \n",
    "    total_scores.append(validation_score/top_num)\n",
    "    print(\"{} / {} \".format(validation_score,(np.sum(list(y_train[test_id].values())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${\\displaystyle k_{1}\\in [1.2,2.0]}, {\\displaystyle b=0.75}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 通姦在刑法上應該除罪化 ['通姦', '刑法', '應該', '除罪化']\n",
      "top query num 100\n",
      "1 應該取消機車強制二段式左轉(待轉) ['應該', '取消', '機車', '強制', '二段式', '左轉', '待轉']\n",
      "top query num 100\n",
      "2 支持博弈特區在台灣合法化 ['支持', '博弈', '特區', '台灣', '合法化']\n",
      "top query num 100\n",
      "3 中華航空空服員罷工是合理的 ['中華', '航空', '空服員', '罷工', '合理']\n",
      "top query num 100\n",
      "4 性交易應該合法化 ['性交易', '應該', '合法化']\n",
      "top query num 100\n",
      "5 ECFA早收清單可（有）達到其預期成效 ['ECFA', '早', '收清單', '達到', '預期', '成效']\n",
      "top query num 100\n",
      "6 應該減免證所稅 ['應該', '減免', '證所稅']\n",
      "top query num 100\n",
      "7 贊成中油在觀塘興建第三天然氣接收站 ['贊成', '中油', '觀塘', '興建', '第三', '天然氣', '接收站']\n",
      "top query num 100\n",
      "8 支持中國學生納入健保 ['支持', '中國', '學生', '納入', '健保']\n",
      "top query num 100\n",
      "9 支持臺灣中小學（含高職、專科）服儀規定（含髮、襪、鞋）給予學生自主 ['支持', '臺灣', '中小學', '含', '高職', '專科', '服儀', '規定', '含髮', '襪', '鞋', '給予', '學生', '自主']\n",
      "top query num 100\n",
      "10 不支持使用加密貨幣 ['不支持', '使用', '加密', '貨幣']\n",
      "top query num 100\n",
      "11 不支持學雜費調漲 ['不支持', '學雜費', '調漲']\n",
      "top query num 100\n",
      "12 同意政府舉債發展前瞻建設計畫 ['同意', '政府', '舉債', '發展', '前瞻建設', '計畫']\n",
      "top query num 100\n",
      "13 支持電競列入體育競技 ['支持', '電競', '列入', '體育競技']\n",
      "top query num 100\n",
      "14 反對台鐵東移徵收案 ['反對', '台鐵', '東移', '徵收案']\n",
      "top query num 100\n",
      "15 支持陳前總統保外就醫 ['支持', '陳前總統', '保外就醫']\n",
      "top query num 100\n",
      "16 年金改革應取消或應調降軍公教月退之優存利率十八趴 ['年金', '改革', '應', '取消', '應', '調降', '軍公教', '月', '退之', '優存', '利率', '十八趴']\n",
      "top query num 100\n"
     ]
    }
   ],
   "source": [
    "total_scores = list()\n",
    "for test_id , text_q in enumerate(test_query):\n",
    "    text = jieba.lcut(text_q)\n",
    "    text = [ t for t in text if t not in stop_words]\n",
    "    scores = bm25Model.get_scores(text)\n",
    "    top_query = np.argsort(scores)[::-1][:100]\n",
    "    irrelevant_query = np.argsort(scores)[:100]\n",
    "    \n",
    "    print(test_id ,text_q, text)\n",
    "    print(\"top query num {}\".format(len(top_query)))\n",
    "    \n",
    "    for query in top_query:\n",
    "        all_words = [ (sentenece_arr[query][cnt], bm25Model.get_score(sentenece_arr[query], cnt))\n",
    "                     for cnt, i in enumerate(sentenece_arr[query])]\n",
    "\n",
    "        all_words = sorted(all_words,key=lambda x:(x[1]))[::-1]\n",
    "        top_words = all_words[:20]\n",
    "        top_words = [x[0] for x in top_words]\n",
    "        text += top_words\n",
    "    scores += 0.3 * np.array(bm25Model.get_scores(text))\n",
    "    \n",
    "    irrelevant = []\n",
    "    for query in irrelevant_query:\n",
    "        all_words = [ (sentenece_arr[query][cnt], bm25Model.get_score(sentenece_arr[query], cnt))\n",
    "                     for cnt, i in enumerate(sentenece_arr[query])]\n",
    "\n",
    "        all_words = sorted(all_words,key=lambda x:(x[1]))[::-1]\n",
    "        top_words = all_words[:20]\n",
    "        top_words = [x[0] for x in top_words]\n",
    "        irrelevant += top_words\n",
    "    \n",
    "    scores -= 0.3 * np.array(bm25Model.get_scores(irrelevant))\n",
    "    \n",
    "#     scores = bm25Model.get_scores(text)\n",
    "    delta = np.max(scores) \n",
    "    if text_q in y_index.keys():\n",
    "        for idx, rel in (y_index[text_q][0]):\n",
    "            scores[idx] += delta * idx\n",
    "            \n",
    "    total_scores.append(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_num = 300\n",
    "search_result = np.zeros((20,top_num))\n",
    "for cnt,i in enumerate(total_scores):\n",
    "    keys = np.argsort(i)[::-1][:top_num]\n",
    "    search_result[cnt] += keys\n",
    "    \n",
    "search_result = search_result.astype(np.int)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Query_Index'] = ['q_{:02d}'.format(i+1) for i in range(20)]\n",
    "\n",
    "for i in range(top_num):\n",
    "    df['Rank_{:03d}'.format(i+1)] = search_result[:, i] + 1\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    df.iloc[i, 1:] = df.iloc[i, 1:].apply(lambda x: 'news_{:06d}'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['simple.csv',\n",
       " 'simple10.csv',\n",
       " 'simple11.csv',\n",
       " 'simple12.csv',\n",
       " 'simple13.csv',\n",
       " 'simple14.csv',\n",
       " 'simple15.csv',\n",
       " 'simple16.csv',\n",
       " 'simple17.csv',\n",
       " 'simple18.csv',\n",
       " 'simple19.csv',\n",
       " 'simple2.csv',\n",
       " 'simple20.csv',\n",
       " 'simple21.csv',\n",
       " 'simple3.csv',\n",
       " 'simple4.csv',\n",
       " 'simple5.csv',\n",
       " 'simple8.csv',\n",
       " 'simple9.csv',\n",
       " 'top2000.csv']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "sorted(os.listdir('output/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'simple22.csv'\n",
    "df.to_csv('output/' + fname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# search_result[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
