{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from procDataSet import TrainingQuery\n",
    "from bert_serving.client import BertClient\n",
    "from multiprocessing import Pool\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim  as optim\n",
    "import argparse\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.functional import F\n",
    "import os\n",
    "import jieba\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from IPython.display import clear_output\n",
    "import jieba.posseg as pseg\n",
    "jieba.load_userdict(os.path.join('data', 'dict.txt.big'))\n",
    "[jieba.add_word(i, freq=None, tag=None) for i in ['不支持','文林苑', '都更案','十八趴','證所稅','前瞻建設']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_num = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/stop_word_zhtw.txt') as file:\n",
    "    data = file.read()\n",
    "    \n",
    "stop_words = data.split('\\n')\n",
    "stop_words += ['「', '」', '，', '\\n', '）', '（', ')', '(']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "folder = 'data/'\n",
    "raw_training_data = pd.read_csv(os.path.join(folder,'TD.csv'))\n",
    "news_urls = pd.read_csv(os.path.join(folder,'NC_1.csv'))\n",
    "contents = pd.read_json(os.path.join(folder,'url2content.json'), typ=pd.Series)\n",
    "\n",
    "## sort the contents by index\n",
    "keys, content_list = contents.keys(), contents.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = np.array(pd.read_csv('./data/QS_1.csv').Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['通姦', '在', '刑法', '上', '應該', '除罪', '罪化', '除罪化']\n",
      "['應該', '取消', '機車', '強制', '二段', '段式', '二段式', '左轉', '(', '待轉', ')']\n",
      "['支持', '博弈', '特區', '在', '台灣', '合法', '合法化']\n",
      "['中華', '航空', '空服', '服員', '空服員', '罷工', '是', '合理', '的']\n",
      "['性交', '交易', '性交易', '應該', '合法', '合法化']\n",
      "['ECFA', '早', '收清', '清單', '收清單', '可', '（', '有', '）', '達到', '其', '預期', '成效']\n",
      "['應該', '減免', '所稅', '證所稅']\n",
      "['贊成', '中油', '在', '觀塘', '興建', '第三', '天然', '天然氣', '接收', '接收站']\n",
      "['支持', '中國', '學生', '納入', '健保']\n",
      "['支持', '臺灣', '中小', '小學', '中小學', '（', '含', '高職', '、', '專科', '）', '服儀', '規定', '（', '含髮', '、', '襪', '、', '鞋', '）', '給予', '學生', '自主']\n",
      "['不支', '支持', '不支持', '使用', '加密', '貨幣']\n",
      "['不支', '支持', '不支持', '雜費', '學雜費', '調漲']\n",
      "['同意', '政府', '舉債', '發展', '前瞻', '建設', '前瞻建設', '計畫']\n",
      "['支持', '電競', '列入', '體育', '競技', '體育競技']\n",
      "['反對', '台鐵', '東移', '徵收', '收案', '徵收案']\n",
      "['支持', '陳前', '總統', '前總統', '陳前總統', '保外', '就醫', '保外就醫']\n",
      "['年金', '改革', '應', '取消', '或', '應', '調降', '公教', '軍公教', '月', '退之', '優存', '利率', '十八', '十八趴']\n",
      "['同意', '動物', '實驗']\n",
      "['油價', '應該', '凍漲', '或', '緩漲']\n",
      "['反對', '旺旺', '中時', '併購', '中嘉']\n"
     ]
    }
   ],
   "source": [
    "for test_id , text_q in enumerate(test_query):\n",
    "    print(jieba.lcut_for_search(text_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "NumberCPU = multiprocessing.cpu_count()\n",
    "jieba.initialize()\n",
    "\n",
    "def jbcut(x):\n",
    "    if x is not None:\n",
    "        sen = jieba.lcut(x, cut_all=False)\n",
    "        sen = [i for i in sen if i not in stop_words]\n",
    "        return sen\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def psegcut(x):\n",
    "    if x is not None:\n",
    "        sen = pseg.lcut(x)\n",
    "        sen = [i for i in sen if i not in stop_words]\n",
    "        return sen\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "pool = multiprocessing.Pool(processes=NumberCPU)\n",
    "sentenece_arr = pool.map(jbcut,content_list)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_arr = np.load('arr.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## okapi model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import bm25\n",
    "bm25Model = bm25.BM25(sentence_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stands = ['支持', '應該', '反對', '贊成', '不'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_query = list(set(raw_training_data.Query))\n",
    "\n",
    "y_train = []\n",
    "y_index = {}\n",
    "for i  in train_query:\n",
    "    index = np.where(raw_training_data.Query==i)\n",
    "    data = raw_training_data.iloc[index]\n",
    "    y = dict(zip(data.News_Index,data.Relevance))\n",
    "    y_idx = [ (int(idx.split('_')[1])-1, rel )  for idx, rel in zip(data.News_Index, data.Relevance)]\n",
    "    y_train.append(y)\n",
    "    y_index[i] = [y_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_query = set(train_query) & set(test_query)\n",
    "common_query = list(common_query)\n",
    "common = [ train_query.index(i) for i in common_query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[95, 230, 40, 139, 230]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ len(list(y_train[i].values())) for i in common]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.23,\n",
       " 2.7866666666666666,\n",
       " 0.5766666666666667,\n",
       " 2.1933333333333334,\n",
       " 1.3233333333333333,\n",
       " 0.2833333333333333,\n",
       " 0.9433333333333334,\n",
       " 0.7066666666666667,\n",
       " 0.77,\n",
       " 1.0533333333333332,\n",
       " 0.14,\n",
       " 2.296666666666667,\n",
       " 0.6933333333333334,\n",
       " 0.21333333333333335,\n",
       " 1.35,\n",
       " 1.4466666666666668,\n",
       " 0.24333333333333335,\n",
       " 1.3,\n",
       " 2.6466666666666665,\n",
       " 1.3733333333333333]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ np.sum(list(y_train[i].values())) / 300 for i in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ np.sum(list(y_train[i].values())) / 300 for i in common]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 油價應該凍漲或緩漲 ['油價/n', '應該/v', '凍漲/Vi', '或/c', '緩漲/Vi']\n",
      "top query num 80\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sentenece_arr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-0f3e2142c577>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_query\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         all_words = [ (sentence_arr[query][cnt], bm25Model.get_score(sentence_arr[query], cnt))\n\u001b[0;32m---> 15\u001b[0;31m                      for cnt, i in enumerate(sentenece_arr[query])]\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mall_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentenece_arr' is not defined"
     ]
    }
   ],
   "source": [
    "total_scores = list()\n",
    "for test_id , text_q in enumerate(train_query):\n",
    "    text = [ str(i) for i in pseg.lcut(text_q)]\n",
    "    text = [ t for t in text if t not in stop_words]\n",
    "    scores = np.array(bm25Model.get_scores(text))\n",
    "    top_query = np.argsort(scores)[::-1][:80]\n",
    "    irrelevant_query = np.argsort(scores)[:80]\n",
    "    \n",
    "    print(test_id ,text_q, text)\n",
    "    print(\"top query num {}\".format(len(top_query)))\n",
    "    \n",
    "    \n",
    "    for query in top_query:\n",
    "        all_words = [ (sentence_arr[query][cnt], bm25Model.get_score(sentence_arr[query], cnt))\n",
    "                     for cnt, i in enumerate(sentenece_arr[query])]\n",
    "\n",
    "        all_words = sorted(all_words,key=lambda x:(x[1]))[::-1]\n",
    "        top_words = all_words[:20]\n",
    "        top_words = [x[0] for x in top_words]\n",
    "        text += top_words\n",
    "    scores += 0.3 * np.array(bm25Model.get_scores(text))\n",
    "    \n",
    "    irrelevant = []\n",
    "    for query in irrelevant_query:\n",
    "        all_words = [ (sentence_arr[query][cnt], bm25Model.get_score(sentence_arr[query], cnt))\n",
    "                     for cnt, i in enumerate(sentenece_arr[query])]\n",
    "\n",
    "        all_words = sorted(all_words,key=lambda x:(x[1]))[::-1]\n",
    "        top_words = all_words[:20]\n",
    "        top_words = [x[0] for x in top_words]\n",
    "        irrelevant += top_words\n",
    "    \n",
    "    scores -= 0.3 * np.array(bm25Model.get_scores(irrelevant))\n",
    "#     for idx, rel in (y_index[text_q][0]):\n",
    "#         scores[idx] += np.mean(scores) * idx\n",
    "    \n",
    "    top_num = 300\n",
    "    keys = pd.DataFrame(np.argsort(scores)[::-1][:top_num])\n",
    "    ans = keys[0].apply(lambda x: 'news_{:06d}'.format(x+1))\n",
    "    \n",
    "    validation_score = 0\n",
    "    for a in ans:\n",
    "        if a in y_train[test_id].keys():\n",
    "            validation_score += y_train[test_id][a]\n",
    "            \n",
    "    total_scores.append(validation_score/top_num)\n",
    "    print(\"{} / {} \".format(validation_score,(np.sum(list(y_train[test_id].values())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${\\displaystyle k_{1}\\in [1.2,2.0]}, {\\displaystyle b=0.75}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores = list()\n",
    "for test_id , text_q in enumerate(test_query):\n",
    "    text = jieba.lcut(text_q)\n",
    "    text = [ t for t in text if t not in stop_words]\n",
    "    scores = bm25Model.get_scores(text)\n",
    "    top_query = np.argsort(scores)[::-1][:100]\n",
    "    irrelevant_query = np.argsort(scores)[:200]\n",
    "    \n",
    "    print(test_id ,text_q, text)\n",
    "    print(\"top query num {}\".format(len(top_query)))\n",
    "    \n",
    "    for query in top_query:\n",
    "        all_words = [ (sentence_arr[query][cnt], bm25Model.get_score(sentence_arr[query], cnt))\n",
    "                     for cnt, i in enumerate(sentenece_arr[query])]\n",
    "\n",
    "        all_words = sorted(all_words,key=lambda x:(x[1]))[::-1]\n",
    "        top_words = all_words[:20]\n",
    "        top_words = [x[0] for x in top_words]\n",
    "        text += top_words\n",
    "    scores += 0.3 * np.array(bm25Model.get_scores(text))\n",
    "    \n",
    "    irrelevant = []\n",
    "    for query in irrelevant_query:\n",
    "        all_words = [ (sentence_arr[query][cnt], bm25Model.get_score(sentence_arr[query], cnt))\n",
    "                     for cnt, i in enumerate(sentenece_arr[query])]\n",
    "\n",
    "        all_words = sorted(all_words,key=lambda x:(x[1]))[::-1]\n",
    "        top_words = all_words[:20]\n",
    "        top_words = [x[0] for x in top_words]\n",
    "        irrelevant += top_words\n",
    "    \n",
    "    scores -= 0.3 * np.array(bm25Model.get_scores(irrelevant))\n",
    "    \n",
    "#     scores = bm25Model.get_scores(text)\n",
    "    delta = np.max(scores) \n",
    "    if text_q in y_index.keys():\n",
    "        for idx, rel in (y_index[text_q][0]):\n",
    "            scores[idx] += delta * idx\n",
    "            \n",
    "    total_scores.append(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_num = 300\n",
    "search_result = np.zeros((20,top_num))\n",
    "for cnt,i in enumerate(total_scores):\n",
    "    keys = np.argsort(i)[::-1][:top_num]\n",
    "    search_result[cnt] += keys\n",
    "    \n",
    "search_result = search_result.astype(np.int)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Query_Index'] = ['q_{:02d}'.format(i+1) for i in range(20)]\n",
    "\n",
    "for i in range(top_num):\n",
    "    df['Rank_{:03d}'.format(i+1)] = search_result[:, i] + 1\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    df.iloc[i, 1:] = df.iloc[i, 1:].apply(lambda x: 'news_{:06d}'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "sorted(os.listdir('output/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'simple23.csv'\n",
    "df.to_csv('output/' + fname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# search_result[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
