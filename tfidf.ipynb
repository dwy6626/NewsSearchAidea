{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.276 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from procDataSet import TrainingQuery\n",
    "from bert_serving.client import BertClient\n",
    "from multiprocessing import Pool\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim  as optim\n",
    "import argparse\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.functional import F\n",
    "\n",
    "import os\n",
    "import jieba\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "jieba.load_userdict(os.path.join('data', 'dict.txt.big'))\n",
    "[jieba.add_word(i, freq=None, tag=None) for i in ['不支持','文林苑', '都更案','十八趴','證所稅','前瞻建設']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_num = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/stop_word_zhtw.txt') as file:\n",
    "    data = file.read()\n",
    "    \n",
    "stop_words = data.split('\\n')\n",
    "stop_words += ['「', '」', '，', '\\n', '）', '（', ')', '(']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "folder = 'data/'\n",
    "raw_training_data = pd.read_csv(os.path.join(folder,'TD.csv'))\n",
    "news_urls = pd.read_csv(os.path.join(folder,'NC_1.csv'))\n",
    "contents = pd.read_json(os.path.join(folder,'url2content.json'), typ=pd.Series)\n",
    "\n",
    "## sort the contents by index\n",
    "keys, content_list = contents.keys(), contents.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = np.array(pd.read_csv('./data/QS_1.csv').Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['通姦', '在', '刑法', '上', '應該', '除罪化']\n",
      "['應該', '取消', '機車', '強制', '二段式', '左轉', '(', '待轉', ')']\n",
      "['支持', '博弈', '特區', '在', '台灣', '合法化']\n",
      "['中華', '航空', '空服員', '罷工', '是', '合理', '的']\n",
      "['性交易', '應該', '合法化']\n",
      "['ECFA', '早', '收清單', '可', '（', '有', '）', '達到', '其', '預期', '成效']\n",
      "['應該', '減免', '證所稅']\n",
      "['贊成', '中油', '在', '觀塘', '興建', '第三', '天然氣', '接收站']\n",
      "['支持', '中國', '學生', '納入', '健保']\n",
      "['支持', '臺灣', '中小學', '（', '含', '高職', '、', '專科', '）', '服儀', '規定', '（', '含髮', '、', '襪', '、', '鞋', '）', '給予', '學生', '自主']\n",
      "['不支持', '使用', '加密', '貨幣']\n",
      "['不支持', '學雜費', '調漲']\n",
      "['同意', '政府', '舉債', '發展', '前瞻建設', '計畫']\n",
      "['支持', '電競', '列入', '體育競技']\n",
      "['反對', '台鐵', '東移', '徵收案']\n",
      "['支持', '陳前總統', '保外就醫']\n",
      "['年金', '改革', '應', '取消', '或', '應', '調降', '軍公教', '月', '退之', '優存', '利率', '十八趴']\n",
      "['同意', '動物', '實驗']\n",
      "['油價', '應該', '凍漲', '或', '緩漲']\n",
      "['反對', '旺旺', '中時', '併購', '中嘉']\n"
     ]
    }
   ],
   "source": [
    "for test_id , text_q in enumerate(test_query):\n",
    "    print(jieba.lcut(text_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "NumberCPU = multiprocessing.cpu_count()\n",
    "jieba.initialize()\n",
    "\n",
    "def jbcut(x):\n",
    "    if x is not None:\n",
    "        sen = jieba.lcut(x, cut_all=False)\n",
    "        sen = [i for i in sen if i not in stop_words]\n",
    "        return sen\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def psegcut(x):\n",
    "    if x is not None:\n",
    "        sen = pseg.lcut(x)\n",
    "        sen = [i for i in sen if i not in stop_words]\n",
    "        return sen\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "pool = multiprocessing.Pool(processes=NumberCPU)\n",
    "sentence_arr = pool.map(jbcut,content_list)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_arr = np.load('arr.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## okapi model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import bm25\n",
    "bm25Model = bm25.BM25(sentence_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stands = ['支持', '應該', '反對', '贊成', '不'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_query = list(set(raw_training_data.Query))\n",
    "\n",
    "y_train = []\n",
    "y_index = {}\n",
    "for i  in train_query:\n",
    "    index = np.where(raw_training_data.Query==i)\n",
    "    data = raw_training_data.iloc[index]\n",
    "    y = dict(zip(data.News_Index,data.Relevance))\n",
    "    y_idx = [ (int(idx.split('_')[1])-1, rel )  for idx, rel in zip(data.News_Index, data.Relevance)]\n",
    "    y_train.append(y)\n",
    "    y_index[i] = [y_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_query = set(train_query) & set(test_query)\n",
    "common_query = list(common_query)\n",
    "common = [ train_query.index(i) for i in common_query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[139, 95, 40, 230, 230]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ len(list(y_train[i].values())) for i in common]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6933333333333334,\n",
       " 2.7866666666666666,\n",
       " 2.296666666666667,\n",
       " 0.7066666666666667,\n",
       " 0.23,\n",
       " 1.4466666666666668,\n",
       " 0.24333333333333335,\n",
       " 1.3,\n",
       " 0.14,\n",
       " 1.0533333333333332,\n",
       " 1.3733333333333333,\n",
       " 0.5766666666666667,\n",
       " 0.21333333333333335,\n",
       " 0.2833333333333333,\n",
       " 1.3233333333333333,\n",
       " 1.35,\n",
       " 2.1933333333333334,\n",
       " 0.9433333333333334,\n",
       " 0.77,\n",
       " 2.6466666666666665]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ np.sum(list(y_train[i].values())) / 300 for i in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def histogram_equalization(image, number_bins=256):\n",
    "    # from http://www.janeriksolem.net/2009/06/histogram-equalization-with-python-and.html\n",
    "\n",
    "    # get image histogram\n",
    "    image_histogram, bins = np.histogram(image.flatten(), number_bins, density=True)\n",
    "    cdf = image_histogram.cumsum() # cumulative distribution function\n",
    "    cdf = 100 * cdf / cdf[-1] # normalize\n",
    "\n",
    "    # use linear interpolation of cdf to find new pixel values\n",
    "    image_equalized = np.interp(image.flatten(), bins[:-1], cdf)\n",
    "    return image_equalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([78.19874842, 19.22857733, 50.37420267, ..., 92.65986579,\n",
       "       84.33728214, 66.55927755])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histogram_equalization(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 反對旺旺中時併購中嘉 ['反對', '旺旺', '中時', '併購', '中嘉']\n",
      "top query num 7\n",
      "152 / 208 \n",
      "1 國際賽事會場內應該可以持中華民國國旗 ['國際賽', '事', '會場', '應該', '持', '中華民國', '國旗']\n",
      "top query num 7\n",
      "132 / 836 \n",
      "2 贊同課綱微調 ['贊同', '課綱', '微調']\n",
      "top query num 7\n",
      "210 / 689 \n",
      "3 支持正名「臺灣」參與國際運動賽事 ['支持', '正名', '臺灣', '參與', '國際', '運動', '賽事']\n",
      "top query num 7\n",
      "143 / 212 \n",
      "4 油價應該凍漲或緩漲 ['油價', '應該', '凍漲', '緩漲']\n",
      "top query num 7\n",
      "43 / 69 \n",
      "5 贊成流浪動物零撲殺 ['贊成', '流浪', '動物', '零', '撲殺']\n",
      "top query num 7\n",
      "294 / 434 \n",
      "6 拒絕公投通過門檻下修 ['拒絕', '公投', '門檻', '下修']\n",
      "top query num 7\n",
      "32 / 73 \n",
      "7 反對二代健保規定 ['反對', '二代', '健保', '規定']\n",
      "top query num 7\n",
      "238 / 390 \n",
      "8 同意動物實驗 ['同意', '動物', '實驗']\n",
      "top query num 7\n",
      "42 / 42 \n",
      "9 核四應該啟用 ['核四', '應該', '啟用']\n",
      "top query num 7\n",
      "132 / 316 \n",
      "10 臺灣應開放含瘦肉精(萊克多巴胺)之美國牛肉進口 ['臺灣', '應', '開放', '含', '瘦肉精', '萊克', '多巴胺', '美國', '牛肉', '進口']\n",
      "top query num 7\n",
      "92 / 412 \n",
      "11 十二年國教高中職「免學費補助」適用對象增加是不對的 ['十二年', '國教', '高中職', '免', '學費', '補助', '適用', '對象', '增加', '不對']\n",
      "top query num 7\n",
      "125 / 173 \n",
      "12 反對無圍牆校園 ['反對', '圍牆', '校園']\n",
      "top query num 7\n",
      "59 / 64 \n",
      "13 贊成文林苑都更案可依法拆除王家 ['贊成', '文林苑', '都更案', '依法', '拆除', '王家']\n",
      "top query num 7\n",
      "60 / 85 \n",
      "14 遠雄大巨蛋工程應停工或拆除 ['遠雄', '大巨蛋', '工程', '應', '停工', '拆除']\n",
      "top query num 7\n",
      "208 / 397 \n",
      "15 支持陳前總統保外就醫 ['支持', '陳前總統', '保外就醫']\n",
      "top query num 7\n",
      "137 / 405 \n",
      "16 堅決反對政府舉債發展前瞻建設計畫 ['堅決', '反對', '政府', '舉債', '發展', '前瞻建設', '計畫']\n",
      "top query num 7\n",
      "294 / 658 \n",
      "17 年金改革應取消或應調降軍公教月退之優存利率十八趴 ['年金', '改革', '應', '取消', '應', '調降', '軍公教', '月', '退之', '優存', '利率', '十八趴']\n",
      "top query num 7\n",
      "93 / 283 \n",
      "18 另立專法保障同婚是正確的 ['另立', '專法', '保障', '同婚', '正確']\n",
      "top query num 7\n",
      "124 / 231 \n",
      "19 應該提高酒駕罰責以有效遏制酒駕 ['應該', '提高', '酒駕', '罰責', '有效', '遏制', '酒駕']\n",
      "top query num 7\n",
      "368 / 794 \n"
     ]
    }
   ],
   "source": [
    "total_scores = list()\n",
    "for test_id , text_q in enumerate(train_query):\n",
    "    text = jieba.lcut(text_q)\n",
    "    text = [ t for t in text if t not in stop_words]\n",
    "    \n",
    "    scores = np.array(bm25Model.get_scores(text))\n",
    "    scores = histogram_equalization(scores)\n",
    "    \n",
    "    top_query = np.argsort(scores)[::-1][:5]\n",
    "    irrelevant_query = np.argsort(scores)[:5]\n",
    "    \n",
    "    print(test_id ,text_q, text)\n",
    "    print(\"top query num {}\".format(len(top_query)))\n",
    "    \n",
    "    for query in top_query:\n",
    "        all_words = [ (sentence_arr[query][cnt], bm25Model.get_score(sentence_arr[query], cnt))\n",
    "                     for cnt, i in enumerate(sentence_arr[query])]\n",
    "\n",
    "        all_words = sorted(all_words,key=lambda x:(x[1]))[::-1]\n",
    "        top_words = all_words[:200]\n",
    "        top_words = [x[0] for x in top_words]\n",
    "        text += top_words\n",
    "        \n",
    "    expansion_score = np.array(bm25Model.get_scores(text))\n",
    "    expansion_score = histogram_equalization(expansion_score)\n",
    "    scores += 5 * expansion_score\n",
    "    \n",
    "    top_num = 300\n",
    "    keys = pd.DataFrame(np.argsort(scores)[::-1][:top_num])\n",
    "    ans = keys[0].apply(lambda x: 'news_{:06d}'.format(x+1))\n",
    "    \n",
    "    validation_score = 0\n",
    "    for a in ans:\n",
    "        if a in y_train[test_id].keys():\n",
    "            validation_score += y_train[test_id][a]\n",
    "            \n",
    "    total_scores.append(validation_score/top_num)\n",
    "    print(\"{} / {} \".format(validation_score,(np.sum(list(y_train[test_id].values())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4963333333333334"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(total_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 通姦在刑法上應該除罪化 ['通姦', '刑法', '應該', '除罪化']\n",
      "top query num 5\n",
      "1 應該取消機車強制二段式左轉(待轉) ['應該', '取消', '機車', '強制', '二段式', '左轉', '待轉']\n",
      "top query num 5\n",
      "2 支持博弈特區在台灣合法化 ['支持', '博弈', '特區', '台灣', '合法化']\n",
      "top query num 5\n",
      "3 中華航空空服員罷工是合理的 ['中華', '航空', '空服員', '罷工', '合理']\n",
      "top query num 5\n",
      "4 性交易應該合法化 ['性交易', '應該', '合法化']\n",
      "top query num 5\n",
      "5 ECFA早收清單可（有）達到其預期成效 ['ECFA', '早', '收清單', '達到', '預期', '成效']\n",
      "top query num 5\n",
      "6 應該減免證所稅 ['應該', '減免', '證所稅']\n",
      "top query num 5\n",
      "7 贊成中油在觀塘興建第三天然氣接收站 ['贊成', '中油', '觀塘', '興建', '第三', '天然氣', '接收站']\n",
      "top query num 5\n",
      "8 支持中國學生納入健保 ['支持', '中國', '學生', '納入', '健保']\n",
      "top query num 5\n",
      "9 支持臺灣中小學（含高職、專科）服儀規定（含髮、襪、鞋）給予學生自主 ['支持', '臺灣', '中小學', '含', '高職', '專科', '服儀', '規定', '含髮', '襪', '鞋', '給予', '學生', '自主']\n",
      "top query num 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-b4ee6dcde90d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtop_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mexpansion_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbm25Model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mexpansion_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistogram_equalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpansion_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m6\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mexpansion_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.6/site-packages/gensim/summarization/bm25.py\u001b[0m in \u001b[0;36mget_scores\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \"\"\"\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.6/site-packages/gensim/summarization/bm25.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \"\"\"\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.6/site-packages/gensim/summarization/bm25.py\u001b[0m in \u001b[0;36mget_score\u001b[0;34m(self, document, index)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mdoc_freqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_freqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_freqs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_scores = list()\n",
    "for test_id , text_q in enumerate(test_query):\n",
    "    text = jieba.lcut(text_q)\n",
    "    text = [ t for t in text if t not in stop_words]\n",
    "    \n",
    "    scores = np.array(bm25Model.get_scores(text))\n",
    "    scores = histogram_equalization(scores)\n",
    "    \n",
    "    top_query = np.argsort(scores)[::-1][:5]\n",
    "    irrelevant_query = np.argsort(scores)[:5]\n",
    "    \n",
    "    print(test_id ,text_q, text)\n",
    "    print(\"top query num {}\".format(len(top_query)))\n",
    "    \n",
    "    for query in top_query:\n",
    "        all_words = [ (sentence_arr[query][cnt], bm25Model.get_score(sentence_arr[query], cnt))\n",
    "                     for cnt, i in enumerate(sentence_arr[query])]\n",
    "\n",
    "        all_words = sorted(all_words,key=lambda x:(x[1]))[::-1]\n",
    "        top_words = all_words[:200]\n",
    "        top_words = [x[0] for x in top_words]\n",
    "        text += top_words\n",
    "    \n",
    "    expansion_score = np.array(bm25Model.get_scores(text))\n",
    "    expansion_score = histogram_equalization(expansion_score)\n",
    "    scores += 6 * expansion_score\n",
    "    \n",
    "    top_num = 300\n",
    "    keys = pd.DataFrame(np.argsort(scores)[::-1][:top_num])\n",
    "    ans = keys[0].apply(lambda x: 'news_{:06d}'.format(x+1))\n",
    "    \n",
    "    delta = np.max(scores) \n",
    "    if text_q in y_index.keys():\n",
    "        for idx, rel in (y_index[text_q][0]):\n",
    "            scores[idx] += delta * rel\n",
    "    \n",
    "    total_scores.append(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_num = 300\n",
    "search_result = np.zeros((20,top_num))\n",
    "for cnt,i in enumerate(total_scores):\n",
    "    keys = np.argsort(i)[::-1][:top_num]\n",
    "    search_result[cnt] += keys\n",
    "    \n",
    "search_result = search_result.astype(np.int)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Query_Index'] = ['q_{:02d}'.format(i+1) for i in range(20)]\n",
    "\n",
    "for i in range(top_num):\n",
    "    df['Rank_{:03d}'.format(i+1)] = search_result[:, i] + 1\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    df.iloc[i, 1:] = df.iloc[i, 1:].apply(lambda x: 'news_{:06d}'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'simple35.csv'\n",
    "assert fname not in (os.listdir('output/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output/' + fname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# search_result[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
