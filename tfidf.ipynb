{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from procDataSet import TrainingQuery\n",
    "from bert_serving.client import BertClient\n",
    "from multiprocessing import Pool\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim  as optim\n",
    "import argparse\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.functional import F\n",
    "import os\n",
    "import jieba\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from IPython.display import clear_output\n",
    "\n",
    "jieba.load_userdict(os.path.join('data', 'dict.txt.big'))\n",
    "[jieba.add_word(i, freq=None, tag=None) for i in ['不支持','文林苑', '都更案','十八趴']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_num = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/stop_word.txt') as file:\n",
    "    data = file.read()\n",
    "    \n",
    "stop_words = data.split('\\n')\n",
    "stop_words += ['「', '」', '，', '\\n', '）', '（', ')', '(']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "folder = 'data/'\n",
    "raw_training_data = pd.read_csv(os.path.join(folder,'TD.csv'))\n",
    "news_urls = pd.read_csv(os.path.join(folder,'NC_1.csv'))\n",
    "contents = pd.read_json(os.path.join(folder,'url2content.json'), typ=pd.Series)\n",
    "\n",
    "## sort the contents by index\n",
    "keys, content_list = contents.keys(), contents.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "NumberCPU = multiprocessing.cpu_count()\n",
    "jieba.initialize()\n",
    "\n",
    "def jbcut(x):\n",
    "    if x is not None:\n",
    "        sen = jieba.lcut(x, cut_all=False)\n",
    "        sen = [i for i in sen if i not in stop_words]\n",
    "        return sen\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "pool = multiprocessing.Pool(processes=NumberCPU)\n",
    "sentenece_arr = pool.map(jbcut,content_list)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "batch_size = 100000\n",
    "folder = 'news_data_1/'\n",
    "test_query = np.array(pd.read_csv('./data/QS_1.csv').Query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "def create_dictionaries(p_model):\n",
    "    gensim_dict = Dictionary()\n",
    "    gensim_dict.doc2bow(p_model.vocab.keys(), allow_update=True)\n",
    "    w2indx = {v: k + 1 for k, v in gensim_dict.items()}\n",
    "    w2vec = {word: w2v_model.wv[word] for word in w2indx.keys()}\n",
    "    return w2indx, w2vec\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     size=100,\n",
    "                     workers=cores-1)\n",
    "\n",
    "w2v_model.build_vocab(sentenece_arr, progress_per=10000)\n",
    "w2v_model.train(sentenece_arr, total_examples=w2v_model.corpus_count, epochs=100, report_delay=1)\n",
    "index_dict, word_vectors= create_dictionaries(w2v_model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('優存', 0.8194789290428162)"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similar_by_word('十八趴')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## okapi model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import bm25\n",
    "bm25Model = bm25.BM25(sentenece_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stands = ['支持', '應該', '反對', '贊成', '不'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_query = list(set(raw_training_data.Query))\n",
    "\n",
    "y_train = []\n",
    "y_index = {}\n",
    "for i  in train_query:\n",
    "    index = np.where(raw_training_data.Query==i)\n",
    "    data = raw_training_data.iloc[index]\n",
    "    y = dict(zip(data.News_Index,data.Relevance))\n",
    "    y_idx = [ (int(idx.split('_')[1])-1, rel )  for idx, rel in zip(data.News_Index, data.Relevance)]\n",
    "    y_train.append(y)\n",
    "    y_index[i] = [y_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_query = set(train_query) & set(test_query)\n",
    "common_query = list(common_query)\n",
    "common = [ train_query.index(i) for i in common_query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[139, 40, 230, 95, 230]"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ len(list(y_train[i].values())) for i in common]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6933333333333334,\n",
       " 0.2833333333333333,\n",
       " 1.4466666666666668,\n",
       " 2.296666666666667,\n",
       " 2.6466666666666665,\n",
       " 0.14,\n",
       " 0.7066666666666667,\n",
       " 1.3,\n",
       " 2.7866666666666666,\n",
       " 0.9433333333333334,\n",
       " 0.23,\n",
       " 0.21333333333333335,\n",
       " 0.5766666666666667,\n",
       " 1.3233333333333333,\n",
       " 0.77,\n",
       " 2.1933333333333334,\n",
       " 0.24333333333333335,\n",
       " 1.0533333333333332,\n",
       " 1.3733333333333333,\n",
       " 1.35]"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ np.sum(list(y_train[i].values())) / 300 for i in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6933333333333334, 0.14, 0.9433333333333334, 0.23, 1.35]"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ np.sum(list(y_train[i].values())) / 300 for i in common]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 反對旺旺中時併購中嘉 ['反對', '旺旺', '中時', '併購', '中嘉']\n",
      "top query num 100\n",
      "208 / 208 \n",
      "1 贊成文林苑都更案可依法拆除王家 ['贊成', '文林苑', '都更案', '依法', '拆除', '王家']\n",
      "top query num 100\n",
      "85 / 85 \n",
      "2 贊成流浪動物零撲殺 ['贊成', '流浪', '動物', '零', '撲殺']\n",
      "top query num 100\n",
      "434 / 434 \n",
      "3 贊同課綱微調 ['贊同', '課綱', '微調']\n",
      "top query num 100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-496-46066c94fca7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtop_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbm25Model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_q\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.6/site-packages/gensim/summarization/bm25.py\u001b[0m in \u001b[0;36mget_scores\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \"\"\"\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.6/site-packages/gensim/summarization/bm25.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \"\"\"\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.6/site-packages/gensim/summarization/bm25.py\u001b[0m in \u001b[0;36mget_score\u001b[0;34m(self, document, index)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mdoc_freqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_freqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_freqs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_scores = list()\n",
    "for test_id , text_q in enumerate(train_query):\n",
    "    text = jieba.lcut(text_q)\n",
    "    text = [ t for t in text if t not in stop_words]\n",
    "    scores = bm25Model.get_scores(text)\n",
    "    top_query = np.argsort(scores)[::-1][:100]\n",
    "    \n",
    "    print(test_id ,text_q, text)\n",
    "    print(\"top query num {}\".format(len(top_query)))\n",
    "    \n",
    "#     print(len(text))\n",
    "    sim_words = []\n",
    "    for i in (text):\n",
    "        if i in index_dict:\n",
    "            sim_words += [p[0] for p in w2v_model.wv.similar_by_word(i)[:10]] \n",
    "\n",
    "    text += sim_words\n",
    "#     print(len(text))\n",
    "    \n",
    "    for query in top_query:\n",
    "        all_words = [ (sentenece_arr[query][cnt], bm25Model.get_score(sentenece_arr[query], cnt))\n",
    "                     for cnt, i in enumerate(sentenece_arr[query])]\n",
    "\n",
    "        all_words = sorted(all_words,key=lambda x:(x[1]))[::-1]\n",
    "        top_words = all_words[:20]\n",
    "        top_words = [x[0] for x in top_words]\n",
    "        text += top_words\n",
    "    \n",
    "    scores = bm25Model.get_scores(text)\n",
    "    for idx, rel in (y_index[text_q][0]):\n",
    "        scores[idx] += np.mean(scores) * idx\n",
    "        \n",
    "    keys = pd.DataFrame(np.argsort(scores)[::-1][:top_num])\n",
    "    ans = keys[0].apply(lambda x: 'news_{:06d}'.format(x+1))\n",
    "    \n",
    "    validation_score = 0\n",
    "    for a in ans:\n",
    "        if a in y_train[test_id].keys():\n",
    "            validation_score += y_train[test_id][a]\n",
    "            \n",
    "    total_scores.append(validation_score/top_num)\n",
    "    print(\"{} / {} \".format(validation_score,(np.sum(list(y_train[test_id].values())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${\\displaystyle k_{1}\\in [1.2,2.0]}, {\\displaystyle b=0.75}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 通姦在刑法上應該除罪化 ['通姦', '刑法', '應該', '除罪', '化']\n",
      "top query num 100\n",
      "1 應該取消機車強制二段式左轉(待轉) ['應該', '取消', '機車', '強制', '二段式', '左轉', '待轉']\n",
      "top query num 100\n",
      "2 支持博弈特區在台灣合法化 ['支持', '博弈', '特區', '台灣', '合法化']\n",
      "top query num 100\n",
      "3 中華航空空服員罷工是合理的 ['中華', '航空', '空服員', '罷工', '合理']\n",
      "top query num 100\n",
      "4 性交易應該合法化 ['性交易', '應該', '合法化']\n",
      "top query num 100\n",
      "5 ECFA早收清單可（有）達到其預期成效 ['ECFA', '早收', '清單', '達到', '預期', '成效']\n",
      "top query num 100\n",
      "6 應該減免證所稅 ['應該', '減', '免證', '所稅']\n",
      "top query num 100\n",
      "7 贊成中油在觀塘興建第三天然氣接收站 ['贊成', '中油', '觀塘', '興建', '第三', '天然氣', '接收站']\n",
      "top query num 100\n",
      "8 支持中國學生納入健保 ['支持', '中國', '學生', '納入', '健保']\n",
      "top query num 100\n",
      "9 支持臺灣中小學（含高職、專科）服儀規定（含髮、襪、鞋）給予學生自主 ['支持', '臺灣', '中小學', '含', '高職', '專科', '服儀', '規定', '含', '髮', '襪', '鞋', '給予', '學生', '自主']\n",
      "top query num 100\n",
      "10 不支持使用加密貨幣 ['不支持', '使用', '加密', '貨幣']\n",
      "top query num 100\n",
      "11 不支持學雜費調漲 ['不支持', '學雜費', '調漲']\n",
      "top query num 100\n",
      "12 同意政府舉債發展前瞻建設計畫 ['同意', '政府', '舉債', '發展', '前瞻', '建', '設計', '畫']\n",
      "top query num 100\n",
      "13 支持電競列入體育競技 ['支持', '電競', '列入', '體育競技']\n",
      "top query num 100\n",
      "14 反對台鐵東移徵收案 ['反對', '台鐵', '東移', '徵收', '案']\n",
      "top query num 100\n",
      "15 支持陳前總統保外就醫 ['支持', '陳', '前', '總統', '保外就醫']\n",
      "top query num 100\n",
      "16 年金改革應取消或應調降軍公教月退之優存利率十八趴 ['年金', '改革', '應', '取消', '或應', '調降', '軍公教', '月', '退', '優存', '利率', '十八趴']\n",
      "top query num 100\n",
      "17 同意動物實驗 ['同意', '動物', '實驗']\n",
      "top query num 100\n",
      "18 油價應該凍漲或緩漲 ['油價', '應該', '凍漲', '緩漲']\n",
      "top query num 100\n",
      "19 反對旺旺中時併購中嘉 ['反對', '旺旺', '中時', '併購', '中嘉']\n",
      "top query num 100\n"
     ]
    }
   ],
   "source": [
    "total_scores = list()\n",
    "for test_id , text_q in enumerate(test_query):\n",
    "    text = jieba.lcut(text_q)\n",
    "    text = [ t for t in text if t not in stop_words]\n",
    "    scores = bm25Model.get_scores(text)\n",
    "    top_query = np.argsort(scores)[::-1][:100]\n",
    "    \n",
    "    print(test_id ,text_q, text)\n",
    "    print(\"top query num {}\".format(len(top_query)))\n",
    "    \n",
    "    for query in top_query:\n",
    "        all_words = [ (sentenece_arr[query][cnt], bm25Model.get_score(sentenece_arr[query], cnt))\n",
    "                     for cnt, i in enumerate(sentenece_arr[query])]\n",
    "\n",
    "        all_words = sorted(all_words,key=lambda x:(x[1]))[::-1]\n",
    "        top_words = all_words[:30]\n",
    "        top_words = [x[0] for x in top_words]\n",
    "        text += top_words\n",
    "    \n",
    "    scores = bm25Model.get_scores(text)\n",
    "    delta = np.max(scores) \n",
    "    if text_q in y_index.keys():\n",
    "        for idx, rel in (y_index[text_q][0]):\n",
    "            scores[idx] += delta * idx\n",
    "            \n",
    "    total_scores.append(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_num = 300\n",
    "search_result = np.zeros((20,top_num))\n",
    "for cnt,i in enumerate(total_scores):\n",
    "    keys = np.argsort(i)[::-1][:top_num]\n",
    "    search_result[cnt] += keys\n",
    "    \n",
    "search_result = search_result.astype(np.int)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Query_Index'] = ['q_{:02d}'.format(i+1) for i in range(20)]\n",
    "\n",
    "for i in range(top_num):\n",
    "    df['Rank_{:03d}'.format(i+1)] = search_result[:, i] + 1\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    df.iloc[i, 1:] = df.iloc[i, 1:].apply(lambda x: 'news_{:06d}'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['simple.csv',\n",
       " 'simple10.csv',\n",
       " 'simple11.csv',\n",
       " 'simple12.csv',\n",
       " 'simple13.csv',\n",
       " 'simple14.csv',\n",
       " 'simple15.csv',\n",
       " 'simple2.csv',\n",
       " 'simple3.csv',\n",
       " 'simple4.csv',\n",
       " 'simple5.csv',\n",
       " 'simple8.csv',\n",
       " 'simple9.csv',\n",
       " 'top2000.csv']"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "sorted(os.listdir('output/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'simple18.csv'\n",
    "df.to_csv('output/' + fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "通姦在刑法上應該除罪化\n",
      "54325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'【綜合報導】司改國是會議第五分組昨決議，應廢除《刑法》通姦罪，若無法馬上廢止也要修法規定不能單獨對配偶撤告。此決議昨引爆熱議，司改委員林志潔指出，除罪化能避免遭性侵者憂慮挨告通姦而不敢舉發，避免再有女作家林奕含事件；委員紀惠容也希望「女人不要為難女人」；但台灣女人連線發言人黃淑英痛批，此決議「與民眾期望背道而馳！」通姦罪涉及對家庭的承諾，「外遇的人等同對家庭背信！」\\n通姦是否除罪，法務部四年前曾網路民調，百分之八十五的民眾反對廢除通姦罪，日前輕生的女作家林奕含事件，外界質疑此案可能因擔心師母提告通姦，當年才未舉發老師。而按我《刑法》規定，已婚者與人通姦，跟相姦者均最重處一年徒刑，《刑事訴訟法》另有例外規定，為維護婚姻和諧，可在通姦案中單獨對配偶撤告，僅告相姦者，也就是撤告配偶「只告小三小王條款」。\\n廢除單獨撤告配偶\\n昨通姦除罪化議題成為司改分組討論焦點，與會委員指南韓、日本等國家都陸續廢除通姦罪，委員、勵馨基金會理事長紀惠容指出，根據法務部統計從二○○八年至二○一五年十月間，因通姦和相姦被起訴的女性人數高達二四○九人，佔五十二％，男性則佔四十七％，而女性遭判決有罪者達五十八點四％，因大老婆常對丈夫撤告，委員、律師賴芳玉也指出，此條文「傷害所有的女性」。\\n該分組認為，國內已有相關法律維護婦女遭家暴、保障夫妻財產和婚姻權益，通姦罪的存在愈趨薄弱，且單獨對配偶撤告的但書，導致女性遭處罰者高於男性，另為避免遭受性侵害者因擔心被追究通姦罪，而不敢舉發，因此決議廢除通姦罪，若無法立即廢除，也應該刪除可單獨對配偶撤告的但書規定。法務部昨表示，將研議如何修法，但也提醒目前多數民意支持通姦罪，未來如何說服立委恐是難題。\\n李秀環：結婚無意義\\n《蘋果》昨試訪曾爆出軌的名人看法。先前陷入毆妻卻對新歡邱惠美告白的北市議員童仲彥，昨晚表示「沒想法，歹勢」，已跟童簽字離婚的李秀環受訪則反對通姦除罪化，她表示，若除罪化，結婚已無意義，「配偶今天可以小三、明天可以跟小四在一起，那就不需要對婚姻承諾了。」至於曾有助理小三的北市議員王世堅、傳出過和熟女上摩鐵的名廚阿基師等，昨均未聯繫上，立委吳育昇也不評論此事。\\n婦女新知基金會表示，一向支持通姦除罪，希望立法院盡速修法通過。兩性專家吳娟瑜也認為，除罪化是先進國家趨勢，未來女人不能再依賴別人（指法律）管老公，對此決議她表示「恭喜、支持！」但台灣女人連線發言人黃淑英則痛批，此一決議「與民眾期望背道而馳！」\\n有利學生舉發狼師\\n補習教育全國總會總會長丘昌其認為，通姦除罪化有利受害學生舉發狼師，但不應只關注補教業。網友昨揶揄此決議，「愛偷吃的應該很高興。」但也有認為「《刑法》管家務事本來就不正常。」此外，昨分組另決議，應讓滿十八歲的女性有墮胎自主權、警察可直接拘提剪斷電子腳鐐的假釋性侵犯等。\\n通姦罪撤告表\\n●妻單獨撤告夫\\u3000835件\\n●夫單獨撤告妻\\u3000648件\\n●妻撤告夫與小三\\u30001269件\\n●夫撤告妻與小王\\u3000739件\\n統計數字為2008年到2015年10月\\n資料來源：國是會議會議紀錄\\n通姦應除罪化理由摘要\\n●為了保護弱勢配偶才有通姦罪，但如今已有《家暴防治法》、《性侵害防治法》《民法》也有處理夫妻財產、子女監護相關規範，通姦罪的正當性愈趨薄弱\\n●通姦罪規定告訴人可對配偶單獨撤告，無法發揮懲罰配偶的目的，也導致女性(第三者)因通姦罪成為受刑人數高於男性\\n●性侵害案件若因證據不足無法定罪，恐反被性侵嫌犯的配偶提告通姦，導致受害人不敢舉發遭受性侵\\n資料來源：《蘋果》採訪整理\\n各國通姦除罪概況\\n●無罪\\n˙日本(1947年廢除)\\n˙義大利(1969年廢除)\\n˙德國(1970年廢除)\\n˙法國(1975年廢除)\\n˙韓國(2015年廢除)\\n˙奧地利(1997年廢除)\\n˙中國、香港、新加坡、挪威、瑞典、丹麥(均無通姦罪)\\n●有罪\\n˙台灣(1年以下徒刑)\\n˙印尼(1年以下徒刑)\\n˙印度(5年以下徒刑)\\n˙沙烏地阿拉伯、埃及、蘇丹、土耳其等國\\n●部分無罪\\n˙美國喬治亞洲等20多州除罪，另20多州有罪\\n資料來源：法務部、《蘋果》採訪整理'"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "print(test_query[i])\n",
    "idx = search_result[i][0]\n",
    "print(idx)\n",
    "content_list[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# search_result[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
