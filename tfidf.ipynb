{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from procDataSet import TrainingQuery\n",
    "from bert_serving.client import BertClient\n",
    "from multiprocessing import Pool\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim  as optim\n",
    "import argparse\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.functional import F\n",
    "import os\n",
    "import jieba\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "folder = 'data/'\n",
    "raw_training_data = pd.read_csv(os.path.join(folder,'TD.csv'))\n",
    "news_urls = pd.read_csv(os.path.join(folder,'NC_1.csv'))\n",
    "contents = pd.read_json(os.path.join(folder,'url2content.json'), typ=pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, content_list = contents.keys(), contents.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doc = content_list# (train_dataset.contents)\n",
    "sentenece_arr = [ jieba.lcut(sen) for sen in all_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "word_list = sentenece_arr\n",
    "dictionary = corpora.Dictionary(word_list)\n",
    "new_corpus = [ dictionary.doc2bow(text) for text in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dict = {}\n",
    "for idx, text in enumerate(new_corpus):\n",
    "    all_words = ([i[0] for i in text])\n",
    "    \n",
    "    for word in all_words:\n",
    "        if word in doc_dict.keys():\n",
    "            doc_dict[word].append(idx)\n",
    "        else:\n",
    "            doc_dict[word]= [idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "batch_size = 100000\n",
    "folder = 'news_data_1/'\n",
    "test_query = np.array(pd.read_csv('./data/QS_1.csv').Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores = list()\n",
    "\n",
    "for test_id , text_q in enumerate(test_query):\n",
    "    print( test_id ,text_q)\n",
    "    text = jieba.lcut(text_q)\n",
    "    text_bow = dictionary.doc2bow(text) \n",
    "    keys, values = zip(*tfidf[text_bow])\n",
    "    \n",
    "    for cnt, key in enumerate(keys):\n",
    "        if cnt == 0:\n",
    "            queried_doc = set(doc_dict[key])\n",
    "        else:\n",
    "            queried_doc = set(doc_dict[key]) | queried_doc\n",
    "    \n",
    "    cosine_sim_score = list()\n",
    "    for cnt, doc in enumerate(queried_doc):\n",
    "        tfidf_score = np.zeros((len(keys)))\n",
    "        result = sentenece_arr[doc]\n",
    "        result = dictionary.doc2bow(result)\n",
    "        result_dict = dict(tfidf[result])\n",
    "        \n",
    "        for idx, i in enumerate(keys):\n",
    "            if i in result_dict.keys():\n",
    "                tfidf_score[idx] = result_dict[i]\n",
    "        \n",
    "        tfidf_score, values  = np.array(tfidf_score).reshape(1,-1), np.array(values).reshape(1,-1)\n",
    "        score = cosine_similarity(tfidf_score, values)\n",
    "        cosine_sim_score.append((doc, score[0][0]))\n",
    "        \n",
    "    total_scores.append(cosine_sim_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result = np.zeros((20,300))\n",
    "for cnt,i in enumerate(total_scores):\n",
    "    keys ,values = zip(*sorted(i, key= lambda x: x[1])[::-1][:300])\n",
    "    search_result[cnt] = keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result=search_result.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = results.reshape(20, -1)\n",
    "# search_result = np.flip(np.argsort(results, axis=1), axis=1)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Query_Index'] = ['q_{:02d}'.format(i+1) for i in range(20)]\n",
    "\n",
    "for i in range(300):\n",
    "    df['Rank_{:03d}'.format(i+1)] = search_result[:, i]\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    df.iloc[i, 1:] = df.iloc[i, 1:].apply(lambda x: 'news_{:06d}'.format(x))\n",
    "    \n",
    "fname = 'simple7.csv'\n",
    "df.to_csv('output/' + fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'有民眾於國發會的公共政策網路參與平臺提議「提升全民行車觀念，以提升全民行車環境品質，解除禁行機慢車道，解除強制二段式左轉」，交通部認為涉及地方政府道路交通工程配置規畫事宜，經函徵各地方政府意見，多數認不宜貿然廢除，應尊重地方政府的權責。\\n交通部表示，因涉地方政府交通管理規畫權責，函徵全國各縣市政府、內政部警政署、公路總局及運輸研究所等單位意見，並於5月26日召開會議，邀請提案人及相關單位共同討論。中央主管機關應尊重各地道路主管機關係基於安全及效率之交通管理規劃。鑑於現行法規已留給地方政府彈性管理之空間，應尚無需研修相關法規，地方政府可依現場道路交通環境情況，因地制宜綜合評估是否設置或取消禁行機車道及兩段式左轉相關標誌標線。\\n目前已有多個縣市開始試辦及追蹤實施後之事故改善情形，並逐步擴大檢討，例如臺北市在42處路段試辦取消第三車道禁行機車，結果25處事故改善，17處惡化，因沒有顯著效果，目前實作仍以個案方式檢討施行。\\n交通部運輸研究所亦已就「路段共享道路空間」及「路口減少轉向衝突」之改善目標，研提具安全性及全國性可適用之規劃原則可供各縣市政府參考，該部將督同公路總局及各直轄市、縣（市）政府因地制宜檢討各級道路配置，以提供更友善的交通環境供不同族群用路人合理使用道路。\\n(中時 )'"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx=search_result[1][0]\n",
    "content_list[search_result[1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'喧騰甚久的陳水扁保外就醫案雖然歷經一波三折，但終究隨著阿扁獲准保外就醫並回到家中，似乎暫告一個段落，但是否真能讓台灣社會回歸平靜，進而藍綠走向和解，關鍵還在於陳水扁自己，他若能做到諸法皆空，才會真的自由自在。\\n曾有民進黨人形容阿扁是「標準的政治動物」，他的興趣不多，也少有休閒娛樂，腦中思考的、心中所想的都是政治上的折衝協調、縱橫捭闔，不折不扣的政治人。\\n所以，陳水扁在剛入監但病情尚未惡化之前，雖然人在坐牢，卻時時與外界的政治情勢相連結，三不五時透過探監的人傳達阿扁的政治判斷或評論，有時還對民進黨下起指導棋，讓不少人深感阿扁的不甘寂寞。\\n基於人道與台灣社會的和諧，社會超越藍綠的獲致最大公約數，多數贊成讓阿扁保外就醫，但阿扁現在也應思考，從總統之尊的天堂到階下囚的地獄，現又再回到溫暖的家中，他是否真能像聖嚴法師說的「放下它」，才是核心。\\n當阿扁心中能夠真正放下，才能真正得到解脫，台灣社會方能放下這個桎梏。\\n(旺報)'"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents[news_urls.values[idx][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with  open('test.pickle') as file:\n",
    "    pickle.dump(, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
