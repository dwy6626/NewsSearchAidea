{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import procDataSet\n",
    "\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format\n",
    "np.set_printoptions(linewidth=140)\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{:>6.2f}\".format(x)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        n_hidden = 4000\n",
    "        dense1 = nn.Linear(768 + 768, n_hidden)\n",
    "        dense2 = nn.Linear(n_hidden, n_hidden)\n",
    "        dense3 = nn.Linear(n_hidden, n_hidden // 2)\n",
    "        dense4 = nn.Linear(n_hidden // 2, 1)\n",
    "\n",
    "        self.DNN = nn.Sequential(\n",
    "            dense1,\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.Dropout(.4),\n",
    "            nn.PReLU(),\n",
    "\n",
    "            dense2,\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.Dropout(.4),\n",
    "            nn.PReLU(),\n",
    "\n",
    "            dense3,\n",
    "            nn.BatchNorm1d(n_hidden // 2),\n",
    "            nn.Dropout(.4),\n",
    "            nn.PReLU(),\n",
    "            \n",
    "            dense4,\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # for l2 regularization\n",
    "        self.regularizations = [dense1.weight, dense2.weight]\n",
    "\n",
    "    \n",
    "    def forward(self, query, text): \n",
    "        x = torch.cat([query, text], dim=1)\n",
    "        return self.DNN(x)\n",
    "\n",
    "def regularization(weights, alpha):\n",
    "    rt = 0\n",
    "    for w in weights:\n",
    "        rt += torch.sum(torch.pow(w, 2))\n",
    "    return alpha * rt\n",
    "\n",
    "\n",
    "def weighted_mse_loss(y_pred, target, weight):\n",
    "    return torch.sum(weight * (y_pred - target) ** 2) / torch.sum(weight)\n",
    "\n",
    "\n",
    "def train(\n",
    "    model, optimizer, train_loader, valid_loader, num_epochs, \n",
    "#     job_name, \n",
    "    scheduler=None,\n",
    "    early_stop=True, no_improve_epochs=15, threshold=1e-3,\n",
    "    alpha=0, grad_clip=1000\n",
    "):\n",
    "#     train_hist = []\n",
    "    last_epoch_tune_lr = 0\n",
    "    valid_tag = valid_loader is not None\n",
    "    best_acc = .7  # it is a threshold\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = valid_loss = 0.\n",
    "        regular = 0.\n",
    "        for q, c, y, w in train_loader:\n",
    "            q, c = q.cuda(), c.cuda()\n",
    "            y, w = y.cuda(), w.cuda()\n",
    "            outputs = model(q, c)\n",
    "#             loss = Criticizer(outputs, y.float())\n",
    "            loss = weighted_mse_loss(outputs, y.float(), w)\n",
    "            train_loss += loss.item()\n",
    "            regular_batch = regularization(model.regularizations, alpha)\n",
    "            loss += regular_batch\n",
    "            regular += regular_batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if grad_clip:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "#             print('   Batch, loss: {:.4f}, l1_regur: {:.4f}, total: {:.4f}'.format(\n",
    "#                 loss.item(), regular_batch, loss.item() + regular_batch\n",
    "#             ))\n",
    "\n",
    "        train_loss /= len(train_loader)     \n",
    "        regular /= len(train_loader)    \n",
    "        total_loss = regular+train_loss\n",
    "        report = 'Epoch [{}/{}], loss: {:.4f}, l1_regur: {:.4f}, total: {:.4f}'.format(\n",
    "            epoch+1, num_epochs, train_loss, regular, total_loss\n",
    "        )\n",
    "#         train_hist.append((train_loss, regular, total_loss))\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(total_loss)\n",
    "        \n",
    "        if valid_tag:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for q, c, y, w in valid_loader:\n",
    "                    q, c = q.cuda(), c.cuda()\n",
    "                    y, w = y.cuda(), w.cuda()\n",
    "                    outputs = model(q, c)\n",
    "    #                 loss = Criticizer(outputs, y.float())\n",
    "                    loss = weighted_mse_loss(outputs, y.float(), w)\n",
    "                    valid_loss += loss.item()\n",
    "\n",
    "            valid_loss /= len(valid_loader)\n",
    "            report += ', valid loss: {:.4f}'.format(valid_loss)\n",
    "#             train_hist[2:, epoch] = valid_loss, valid_acc\n",
    "        print(report)      \n",
    "#         if early_stop and epoch > no_improve_epochs + scheduler.last_epoch and np.amin(train_hist[:-no_improve_epochs]) + threshold < np.amin(train_hist[-no_improve_epochs:]):\n",
    "#             print('Trigger early stop.')\n",
    "#             break\n",
    "\n",
    "#     train_hist_fname = os.path.join('output', job_name + '_hist')\n",
    "#     print('save training history to {}.npy'.format(train_hist_fname))\n",
    "#     np.save(train_hist_fname, train_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30000\n",
    "NumberCPU = multiprocessing.cpu_count()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    procDataSet.TrainingQueryAll(validation_queries=[0], normalize01=True, weight=400), \n",
    "    batch_size=batch_size, shuffle=True, num_workers=NumberCPU\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    procDataSet.TrainingQueryAll(validation_queries=[0], normalize01=True, weight=400, is_valid=True), \n",
    "    batch_size=batch_size, shuffle=False, num_workers=NumberCPU\n",
    ")\n",
    "\n",
    "model = SimpleModel().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], loss: 0.1068, l1_regur: 0.0000, total: 0.1068, valid loss: 0.2268\n",
      "Epoch [2/20], loss: 0.1073, l1_regur: 0.0000, total: 0.1073, valid loss: 0.2268\n",
      "Epoch [3/20], loss: 0.1068, l1_regur: 0.0000, total: 0.1068, valid loss: 0.2272\n",
      "Epoch [4/20], loss: 0.1068, l1_regur: 0.0000, total: 0.1068, valid loss: 0.2275\n",
      "Epoch [5/20], loss: 0.1069, l1_regur: 0.0000, total: 0.1069, valid loss: 0.2274\n",
      "Epoch [6/20], loss: 0.1065, l1_regur: 0.0000, total: 0.1065, valid loss: 0.2278\n",
      "Epoch [7/20], loss: 0.1066, l1_regur: 0.0000, total: 0.1066, valid loss: 0.2278\n",
      "Epoch [8/20], loss: 0.1069, l1_regur: 0.0000, total: 0.1069, valid loss: 0.2279\n",
      "Epoch [9/20], loss: 0.1076, l1_regur: 0.0000, total: 0.1076, valid loss: 0.2290\n",
      "Epoch [10/20], loss: 0.1067, l1_regur: 0.0000, total: 0.1067, valid loss: 0.2285\n",
      "Epoch [11/20], loss: 0.1063, l1_regur: 0.0000, total: 0.1063, valid loss: 0.2290\n",
      "Epoch [12/20], loss: 0.1069, l1_regur: 0.0000, total: 0.1069, valid loss: 0.2296\n",
      "Epoch [13/20], loss: 0.1068, l1_regur: 0.0000, total: 0.1068, valid loss: 0.2286\n",
      "Epoch [14/20], loss: 0.1070, l1_regur: 0.0000, total: 0.1070, valid loss: 0.2294\n",
      "Epoch [15/20], loss: 0.1062, l1_regur: 0.0000, total: 0.1062, valid loss: 0.2299\n",
      "Epoch [16/20], loss: 0.1067, l1_regur: 0.0000, total: 0.1067, valid loss: 0.2298\n",
      "Epoch [17/20], loss: 0.1069, l1_regur: 0.0000, total: 0.1069, valid loss: 0.2305\n",
      "Epoch [18/20], loss: 0.1065, l1_regur: 0.0000, total: 0.1065, valid loss: 0.2305\n",
      "Epoch [19/20], loss: 0.1067, l1_regur: 0.0000, total: 0.1067, valid loss: 0.2308\n",
      "Epoch [20/20], loss: 0.1064, l1_regur: 0.0000, total: 0.1064, valid loss: 0.2309\n"
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, amsgrad=True)\n",
    "\n",
    "# scheduler = ReduceLROnPlateau(\n",
    "#     optimizer, 'min', verbose=True,\n",
    "#     patience=2, factor=.1**.5, min_lr=1e-5, threshold=0, cooldown=15\n",
    "# )\n",
    "\n",
    "train(\n",
    "    model, optimizer, train_loader, valid_loader, 20,\n",
    "    threshold=3e-4, alpha=0, grad_clip=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], loss: 0.1059, l1_regur: 0.0286, total: 0.1345, valid loss: 0.2339\n",
      "Epoch [2/20], loss: 0.1061, l1_regur: 0.0281, total: 0.1342, valid loss: 0.2345\n",
      "Epoch [3/20], loss: 0.1061, l1_regur: 0.0276, total: 0.1337, valid loss: 0.2340\n",
      "Epoch [4/20], loss: 0.1059, l1_regur: 0.0272, total: 0.1330, valid loss: 0.2337\n",
      "Epoch [5/20], loss: 0.1059, l1_regur: 0.0268, total: 0.1326, valid loss: 0.2352\n",
      "Epoch [6/20], loss: 0.1056, l1_regur: 0.0264, total: 0.1320, valid loss: 0.2349\n",
      "Epoch [7/20], loss: 0.1053, l1_regur: 0.0260, total: 0.1313, valid loss: 0.2350\n",
      "Epoch [8/20], loss: 0.1058, l1_regur: 0.0256, total: 0.1314, valid loss: 0.2359\n",
      "Epoch [9/20], loss: 0.1054, l1_regur: 0.0253, total: 0.1306, valid loss: 0.2351\n",
      "Epoch [10/20], loss: 0.1062, l1_regur: 0.0249, total: 0.1311, valid loss: 0.2355\n",
      "Epoch [11/20], loss: 0.1052, l1_regur: 0.0246, total: 0.1298, valid loss: 0.2351\n",
      "Epoch [12/20], loss: 0.1057, l1_regur: 0.0243, total: 0.1300, valid loss: 0.2353\n",
      "Epoch [13/20], loss: 0.1059, l1_regur: 0.0240, total: 0.1299, valid loss: 0.2358\n",
      "Epoch [14/20], loss: 0.1060, l1_regur: 0.0237, total: 0.1297, valid loss: 0.2363\n",
      "Epoch [15/20], loss: 0.1058, l1_regur: 0.0234, total: 0.1292, valid loss: 0.2369\n",
      "Epoch [16/20], loss: 0.1056, l1_regur: 0.0232, total: 0.1287, valid loss: 0.2361\n",
      "Epoch [17/20], loss: 0.1058, l1_regur: 0.0229, total: 0.1287, valid loss: 0.2364\n",
      "Epoch [18/20], loss: 0.1051, l1_regur: 0.0227, total: 0.1278, valid loss: 0.2379\n",
      "Epoch [19/20], loss: 0.1049, l1_regur: 0.0224, total: 0.1273, valid loss: 0.2378\n",
      "Epoch [20/20], loss: 0.1049, l1_regur: 0.0222, total: 0.1271, valid loss: 0.2374\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model, optimizer, train_loader, valid_loader, 20,\n",
    "    threshold=3e-4, alpha=1e-5, grad_clip=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], loss: 0.1119, l1_regur: 0.0138, total: 0.1257\n",
      "Epoch [2/50], loss: 0.1115, l1_regur: 0.0138, total: 0.1252\n",
      "Epoch [3/50], loss: 0.1108, l1_regur: 0.0137, total: 0.1246\n",
      "Epoch [4/50], loss: 0.1095, l1_regur: 0.0137, total: 0.1232\n",
      "Epoch [5/50], loss: 0.1097, l1_regur: 0.0137, total: 0.1233\n",
      "Epoch [6/50], loss: 0.1092, l1_regur: 0.0136, total: 0.1228\n",
      "Epoch [7/50], loss: 0.1090, l1_regur: 0.0136, total: 0.1226\n",
      "Epoch [8/50], loss: 0.1095, l1_regur: 0.0136, total: 0.1231\n",
      "Epoch [9/50], loss: 0.1088, l1_regur: 0.0135, total: 0.1223\n",
      "Epoch [10/50], loss: 0.1089, l1_regur: 0.0135, total: 0.1224\n",
      "Epoch [11/50], loss: 0.1085, l1_regur: 0.0135, total: 0.1220\n",
      "Epoch [12/50], loss: 0.1076, l1_regur: 0.0135, total: 0.1211\n",
      "Epoch [13/50], loss: 0.1074, l1_regur: 0.0134, total: 0.1209\n",
      "Epoch [14/50], loss: 0.1074, l1_regur: 0.0134, total: 0.1208\n",
      "Epoch [15/50], loss: 0.1066, l1_regur: 0.0134, total: 0.1200\n",
      "Epoch [16/50], loss: 0.1067, l1_regur: 0.0134, total: 0.1200\n",
      "Epoch [17/50], loss: 0.1063, l1_regur: 0.0133, total: 0.1196\n",
      "Epoch [18/50], loss: 0.1063, l1_regur: 0.0133, total: 0.1196\n",
      "Epoch [19/50], loss: 0.1060, l1_regur: 0.0133, total: 0.1193\n",
      "Epoch [20/50], loss: 0.1053, l1_regur: 0.0133, total: 0.1185\n",
      "Epoch [21/50], loss: 0.1054, l1_regur: 0.0132, total: 0.1187\n",
      "Epoch [22/50], loss: 0.1054, l1_regur: 0.0132, total: 0.1186\n",
      "Epoch [23/50], loss: 0.1045, l1_regur: 0.0132, total: 0.1177\n",
      "Epoch [24/50], loss: 0.1053, l1_regur: 0.0132, total: 0.1184\n",
      "Epoch [25/50], loss: 0.1040, l1_regur: 0.0132, total: 0.1171\n",
      "Epoch [26/50], loss: 0.1041, l1_regur: 0.0132, total: 0.1173\n",
      "Epoch [27/50], loss: 0.1034, l1_regur: 0.0131, total: 0.1165\n",
      "Epoch [28/50], loss: 0.1031, l1_regur: 0.0131, total: 0.1162\n",
      "Epoch [29/50], loss: 0.1028, l1_regur: 0.0131, total: 0.1159\n",
      "Epoch [30/50], loss: 0.1031, l1_regur: 0.0131, total: 0.1162\n",
      "Epoch [31/50], loss: 0.1028, l1_regur: 0.0131, total: 0.1159\n",
      "Epoch [32/50], loss: 0.1023, l1_regur: 0.0131, total: 0.1153\n",
      "Epoch [33/50], loss: 0.1023, l1_regur: 0.0130, total: 0.1154\n",
      "Epoch [34/50], loss: 0.1016, l1_regur: 0.0130, total: 0.1146\n",
      "Epoch [35/50], loss: 0.1019, l1_regur: 0.0130, total: 0.1149\n",
      "Epoch [36/50], loss: 0.1007, l1_regur: 0.0130, total: 0.1137\n",
      "Epoch [37/50], loss: 0.1013, l1_regur: 0.0130, total: 0.1142\n",
      "Epoch [38/50], loss: 0.1010, l1_regur: 0.0130, total: 0.1140\n",
      "Epoch [39/50], loss: 0.1001, l1_regur: 0.0130, total: 0.1131\n",
      "Epoch [40/50], loss: 0.0993, l1_regur: 0.0129, total: 0.1122\n",
      "Epoch [41/50], loss: 0.1000, l1_regur: 0.0129, total: 0.1129\n",
      "Epoch [42/50], loss: 0.0995, l1_regur: 0.0129, total: 0.1124\n",
      "Epoch [43/50], loss: 0.0993, l1_regur: 0.0129, total: 0.1122\n",
      "Epoch [44/50], loss: 0.0990, l1_regur: 0.0129, total: 0.1119\n",
      "Epoch [45/50], loss: 0.0985, l1_regur: 0.0129, total: 0.1114\n",
      "Epoch [46/50], loss: 0.0980, l1_regur: 0.0129, total: 0.1108\n",
      "Epoch [47/50], loss: 0.0987, l1_regur: 0.0129, total: 0.1116\n",
      "Epoch [48/50], loss: 0.0975, l1_regur: 0.0129, total: 0.1103\n",
      "Epoch [49/50], loss: 0.0965, l1_regur: 0.0128, total: 0.1093\n",
      "Epoch [50/50], loss: 0.0976, l1_regur: 0.0128, total: 0.1104\n"
     ]
    }
   ],
   "source": [
    "# train w/o validation\n",
    "torch.save(model.state_dict(), 'simple_re.pth')\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    procDataSet.TrainingQueryAll(normalize01=True, weight=400, validation_queries=[]), \n",
    "    batch_size=batch_size, shuffle=True, num_workers=NumberCPU\n",
    ")\n",
    "train(\n",
    "    model, optimizer, train_loader, None, 50,\n",
    "    threshold=3e-4, alpha=1e-5, grad_clip=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100000\n",
    "test_loader = DataLoader(procDataSet.TestQuery(), batch_size=batch_size, shuffle=False, num_workers=NumberCPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "model.eval()\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    for q, c in test_loader:\n",
    "        q, c = q.cuda(), c.cuda()\n",
    "        outputs = model(q, c)\n",
    "        results.append(outputs.cpu().data.numpy())\n",
    "        i += 1\n",
    "results = np.concatenate(results, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.reshape(20, -1)\n",
    "search_result = np.flip(np.argsort(results, axis=1), axis=1)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Query_Index'] = ['q_{:02d}'.format(i+1) for i in range(20)]\n",
    "\n",
    "for i in range(300):\n",
    "    df['Rank_{:03d}'.format(i+1)] = search_result[:, i]\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    df.iloc[i, 1:] = df.iloc[i, 1:].apply(lambda x: 'news_{:06d}'.format(x))\n",
    "fname = 'simple.csv'\n",
    "df.to_csv('output/' + fname,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_show = results.reshape(20, -1) * 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.17,   0.29,   0.42, ...,   0.56,   1.03,   1.14],\n",
       "       [  1.19,   0.84,   0.46, ...,   1.38,   1.19,   1.38],\n",
       "       [  0.78,   0.77,   0.14, ...,   1.52,   0.55,   1.04],\n",
       "       ...,\n",
       "       [  0.43,   0.04,   0.09, ...,   0.11,   0.38,   0.43],\n",
       "       [  0.23,   0.04,   0.06, ...,   0.27,   0.17,   0.20],\n",
       "       [  0.38,   0.17,   0.09, ...,   0.82,   1.04,   0.51]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "651141"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(results_show > 1.2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected news : 2538\n",
      "不重複：2538\n",
      "和非0不重複：2362\n"
     ]
    }
   ],
   "source": [
    "a = set(df.iloc[:, 1:].values.flat)\n",
    "print('selected news : {}'.format(len(a)))\n",
    "td = pd.read_csv('data/TD.csv')\n",
    "# td = td.iloc[230:]\n",
    "b = set(td[td['Relevance'] != 0]['News_Index'])\n",
    "print('不重複：{}'.format(len(a - set(td['Relevance']))))\n",
    "print('和非0不重複：{}'.format(len(a - b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.22,   2.12,   2.10, ...,   0.00,   0.00,   0.00],\n",
       "       [  2.13,   2.13,   2.12, ...,   0.01,   0.01,   0.00],\n",
       "       [  2.27,   2.26,   2.21, ...,   0.01,   0.01,   0.00],\n",
       "       ...,\n",
       "       [  2.17,   2.10,   2.09, ...,   0.00,   0.00,   0.00],\n",
       "       [  0.94,   0.94,   0.91, ...,   0.00,   0.00,   0.00],\n",
       "       [  2.56,   2.53,   2.53, ...,   0.00,   0.00,   0.00]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.flip(np.sort(results, axis=1), axis=1) * 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2019",
   "language": "python",
   "name": "ml2019"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
