{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import procDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        n_hidden = 1000\n",
    "        dense1 = nn.Linear(768 + 768, n_hidden)\n",
    "        dense2 = nn.Linear(n_hidden, n_hidden // 2)\n",
    "        dense3 = nn.Linear(n_hidden // 2, 1)\n",
    "\n",
    "        self.DNN = nn.Sequential(\n",
    "            dense1,\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.Dropout(.4),\n",
    "            nn.SELU(),\n",
    "\n",
    "            dense2,\n",
    "            nn.BatchNorm1d(n_hidden // 2),\n",
    "            nn.Dropout(.4),\n",
    "            nn.SELU(),\n",
    "            \n",
    "            dense3,\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # for l2 regularization\n",
    "        self.regularizations = [dense1.weight, dense2.weight]\n",
    "\n",
    "    \n",
    "    def forward(self, query, text): \n",
    "        x = torch.cat([query, text], dim=1)\n",
    "        return self.DNN(x)\n",
    "\n",
    "def regularization(weights, alpha):\n",
    "    rt = 0\n",
    "    for w in weights:\n",
    "        rt += torch.sum(torch.abs(w))\n",
    "    return alpha * rt\n",
    "\n",
    "def train(\n",
    "    model, optimizer, scheduler, train_loader, valid_loader, num_epochs, \n",
    "#     job_name, \n",
    "    early_stop=True, no_improve_epochs=15, threshold=1e-3,\n",
    "    alpha=0, grad_clip=1000\n",
    "):\n",
    "#     train_hist = []\n",
    "    last_epoch_tune_lr = 0\n",
    "    valid_tag = valid_loader is not None\n",
    "    best_acc = .7  # it is a threshold\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = valid_loss = 0.\n",
    "        regular = 0.\n",
    "        for q, c, y in train_loader:\n",
    "            q, c = q.cuda(), c.cuda()\n",
    "            y = y.cuda()\n",
    "            outputs = model(q, c)\n",
    "            loss = loss_func(outputs, y.float())\n",
    "            train_loss += loss.item()\n",
    "            regular_batch = regularization(model.regularizations, alpha)\n",
    "            loss += regular_batch\n",
    "            regular += regular_batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss /= len(train_loader)     \n",
    "        regular /= len(train_loader)    \n",
    "        total_loss = regular+train_loss\n",
    "        report = 'Epoch [{}/{}], loss: {:.4f}, l1_regur: {:.4f}, total: {:.4f}'.format(\n",
    "            epoch+1, num_epochs, train_loss, regular, total_loss\n",
    "        )\n",
    "#         train_hist.append((train_loss, regular, total_loss))\n",
    "        \n",
    "        scheduler.step(total_loss)\n",
    "        \n",
    "        if valid_tag:\n",
    "            model.eval()\n",
    "            for q, c, y in valid_loader:\n",
    "                q, c = q.cuda(), c.cuda()\n",
    "                y = y.cuda()\n",
    "                outputs = model(q, c)\n",
    "                loss = loss_func(outputs, y.float())\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "            valid_loss /= len(valid_loader)\n",
    "            report += ', valid loss: {:.4f}'.format(valid_loss)\n",
    "#             train_hist[2:, epoch] = valid_loss, valid_acc\n",
    "        print(report)      \n",
    "        if early_stop and epoch > no_improve_epochs + scheduler.last_epoch and np.amin(train_hist[:-no_improve_epochs]) + threshold < np.amin(train_hist[-no_improve_epochs:]):\n",
    "            print('Trigger early stop.')\n",
    "            break\n",
    "\n",
    "#     train_hist_fname = os.path.join('output', job_name + '_hist')\n",
    "#     print('save training history to {}.npy'.format(train_hist_fname))\n",
    "#     np.save(train_hist_fname, train_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 18000\n",
    "NumberCPU = multiprocessing.cpu_count()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    procDataSet.TrainingQueryAll(validation_queries=[0], normalize01=True), \n",
    "    batch_size=batch_size, shuffle=True, num_workers=NumberCPU\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    procDataSet.TrainingQueryAll(validation_queries=[0], normalize01=True, is_valid=True), \n",
    "    batch_size=batch_size, shuffle=False, num_workers=NumberCPU\n",
    ")\n",
    "\n",
    "model = SimpleModel().cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer, 'min', verbose=True,\n",
    "    patience=2, factor=.1**.5, min_lr=1e-5, threshold=0, cooldown=15\n",
    ")\n",
    "\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], loss: 0.0818, l1_regur: 0.1196, total: 0.2014, valid loss: 0.0011\n",
      "Epoch [2/40], loss: 0.0587, l1_regur: 0.0479, total: 0.1066, valid loss: 0.0011\n",
      "Epoch [3/40], loss: 0.0532, l1_regur: 0.0118, total: 0.0650, valid loss: 0.0011\n",
      "Epoch [4/40], loss: 0.0594, l1_regur: 0.0607, total: 0.1201, valid loss: 0.0011\n",
      "Epoch [5/40], loss: 0.0532, l1_regur: 0.0148, total: 0.0680, valid loss: 0.0011\n",
      "Epoch [6/40], loss: 0.0532, l1_regur: 0.0052, total: 0.0584, valid loss: 0.0011\n",
      "Epoch [7/40], loss: 0.0725, l1_regur: 0.0840, total: 0.1566, valid loss: 0.0011\n",
      "Epoch [8/40], loss: 0.0007, l1_regur: 0.0793, total: 0.0800, valid loss: 0.0011\n",
      "Epoch [9/40], loss: 0.0007, l1_regur: 0.0267, total: 0.0274, valid loss: 0.0011\n",
      "Epoch [10/40], loss: 0.0007, l1_regur: 0.0101, total: 0.0108, valid loss: 0.0011\n",
      "Epoch [11/40], loss: 0.0008, l1_regur: 0.0132, total: 0.0139, valid loss: 0.0011\n",
      "Epoch [12/40], loss: 0.0007, l1_regur: 0.0106, total: 0.0113, valid loss: 0.0011\n",
      "Epoch    12: reducing learning rate of group 0 to 3.1623e-04.\n",
      "Epoch [13/40], loss: 0.0008, l1_regur: 0.0200, total: 0.0208, valid loss: 0.0011\n",
      "Epoch [14/40], loss: 0.0007, l1_regur: 0.0192, total: 0.0199, valid loss: 0.0011\n",
      "Epoch [15/40], loss: 0.0007, l1_regur: 0.0095, total: 0.0102, valid loss: 0.0011\n",
      "Epoch [16/40], loss: 0.0007, l1_regur: 0.0051, total: 0.0058, valid loss: 0.0011\n",
      "Epoch [17/40], loss: 0.0007, l1_regur: 0.0030, total: 0.0037, valid loss: 0.0011\n",
      "Epoch [18/40], loss: 0.0007, l1_regur: 0.0018, total: 0.0025, valid loss: 0.0011\n",
      "Epoch [19/40], loss: 0.0007, l1_regur: 0.0125, total: 0.0132, valid loss: 0.0011\n",
      "Epoch [20/40], loss: 0.0007, l1_regur: 0.0064, total: 0.0071, valid loss: 0.0011\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c2fe9d250539>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m train(\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-2-27dbd51ec72e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, train_loader, valid_loader, num_epochs, early_stop, no_improve_epochs, threshold, alpha, grad_clip)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml2019/lib/python3.6/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml2019/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(self, p, dim, keepdim)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fro\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;34mr\"\"\"See :func: `torch.norm`\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbtrifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpivot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml2019/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrobenius_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"nuc\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"fro\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model, optimizer, scheduler, train_loader, valid_loader, 40,\n",
    "    threshold=3e-4, alpha=1e-5, grad_clip=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100000\n",
    "test_loader = DataLoader(procDataSet.TestQuery(), batch_size=batch_size, shuffle=False, num_workers=NumberCPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "0.1\n",
      "0.15\n",
      "0.2\n",
      "0.25\n",
      "0.3\n",
      "0.35\n",
      "0.4\n",
      "0.45\n",
      "0.5\n",
      "0.55\n",
      "0.6\n",
      "0.65\n",
      "0.7\n",
      "0.75\n",
      "0.8\n",
      "0.85\n",
      "0.9\n",
      "0.95\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "model.eval()\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    for q, c in test_loader:\n",
    "        q, c = q.cuda(), c.cuda()\n",
    "        outputs = model(q, c)\n",
    "        results.append(outputs.cpu().data.numpy())\n",
    "        i += 1\n",
    "        print(i / len(test_loader))\n",
    "results = np.concatenate(results, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.reshape(20, -1)\n",
    "search_result = np.flip(np.argsort(results, axis=1), axis=1)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Query_Index'] = ['q_{:02d}'.format(i+1) for i in range(20)]\n",
    "\n",
    "for i in range(300):\n",
    "    df['Rank_{:03d}'.format(i+1)] = search_result[:, i]\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    df.iloc[i, 1:] = df.iloc[i, 1:].apply(lambda x: 'news_{:06d}'.format(x))\n",
    "fname = 'simple.csv'\n",
    "df.to_csv('output/' + fname,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = results.reshape(20, -1)\n",
    "np.where(results > .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00126912, -0.00113342, -0.00126483, ..., -0.00119252,\n",
       "        -0.00125728, -0.0012664 ],\n",
       "       [-0.00142819, -0.00129511, -0.00142516, ..., -0.00135495,\n",
       "        -0.00141793, -0.0014262 ],\n",
       "       [-0.0016579 , -0.00153257, -0.00165507, ..., -0.0015901 ,\n",
       "        -0.00164867, -0.00165643],\n",
       "       ...,\n",
       "       [-0.00138995, -0.00125749, -0.00138701, ..., -0.00131617,\n",
       "        -0.00137884, -0.00138806],\n",
       "       [-0.00095255, -0.00080123, -0.000945  , ..., -0.00087102,\n",
       "        -0.00093861, -0.00094846],\n",
       "       [-0.00134824, -0.0012053 , -0.00134352, ..., -0.0012709 ,\n",
       "        -0.00133587, -0.00134562]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2019",
   "language": "python",
   "name": "ml2019"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
