{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import procDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        n_hidden = 1000\n",
    "        dense1 = nn.Linear(768 + 768, n_hidden)\n",
    "        dense2 = nn.Linear(n_hidden, n_hidden // 2)\n",
    "        dense3 = nn.Linear(n_hidden // 2, 1)\n",
    "\n",
    "        self.DNN = nn.Sequential(\n",
    "            dense1,\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.Dropout(.4),\n",
    "            nn.SELU(),\n",
    "\n",
    "            dense2,\n",
    "            nn.BatchNorm1d(n_hidden // 2),\n",
    "            nn.Dropout(.4),\n",
    "            nn.SELU(),\n",
    "            \n",
    "            dense3,\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        # for l2 regularization\n",
    "        self.regularizations = [dense1.weight, dense2.weight]\n",
    "\n",
    "    \n",
    "    def forward(self, query, text): \n",
    "        x = torch.cat([query, text], dim=1)\n",
    "        return self.DNN(x)\n",
    "\n",
    "def regularization(weights, alpha):\n",
    "    rt = 0\n",
    "    for w in weights:\n",
    "        rt += torch.sum(torch.abs(w))\n",
    "    return alpha * rt\n",
    "\n",
    "def train(\n",
    "    model, optimizer, scheduler, train_loader, num_epochs, \n",
    "#     job_name, \n",
    "    early_stop=True, no_improve_epochs=15, threshold=1e-3,\n",
    "    alpha=0, grad_clip=1000\n",
    "):\n",
    "#     train_hist = []\n",
    "    last_epoch_tune_lr = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.\n",
    "        regular = 0.\n",
    "\n",
    "        for q, c, y in train_loader:\n",
    "            q, c = q.cuda(), c.cuda()\n",
    "            y = y.cuda()\n",
    "            outputs = model(q, c)\n",
    "            loss = loss_func(outputs, y.float())\n",
    "            train_loss += loss.item()\n",
    "            regular_batch = regularization(model.regularizations, alpha)\n",
    "            loss += regular_batch\n",
    "            regular += regular_batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss /= len(train_loader)     \n",
    "        regular /= len(train_loader)    \n",
    "        total_loss = regular+train_loss\n",
    "        print('Epoch [{}/{}], loss: {:.4f}, l1_regur: {:.4f}, total: {:.4f}'\n",
    "              .format(epoch+1, num_epochs, train_loss, regular, total_loss))\n",
    "#         train_hist.append((train_loss, regular, total_loss))\n",
    "        \n",
    "        scheduler.step(total_loss)\n",
    "        if early_stop and epoch > no_improve_epochs + scheduler.last_epoch and np.amin(train_hist[:-no_improve_epochs]) + threshold < np.amin(train_hist[-no_improve_epochs:]):\n",
    "            print('Trigger early stop.')\n",
    "            break\n",
    "\n",
    "#     train_hist_fname = os.path.join('output', job_name + '_hist')\n",
    "#     print('save training history to {}.npy'.format(train_hist_fname))\n",
    "#     np.save(train_hist_fname, train_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2731\n",
    "\n",
    "train_loader = DataLoader(procDataSet.TrainingQuery(), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = SimpleModel().cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer, 'min', verbose=True,\n",
    "    patience=2, factor=.1**.5, min_lr=1e-5, threshold=0, cooldown=15\n",
    ")\n",
    "\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], loss: 3.9028, l1_regur: 0.2748, total: 4.1776\n",
      "Epoch [2/200], loss: 3.3842, l1_regur: 0.2738, total: 3.6579\n",
      "Epoch [3/200], loss: 2.6769, l1_regur: 0.2729, total: 2.9498\n",
      "Epoch [4/200], loss: 2.3532, l1_regur: 0.2720, total: 2.6252\n",
      "Epoch [5/200], loss: 2.1003, l1_regur: 0.2710, total: 2.3713\n",
      "Epoch [6/200], loss: 1.9247, l1_regur: 0.2699, total: 2.1946\n",
      "Epoch [7/200], loss: 1.7482, l1_regur: 0.2688, total: 2.0170\n",
      "Epoch [8/200], loss: 1.5839, l1_regur: 0.2675, total: 1.8514\n",
      "Epoch [9/200], loss: 1.4546, l1_regur: 0.2662, total: 1.7208\n",
      "Epoch [10/200], loss: 1.3539, l1_regur: 0.2648, total: 1.6187\n",
      "Epoch [11/200], loss: 1.2514, l1_regur: 0.2634, total: 1.5148\n",
      "Epoch [12/200], loss: 1.1493, l1_regur: 0.2621, total: 1.4113\n",
      "Epoch [13/200], loss: 1.0644, l1_regur: 0.2607, total: 1.3251\n",
      "Epoch [14/200], loss: 0.9821, l1_regur: 0.2593, total: 1.2414\n",
      "Epoch [15/200], loss: 0.9236, l1_regur: 0.2579, total: 1.1815\n",
      "Epoch [16/200], loss: 0.8767, l1_regur: 0.2565, total: 1.1332\n",
      "Epoch [17/200], loss: 0.8408, l1_regur: 0.2550, total: 1.0958\n",
      "Epoch [18/200], loss: 0.8190, l1_regur: 0.2535, total: 1.0725\n",
      "Epoch [19/200], loss: 0.8089, l1_regur: 0.2520, total: 1.0609\n",
      "Epoch [20/200], loss: 0.7938, l1_regur: 0.2504, total: 1.0442\n",
      "Epoch [21/200], loss: 0.7966, l1_regur: 0.2488, total: 1.0455\n",
      "Epoch [22/200], loss: 0.7912, l1_regur: 0.2473, total: 1.0385\n",
      "Epoch [23/200], loss: 0.8003, l1_regur: 0.2457, total: 1.0460\n",
      "Epoch [24/200], loss: 0.7993, l1_regur: 0.2441, total: 1.0434\n",
      "Epoch [25/200], loss: 0.8020, l1_regur: 0.2425, total: 1.0446\n",
      "Epoch    24: reducing learning rate of group 0 to 3.1623e-04.\n",
      "Epoch [26/200], loss: 0.7968, l1_regur: 0.2412, total: 1.0380\n",
      "Epoch [27/200], loss: 0.7979, l1_regur: 0.2406, total: 1.0385\n",
      "Epoch [28/200], loss: 0.7970, l1_regur: 0.2400, total: 1.0369\n",
      "Epoch [29/200], loss: 0.7954, l1_regur: 0.2394, total: 1.0348\n",
      "Epoch [30/200], loss: 0.7960, l1_regur: 0.2387, total: 1.0347\n",
      "Epoch [31/200], loss: 0.7920, l1_regur: 0.2381, total: 1.0302\n",
      "Epoch [32/200], loss: 0.7878, l1_regur: 0.2375, total: 1.0254\n",
      "Epoch [33/200], loss: 0.7847, l1_regur: 0.2369, total: 1.0217\n",
      "Epoch [34/200], loss: 0.7904, l1_regur: 0.2363, total: 1.0268\n",
      "Epoch [35/200], loss: 0.7841, l1_regur: 0.2358, total: 1.0199\n",
      "Epoch [36/200], loss: 0.7855, l1_regur: 0.2352, total: 1.0207\n",
      "Epoch [37/200], loss: 0.7787, l1_regur: 0.2346, total: 1.0133\n",
      "Epoch [38/200], loss: 0.7835, l1_regur: 0.2340, total: 1.0176\n",
      "Epoch [39/200], loss: 0.7808, l1_regur: 0.2335, total: 1.0142\n",
      "Epoch [40/200], loss: 0.7804, l1_regur: 0.2329, total: 1.0133\n",
      "Epoch [41/200], loss: 0.7765, l1_regur: 0.2323, total: 1.0088\n",
      "Epoch [42/200], loss: 0.7831, l1_regur: 0.2317, total: 1.0148\n",
      "Epoch [43/200], loss: 0.7816, l1_regur: 0.2312, total: 1.0128\n",
      "Epoch [44/200], loss: 0.7822, l1_regur: 0.2306, total: 1.0127\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch [45/200], loss: 0.7806, l1_regur: 0.2301, total: 1.0106\n",
      "Epoch [46/200], loss: 0.7768, l1_regur: 0.2299, total: 1.0067\n",
      "Epoch [47/200], loss: 0.7832, l1_regur: 0.2297, total: 1.0129\n",
      "Epoch [48/200], loss: 0.7801, l1_regur: 0.2295, total: 1.0095\n",
      "Epoch [49/200], loss: 0.7774, l1_regur: 0.2293, total: 1.0066\n",
      "Epoch [50/200], loss: 0.7787, l1_regur: 0.2291, total: 1.0077\n",
      "Epoch [51/200], loss: 0.7723, l1_regur: 0.2289, total: 1.0012\n",
      "Epoch [52/200], loss: 0.7782, l1_regur: 0.2287, total: 1.0069\n",
      "Epoch [53/200], loss: 0.7768, l1_regur: 0.2285, total: 1.0053\n",
      "Epoch [54/200], loss: 0.7822, l1_regur: 0.2283, total: 1.0105\n",
      "Epoch [55/200], loss: 0.7787, l1_regur: 0.2281, total: 1.0068\n",
      "Epoch [56/200], loss: 0.7748, l1_regur: 0.2279, total: 1.0026\n",
      "Epoch [57/200], loss: 0.7803, l1_regur: 0.2277, total: 1.0080\n",
      "Epoch [58/200], loss: 0.7750, l1_regur: 0.2275, total: 1.0025\n",
      "Epoch [59/200], loss: 0.7751, l1_regur: 0.2273, total: 1.0024\n",
      "Epoch [60/200], loss: 0.7706, l1_regur: 0.2271, total: 0.9977\n",
      "Epoch [61/200], loss: 0.7778, l1_regur: 0.2269, total: 1.0047\n",
      "Epoch [62/200], loss: 0.7787, l1_regur: 0.2267, total: 1.0054\n",
      "Epoch [63/200], loss: 0.7795, l1_regur: 0.2265, total: 1.0060\n",
      "Epoch    62: reducing learning rate of group 0 to 3.1623e-05.\n",
      "Epoch [64/200], loss: 0.7783, l1_regur: 0.2263, total: 1.0046\n",
      "Epoch [65/200], loss: 0.7747, l1_regur: 0.2262, total: 1.0009\n",
      "Epoch [66/200], loss: 0.7760, l1_regur: 0.2262, total: 1.0022\n",
      "Epoch [67/200], loss: 0.7737, l1_regur: 0.2261, total: 0.9998\n",
      "Epoch [68/200], loss: 0.7758, l1_regur: 0.2260, total: 1.0019\n",
      "Epoch [69/200], loss: 0.7756, l1_regur: 0.2260, total: 1.0016\n",
      "Epoch [70/200], loss: 0.7726, l1_regur: 0.2259, total: 0.9985\n",
      "Epoch [71/200], loss: 0.7763, l1_regur: 0.2258, total: 1.0021\n",
      "Epoch [72/200], loss: 0.7742, l1_regur: 0.2257, total: 0.9999\n",
      "Epoch [73/200], loss: 0.7764, l1_regur: 0.2257, total: 1.0021\n",
      "Epoch [74/200], loss: 0.7750, l1_regur: 0.2256, total: 1.0006\n",
      "Epoch [75/200], loss: 0.7764, l1_regur: 0.2255, total: 1.0020\n",
      "Epoch [76/200], loss: 0.7729, l1_regur: 0.2255, total: 0.9984\n",
      "Epoch [77/200], loss: 0.7755, l1_regur: 0.2254, total: 1.0009\n",
      "Epoch [78/200], loss: 0.7774, l1_regur: 0.2253, total: 1.0028\n",
      "Epoch [79/200], loss: 0.7703, l1_regur: 0.2253, total: 0.9956\n",
      "Epoch [80/200], loss: 0.7753, l1_regur: 0.2252, total: 1.0005\n",
      "Epoch [81/200], loss: 0.7742, l1_regur: 0.2251, total: 0.9993\n",
      "Epoch [82/200], loss: 0.7731, l1_regur: 0.2250, total: 0.9981\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch [83/200], loss: 0.7780, l1_regur: 0.2250, total: 1.0030\n",
      "Epoch [84/200], loss: 0.7763, l1_regur: 0.2250, total: 1.0013\n",
      "Epoch [85/200], loss: 0.7737, l1_regur: 0.2249, total: 0.9986\n",
      "Epoch [86/200], loss: 0.7776, l1_regur: 0.2249, total: 1.0025\n",
      "Epoch [87/200], loss: 0.7748, l1_regur: 0.2249, total: 0.9997\n",
      "Epoch [88/200], loss: 0.7749, l1_regur: 0.2249, total: 0.9997\n",
      "Epoch [89/200], loss: 0.7714, l1_regur: 0.2248, total: 0.9963\n",
      "Epoch [90/200], loss: 0.7672, l1_regur: 0.2248, total: 0.9921\n",
      "Epoch [91/200], loss: 0.7792, l1_regur: 0.2248, total: 1.0040\n",
      "Epoch [92/200], loss: 0.7707, l1_regur: 0.2248, total: 0.9954\n",
      "Epoch [93/200], loss: 0.7689, l1_regur: 0.2247, total: 0.9937\n",
      "Epoch [94/200], loss: 0.7737, l1_regur: 0.2247, total: 0.9984\n",
      "Epoch [95/200], loss: 0.7662, l1_regur: 0.2247, total: 0.9909\n",
      "Epoch [96/200], loss: 0.7732, l1_regur: 0.2247, total: 0.9978\n",
      "Epoch [97/200], loss: 0.7728, l1_regur: 0.2246, total: 0.9975\n",
      "Epoch [98/200], loss: 0.7758, l1_regur: 0.2246, total: 1.0004\n",
      "Epoch [99/200], loss: 0.7761, l1_regur: 0.2246, total: 1.0007\n",
      "Epoch [100/200], loss: 0.7736, l1_regur: 0.2246, total: 0.9982\n",
      "Epoch [101/200], loss: 0.7791, l1_regur: 0.2245, total: 1.0036\n",
      "Epoch [102/200], loss: 0.7754, l1_regur: 0.2245, total: 0.9999\n",
      "Epoch [103/200], loss: 0.7749, l1_regur: 0.2245, total: 0.9994\n",
      "Epoch [104/200], loss: 0.7753, l1_regur: 0.2245, total: 0.9997\n",
      "Epoch [105/200], loss: 0.7723, l1_regur: 0.2244, total: 0.9968\n",
      "Epoch [106/200], loss: 0.7774, l1_regur: 0.2244, total: 1.0018\n",
      "Epoch [107/200], loss: 0.7706, l1_regur: 0.2244, total: 0.9950\n",
      "Epoch [108/200], loss: 0.7730, l1_regur: 0.2244, total: 0.9973\n",
      "Epoch [109/200], loss: 0.7741, l1_regur: 0.2243, total: 0.9985\n",
      "Epoch [110/200], loss: 0.7736, l1_regur: 0.2243, total: 0.9979\n",
      "Epoch [111/200], loss: 0.7711, l1_regur: 0.2243, total: 0.9954\n",
      "Epoch [112/200], loss: 0.7705, l1_regur: 0.2243, total: 0.9948\n",
      "Epoch [113/200], loss: 0.7751, l1_regur: 0.2242, total: 0.9994\n",
      "Epoch [114/200], loss: 0.7742, l1_regur: 0.2242, total: 0.9984\n",
      "Epoch [115/200], loss: 0.7738, l1_regur: 0.2242, total: 0.9980\n",
      "Epoch [116/200], loss: 0.7741, l1_regur: 0.2242, total: 0.9983\n",
      "Epoch [117/200], loss: 0.7725, l1_regur: 0.2241, total: 0.9967\n",
      "Epoch [118/200], loss: 0.7702, l1_regur: 0.2241, total: 0.9943\n",
      "Epoch [119/200], loss: 0.7697, l1_regur: 0.2241, total: 0.9938\n",
      "Epoch [120/200], loss: 0.7747, l1_regur: 0.2241, total: 0.9987\n",
      "Epoch [121/200], loss: 0.7695, l1_regur: 0.2240, total: 0.9936\n",
      "Epoch [122/200], loss: 0.7737, l1_regur: 0.2240, total: 0.9977\n",
      "Epoch [123/200], loss: 0.7742, l1_regur: 0.2240, total: 0.9982\n",
      "Epoch [124/200], loss: 0.7746, l1_regur: 0.2239, total: 0.9985\n",
      "Epoch [125/200], loss: 0.7741, l1_regur: 0.2239, total: 0.9981\n",
      "Epoch [126/200], loss: 0.7760, l1_regur: 0.2239, total: 0.9999\n",
      "Epoch [127/200], loss: 0.7739, l1_regur: 0.2239, total: 0.9977\n",
      "Epoch [128/200], loss: 0.7747, l1_regur: 0.2238, total: 0.9986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [129/200], loss: 0.7751, l1_regur: 0.2238, total: 0.9989\n",
      "Epoch [130/200], loss: 0.7712, l1_regur: 0.2238, total: 0.9949\n",
      "Epoch [131/200], loss: 0.7733, l1_regur: 0.2238, total: 0.9971\n",
      "Epoch [132/200], loss: 0.7751, l1_regur: 0.2237, total: 0.9988\n",
      "Epoch [133/200], loss: 0.7741, l1_regur: 0.2237, total: 0.9978\n",
      "Epoch [134/200], loss: 0.7700, l1_regur: 0.2237, total: 0.9937\n",
      "Epoch [135/200], loss: 0.7710, l1_regur: 0.2236, total: 0.9947\n",
      "Epoch [136/200], loss: 0.7729, l1_regur: 0.2236, total: 0.9965\n",
      "Epoch [137/200], loss: 0.7719, l1_regur: 0.2236, total: 0.9955\n",
      "Epoch [138/200], loss: 0.7682, l1_regur: 0.2236, total: 0.9918\n",
      "Epoch [139/200], loss: 0.7683, l1_regur: 0.2235, total: 0.9918\n",
      "Epoch [140/200], loss: 0.7717, l1_regur: 0.2235, total: 0.9952\n",
      "Epoch [141/200], loss: 0.7739, l1_regur: 0.2235, total: 0.9974\n",
      "Epoch [142/200], loss: 0.7726, l1_regur: 0.2234, total: 0.9960\n",
      "Epoch [143/200], loss: 0.7710, l1_regur: 0.2234, total: 0.9944\n",
      "Epoch [144/200], loss: 0.7744, l1_regur: 0.2234, total: 0.9978\n",
      "Epoch [145/200], loss: 0.7778, l1_regur: 0.2234, total: 1.0011\n",
      "Epoch [146/200], loss: 0.7727, l1_regur: 0.2233, total: 0.9960\n",
      "Epoch [147/200], loss: 0.7766, l1_regur: 0.2233, total: 0.9999\n",
      "Epoch [148/200], loss: 0.7716, l1_regur: 0.2233, total: 0.9948\n",
      "Epoch [149/200], loss: 0.7775, l1_regur: 0.2232, total: 1.0007\n",
      "Epoch [150/200], loss: 0.7765, l1_regur: 0.2232, total: 0.9997\n",
      "Epoch [151/200], loss: 0.7683, l1_regur: 0.2232, total: 0.9915\n",
      "Epoch [152/200], loss: 0.7788, l1_regur: 0.2232, total: 1.0019\n",
      "Epoch [153/200], loss: 0.7729, l1_regur: 0.2231, total: 0.9960\n",
      "Epoch [154/200], loss: 0.7723, l1_regur: 0.2231, total: 0.9954\n",
      "Epoch [155/200], loss: 0.7741, l1_regur: 0.2231, total: 0.9972\n",
      "Epoch [156/200], loss: 0.7770, l1_regur: 0.2230, total: 1.0000\n",
      "Epoch [157/200], loss: 0.7704, l1_regur: 0.2230, total: 0.9934\n",
      "Epoch [158/200], loss: 0.7729, l1_regur: 0.2230, total: 0.9958\n",
      "Epoch [159/200], loss: 0.7731, l1_regur: 0.2229, total: 0.9960\n",
      "Epoch [160/200], loss: 0.7717, l1_regur: 0.2229, total: 0.9946\n",
      "Epoch [161/200], loss: 0.7722, l1_regur: 0.2229, total: 0.9951\n",
      "Epoch [162/200], loss: 0.7776, l1_regur: 0.2229, total: 1.0004\n",
      "Epoch [163/200], loss: 0.7699, l1_regur: 0.2228, total: 0.9927\n",
      "Epoch [164/200], loss: 0.7780, l1_regur: 0.2228, total: 1.0008\n",
      "Epoch [165/200], loss: 0.7693, l1_regur: 0.2228, total: 0.9920\n",
      "Epoch [166/200], loss: 0.7800, l1_regur: 0.2227, total: 1.0028\n",
      "Epoch [167/200], loss: 0.7733, l1_regur: 0.2227, total: 0.9960\n",
      "Epoch [168/200], loss: 0.7720, l1_regur: 0.2227, total: 0.9947\n",
      "Epoch [169/200], loss: 0.7706, l1_regur: 0.2226, total: 0.9933\n",
      "Epoch [170/200], loss: 0.7720, l1_regur: 0.2226, total: 0.9946\n",
      "Epoch [171/200], loss: 0.7736, l1_regur: 0.2226, total: 0.9962\n",
      "Epoch [172/200], loss: 0.7721, l1_regur: 0.2225, total: 0.9947\n",
      "Epoch [173/200], loss: 0.7749, l1_regur: 0.2225, total: 0.9974\n",
      "Epoch [174/200], loss: 0.7742, l1_regur: 0.2225, total: 0.9967\n",
      "Epoch [175/200], loss: 0.7673, l1_regur: 0.2225, total: 0.9897\n",
      "Epoch [176/200], loss: 0.7690, l1_regur: 0.2224, total: 0.9915\n",
      "Epoch [177/200], loss: 0.7744, l1_regur: 0.2224, total: 0.9968\n",
      "Epoch [178/200], loss: 0.7710, l1_regur: 0.2224, total: 0.9934\n",
      "Epoch [179/200], loss: 0.7663, l1_regur: 0.2223, total: 0.9887\n",
      "Epoch [180/200], loss: 0.7693, l1_regur: 0.2223, total: 0.9916\n",
      "Epoch [181/200], loss: 0.7683, l1_regur: 0.2223, total: 0.9905\n",
      "Epoch [182/200], loss: 0.7676, l1_regur: 0.2222, total: 0.9898\n",
      "Epoch [183/200], loss: 0.7712, l1_regur: 0.2222, total: 0.9934\n",
      "Epoch [184/200], loss: 0.7710, l1_regur: 0.2222, total: 0.9932\n",
      "Epoch [185/200], loss: 0.7741, l1_regur: 0.2221, total: 0.9963\n",
      "Epoch [186/200], loss: 0.7755, l1_regur: 0.2221, total: 0.9976\n",
      "Epoch [187/200], loss: 0.7724, l1_regur: 0.2221, total: 0.9945\n",
      "Epoch [188/200], loss: 0.7704, l1_regur: 0.2220, total: 0.9925\n",
      "Epoch [189/200], loss: 0.7708, l1_regur: 0.2220, total: 0.9928\n",
      "Epoch [190/200], loss: 0.7690, l1_regur: 0.2220, total: 0.9910\n",
      "Epoch [191/200], loss: 0.7707, l1_regur: 0.2219, total: 0.9926\n",
      "Epoch [192/200], loss: 0.7800, l1_regur: 0.2219, total: 1.0019\n",
      "Epoch [193/200], loss: 0.7725, l1_regur: 0.2219, total: 0.9944\n",
      "Epoch [194/200], loss: 0.7706, l1_regur: 0.2218, total: 0.9924\n",
      "Epoch [195/200], loss: 0.7731, l1_regur: 0.2218, total: 0.9949\n",
      "Epoch [196/200], loss: 0.7702, l1_regur: 0.2218, total: 0.9920\n",
      "Epoch [197/200], loss: 0.7711, l1_regur: 0.2217, total: 0.9928\n",
      "Epoch [198/200], loss: 0.7745, l1_regur: 0.2217, total: 0.9962\n",
      "Epoch [199/200], loss: 0.7702, l1_regur: 0.2217, total: 0.9919\n",
      "Epoch [200/200], loss: 0.7745, l1_regur: 0.2217, total: 0.9961\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model, optimizer, scheduler, train_loader, 200,\n",
    "    threshold=3e-4, alpha=1e-5, grad_clip=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100000\n",
    "test_loader = DataLoader(procDataSet.TestQuery(), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "0.1\n",
      "0.15\n",
      "0.2\n",
      "0.25\n",
      "0.3\n",
      "0.35\n",
      "0.4\n",
      "0.45\n",
      "0.5\n",
      "0.55\n",
      "0.6\n",
      "0.65\n",
      "0.7\n",
      "0.75\n",
      "0.8\n",
      "0.85\n",
      "0.9\n",
      "0.95\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "model.eval()\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    for q, c in test_loader:\n",
    "        q, c = q.cuda(), c.cuda()\n",
    "        outputs = model(q, c)\n",
    "        results.append(outputs.cpu().data.numpy())\n",
    "        i += 1\n",
    "        print(i / len(test_loader))\n",
    "results = np.concatenate(results, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.reshape(20, -1)\n",
    "search_result = np.argsort(results, axis=1)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Query_Index'] = ['q_{:02d}'.format(i+1) for i in range(20)]\n",
    "\n",
    "for i in range(300):\n",
    "    df['Rank_{:03d}'.format(i+1)] = search_result[:, i]\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    df.iloc[i, 1:] = df.iloc[i, 1:].apply(lambda x: 'news_{:06d}'.format(x))\n",
    "fname = 'simple.csv'\n",
    "df.to_csv('output/' + fname,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2019",
   "language": "python",
   "name": "ml2019"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
