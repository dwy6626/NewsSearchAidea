{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from procDataSet import TrainingQuery\n",
    "\n",
    "\n",
    "top_num = 300\n",
    "\n",
    "jieba.load_userdict(os.path.join('data', 'dict.txt.big'))\n",
    "[jieba.add_word(i, freq=None, tag=None) for i in ['不支持','文林苑', '都更案','十八趴','證所稅','前瞻建設', '月退']]\n",
    "\n",
    "# add NTUSD\n",
    "positive_words = np.squeeze(pd.read_csv('data/NTUSD/ntusd-positive_zhtw.txt', header=None).values).tolist()\n",
    "negative_words = np.squeeze(pd.read_csv('data/NTUSD/ntusd-negative_zhtw.txt', header=None).values).tolist()\n",
    "for w in positive_words + negative_words:\n",
    "    jieba.add_word(w)\n",
    "    \n",
    "def histogram_equalization(image, number_bins=256):\n",
    "    # from http://www.janeriksolem.net/2009/06/histogram-equalization-with-python-and.html\n",
    "\n",
    "    # get image histogram\n",
    "    image_histogram, bins = np.histogram(image.flatten(), number_bins, density=True)\n",
    "    cdf = image_histogram.cumsum() # cumulative distribution function\n",
    "    cdf = 100 * cdf / cdf[-1] # normalize\n",
    "\n",
    "    # use linear interpolation of cdf to find new pixel values\n",
    "    image_equalized = np.interp(image.flatten(), bins[:-1], cdf)\n",
    "    return image_equalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/stop_word_zhtw.txt') as file:\n",
    "    data = file.read()\n",
    "    \n",
    "stop_words = data.split('\\n')\n",
    "stop_words += ['「', '」', '，', '\\n', '）', '（', ')', '(']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'data/'\n",
    "raw_training_data = pd.read_csv(os.path.join(folder,'TD.csv'))\n",
    "news_urls = pd.read_csv(os.path.join(folder,'NC_1.csv'))\n",
    "contents = pd.read_json(os.path.join(folder,'url2content.json'), typ=pd.Series)\n",
    "test_query = np.array(pd.read_csv('./data/QS_1.csv').Query)\n",
    "\n",
    "## sort the contents by index\n",
    "keys, content_list = contents.keys(), contents.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add back some words\n",
    "stop_words = list(set(stop_words) - set(negative_words) - set(positive_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['通姦', '刑法', '應該', '除罪', '化']\n",
      "-1\n",
      "['應該', '取消', '機車', '強制', '二段式', '左轉', '待轉']\n",
      "-3\n",
      "['支持', '博弈', '特區', '台灣', '合法化']\n",
      "0\n",
      "['中華', '航空', '空服員', '罷工', '合理的']\n",
      "0\n",
      "['性交易', '應該', '合法化']\n",
      "0\n",
      "['ECFA', '早收', '清單', '達到', '預期', '成效']\n",
      "1\n",
      "['應該', '減免', '證所稅']\n",
      "0\n",
      "['贊成', '中油', '觀塘', '興建', '第三', '天然氣', '接收站']\n",
      "1\n",
      "['支持', '中國', '學生', '納入', '健保']\n",
      "0\n",
      "['支持', '臺灣', '中小學', '含', '高職', '專科', '服儀', '規定', '含', '髮', '襪', '鞋', '給予', '學生', '自主']\n",
      "2\n",
      "['不支持', '使用', '加密', '貨幣']\n",
      "0\n",
      "['不支持', '學雜費', '調漲']\n",
      "0\n",
      "['同意', '政府', '舉債', '發展', '前瞻建設', '計畫']\n",
      "2\n",
      "['支持', '電競', '列入', '體育競技']\n",
      "0\n",
      "['反對', '台鐵', '東移', '徵收', '案']\n",
      "-2\n",
      "['支持', '陳', '前', '總統', '保外就醫']\n",
      "0\n",
      "['年金', '改革', '應', '取消', '或應', '調降', '軍公教', '月退', '優存', '利率', '十八趴']\n",
      "0\n",
      "['同意', '動物', '實驗']\n",
      "1\n",
      "['油價', '應該', '凍漲', '緩漲']\n",
      "0\n",
      "['反對', '旺旺', '中時', '併購', '中嘉']\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "def find_sentiment(cuts):\n",
    "    p = n = 0\n",
    "    for c in cuts:\n",
    "        p += c in positive_words\n",
    "        n += c in negative_words\n",
    "    return p - n\n",
    "\n",
    "for q in test_query:\n",
    "    cuts = jieba.lcut(q)\n",
    "    cuts = [c for c in cuts if c not in stop_words]\n",
    "    print(cuts)\n",
    "    print(find_sentiment(cuts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "NumberCPU = multiprocessing.cpu_count()\n",
    "jieba.initialize()\n",
    "\n",
    "def jbcut(x):\n",
    "    if x is not None:\n",
    "        sen = jieba.lcut(x, cut_all=False)\n",
    "        sen = [i for i in sen if i not in stop_words]\n",
    "        return sen\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def psegcut(x):\n",
    "    if x is not None:\n",
    "        sen = pseg.lcut(x)\n",
    "        sen = [i for i in sen if i not in stop_words]\n",
    "        return sen\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "pool = multiprocessing.Pool(processes=NumberCPU)\n",
    "sentence_arr = pool.map(jbcut,content_list)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_arr = np.load('data/sentence_arr.npy')\n",
    "# np.save('data/sentence_arr', sentence_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add NTUSD scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要跑超久\n",
    "pool = multiprocessing.Pool(processes=NumberCPU)\n",
    "sentiment_arr = pool.map(find_sentiment, sentence_arr)\n",
    "pool.close()\n",
    "pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('data/sentiment_arr', sentiment_arr)\n",
    "sentiment_arr = np.load('data/sentiment_arr.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## okapi model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import bm25\n",
    "bm25Model = bm25.BM25(sentence_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "十二年國教高中職「免學費補助」適用對象增加是不對的 93\n",
      "反對二代健保規定 232\n",
      "反對旺旺中時併購中嘉 109\n",
      "反對無圍牆校園 35\n",
      "另立專法保障同婚是正確的 135\n",
      "同意動物實驗 20\n",
      "國際賽事會場內應該可以持中華民國國旗 766\n",
      "堅決反對政府舉債發展前瞻建設計畫 388\n",
      "年金改革應取消或應調降軍公教月退之優存利率十八趴 200\n",
      "應該提高酒駕罰責以有效遏制酒駕 441\n",
      "拒絕公投通過門檻下修 52\n",
      "支持正名「臺灣」參與國際運動賽事 110\n",
      "支持陳前總統保外就醫 200\n",
      "核四應該啟用 205\n",
      "油價應該凍漲或緩漲 68\n",
      "臺灣應開放含瘦肉精(萊克多巴胺)之美國牛肉進口 248\n",
      "贊同課綱微調 310\n",
      "贊成文林苑都更案可依法拆除王家 65\n",
      "贊成流浪動物零撲殺 270\n",
      "遠雄大巨蛋工程應停工或拆除 240\n"
     ]
    }
   ],
   "source": [
    "train_query = sorted(set(raw_training_data.Query))\n",
    "\n",
    "y_train = []\n",
    "y_index = {}\n",
    "for i  in train_query:\n",
    "    index = np.where(raw_training_data.Query==i)\n",
    "    data = raw_training_data.iloc[index]\n",
    "\n",
    "    # sort by relvanance\n",
    "    y = np.squeeze(data[data.Relevance!=0].sort_values('Relevance', ascending=False).News_Index.values).tolist()\n",
    "    y_idx = [ (int(idx.split('_')[1])-1, rel )  for idx, rel in zip(data.News_Index, data.Relevance)]\n",
    "    y_train.append(y)\n",
    "    y_index[i] = [y_idx]\n",
    "    \n",
    "    print(i, len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 十二年國教高中職「免學費補助」適用對象增加是不對的 ['十二年', '國教', '高中', '職', '免', '學費', '補助', '適用', '對象', '增加', '不對']\n",
      "Total: 93, AP: 0.17, recall: 0.59\n",
      "1 反對二代健保規定 ['反對', '二代', '健保', '規定']\n",
      "Total: 232, AP: 0.39, recall: 0.64\n",
      "2 反對旺旺中時併購中嘉 ['反對', '旺旺', '中時', '併購', '中嘉']\n",
      "Total: 109, AP: 0.17, recall: 0.61\n",
      "3 反對無圍牆校園 ['反對', '圍牆', '校園']\n",
      "Total: 35, AP: 0.21, recall: 0.54\n",
      "4 另立專法保障同婚是正確的 ['另立', '專法', '保障', '同婚', '正確的']\n",
      "Total: 135, AP: 0.13, recall: 0.44\n",
      "5 同意動物實驗 ['同意', '動物', '實驗']\n",
      "Total: 20, AP: 0.01, recall: 0.15\n",
      "6 國際賽事會場內應該可以持中華民國國旗 ['國際', '賽事', '會場', '應該', '持', '中華民國', '國旗']\n",
      "Total: 300, AP: 0.03, recall: 0.17\n",
      "7 堅決反對政府舉債發展前瞻建設計畫 ['堅決', '反對', '政府', '舉債', '發展', '前瞻建設', '計畫']\n",
      "Total: 300, AP: 0.29, recall: 0.47\n",
      "8 年金改革應取消或應調降軍公教月退之優存利率十八趴 ['年金', '改革', '應', '取消', '或應', '調降', '軍公教', '月退', '優存', '利率', '十八趴']\n",
      "Total: 200, AP: 0.04, recall: 0.26\n",
      "9 應該提高酒駕罰責以有效遏制酒駕 ['應該', '提高', '酒駕', '罰責以', '有效', '遏制', '酒駕']\n",
      "Total: 300, AP: 0.44, recall: 0.64\n",
      "10 拒絕公投通過門檻下修 ['拒絕', '公投', '通過', '門檻', '下修']\n",
      "Total: 52, AP: 0.06, recall: 0.46\n",
      "11 支持正名「臺灣」參與國際運動賽事 ['支持', '正名', '臺灣', '參與', '國際', '運動', '賽事']\n",
      "Total: 110, AP: 0.13, recall: 0.49\n",
      "12 支持陳前總統保外就醫 ['支持', '陳', '前', '總統', '保外就醫']\n",
      "Total: 200, AP: 0.05, recall: 0.24\n",
      "13 核四應該啟用 ['核四', '應該', '啟用']\n",
      "Total: 205, AP: 0.09, recall: 0.25\n",
      "14 油價應該凍漲或緩漲 ['油價', '應該', '凍漲', '緩漲']\n",
      "Total: 68, AP: 0.03, recall: 0.16\n",
      "15 臺灣應開放含瘦肉精(萊克多巴胺)之美國牛肉進口 ['臺灣', '應', '開放', '含', '瘦肉精', '萊克', '多巴胺', '美國', '牛肉', '進口']\n",
      "Total: 248, AP: 0.01, recall: 0.14\n",
      "16 贊同課綱微調 ['贊同', '課綱', '微調']\n",
      "Total: 300, AP: 0.12, recall: 0.25\n",
      "17 贊成文林苑都更案可依法拆除王家 ['贊成', '文林苑', '都更案', '依法', '拆除', '王家']\n",
      "Total: 65, AP: 0.12, recall: 0.48\n",
      "18 贊成流浪動物零撲殺 ['贊成', '流浪', '動物', '零', '撲殺']\n",
      "Total: 270, AP: 0.24, recall: 0.47\n",
      "19 遠雄大巨蛋工程應停工或拆除 ['遠', '雄大', '巨蛋', '工程', '應', '停工', '拆除']\n",
      "Total: 240, AP: 0.12, recall: 0.33\n",
      "MAP: 0.14, avg recall: 0.39\n"
     ]
    }
   ],
   "source": [
    "total_scores = []\n",
    "results = []\n",
    "total_recall = []\n",
    "\n",
    "for test_id , text_q in enumerate(train_query):\n",
    "\n",
    "    text = jieba.lcut(text_q)\n",
    "    text = [ t for t in text if t not in stop_words]\n",
    "    \n",
    "    scores = np.array(bm25Model.get_scores(text))\n",
    "    scores = histogram_equalization(scores)\n",
    "    \n",
    "    top_query = np.argsort(scores)[::-1][:5]\n",
    "    irrelevant_query = np.argsort(scores)[:5]\n",
    "    \n",
    "    print(test_id ,text_q, text)\n",
    "    \n",
    "    for query in top_query:\n",
    "        all_words = [ (sentence_arr[query][cnt], bm25Model.get_score(sentence_arr[query], cnt))\n",
    "                     for cnt, i in enumerate(sentence_arr[query])]\n",
    "\n",
    "        all_words = sorted(all_words,key=lambda x:(x[1]))[::-1]\n",
    "        top_words = all_words[:200]\n",
    "        top_words = [x[0] for x in top_words]\n",
    "        text += top_words\n",
    "        \n",
    "    expansion_score = np.array(bm25Model.get_scores(text))\n",
    "    expansion_score = histogram_equalization(expansion_score)\n",
    "    scores += 5 * expansion_score\n",
    "    sentiment_alpha = find_sentiment(text)\n",
    "        \n",
    "    if sentiment_alpha != 0:\n",
    "#         sentiment_matching = sentiment_alpha * sentiment_arr  # MAP = 0.05\n",
    "        sentiment_matching = sentiment_alpha * np.sign(sentiment_arr)  # MAP = 0.13\n",
    "        scores += histogram_equalization(sentiment_matching)   # 多了 .5 scaling : MAP = 0.14\n",
    "    \n",
    "    # scoring\n",
    "    top_num = 300\n",
    "    keys = pd.DataFrame(np.argsort(scores)[::-1][:top_num])\n",
    "    ans = keys[0].apply(lambda x: 'news_{:06d}'.format(x+1))\n",
    "    results.append(keys[0])\n",
    "    \n",
    "    count = 0\n",
    "    ap = 0\n",
    "    total = min([top_num, len(y_train[test_id])])\n",
    "    for i in range(top_num):\n",
    "        r = ans[i] in y_train[test_id]\n",
    "        count += r\n",
    "        p = count / (i+1)\n",
    "        ap += p * r\n",
    "    ap /= total\n",
    "    rc = count / total\n",
    "    print(\"Total: {}, AP: {:.2f}, recall: {:.2f}\".format(total, ap, rc))\n",
    "    total_scores += [ap]\n",
    "    total_recall += [rc]\n",
    "\n",
    "print('MAP: {:.2f}, avg recall: {:.2f}'.format(np.average(total_scores), np.average(total_recall)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 十二年國教高中職「免學費補助」適用對象增加是不對的 ['十二年', '國教', '高中', '職', '免', '學費', '補助', '適用', '對象', '增加', '不對']\n",
      "top query num 5\n",
      "Total: 93, AP: 0.17, recall: 0.61\n",
      "1 反對二代健保規定 ['反對', '二代', '健保', '規定']\n",
      "top query num 5\n",
      "Total: 232, AP: 0.33, recall: 0.59\n",
      "2 反對旺旺中時併購中嘉 ['反對', '旺旺', '中時', '併購', '中嘉']\n",
      "top query num 5\n",
      "Total: 109, AP: 0.18, recall: 0.64\n",
      "3 反對無圍牆校園 ['反對', '圍牆', '校園']\n",
      "top query num 5\n",
      "Total: 35, AP: 0.37, recall: 0.86\n",
      "4 另立專法保障同婚是正確的 ['另立', '專法', '保障', '同婚', '正確的']\n",
      "top query num 5\n",
      "Total: 135, AP: 0.12, recall: 0.45\n",
      "5 同意動物實驗 ['同意', '動物', '實驗']\n",
      "top query num 5\n",
      "Total: 20, AP: 0.20, recall: 1.00\n",
      "6 國際賽事會場內應該可以持中華民國國旗 ['國際', '賽事', '會場', '應該', '持', '中華民國', '國旗']\n",
      "top query num 5\n",
      "Total: 300, AP: 0.09, recall: 0.30\n",
      "7 堅決反對政府舉債發展前瞻建設計畫 ['堅決', '反對', '政府', '舉債', '發展', '前瞻建設', '計畫']\n",
      "top query num 5\n",
      "Total: 300, AP: 0.23, recall: 0.44\n",
      "8 年金改革應取消或應調降軍公教月退之優存利率十八趴 ['年金', '改革', '應', '取消', '或應', '調降', '軍公教', '月退', '優存', '利率', '十八趴']\n",
      "top query num 5\n",
      "Total: 200, AP: 0.05, recall: 0.27\n",
      "9 應該提高酒駕罰責以有效遏制酒駕 ['應該', '提高', '酒駕', '罰責以', '有效', '遏制', '酒駕']\n",
      "top query num 5\n",
      "Total: 300, AP: 0.47, recall: 0.67\n",
      "10 拒絕公投通過門檻下修 ['拒絕', '公投', '通過', '門檻', '下修']\n",
      "top query num 5\n",
      "Total: 52, AP: 0.05, recall: 0.52\n",
      "11 支持正名「臺灣」參與國際運動賽事 ['支持', '正名', '臺灣', '參與', '國際', '運動', '賽事']\n",
      "top query num 5\n",
      "Total: 110, AP: 0.14, recall: 0.60\n",
      "12 支持陳前總統保外就醫 ['支持', '陳', '前', '總統', '保外就醫']\n",
      "top query num 5\n",
      "Total: 200, AP: 0.06, recall: 0.30\n",
      "13 核四應該啟用 ['核四', '應該', '啟用']\n",
      "top query num 5\n",
      "Total: 205, AP: 0.10, recall: 0.29\n",
      "14 油價應該凍漲或緩漲 ['油價', '應該', '凍漲', '緩漲']\n",
      "top query num 5\n",
      "Total: 68, AP: 0.14, recall: 0.62\n",
      "15 臺灣應開放含瘦肉精(萊克多巴胺)之美國牛肉進口 ['臺灣', '應', '開放', '含', '瘦肉精', '萊克', '多巴胺', '美國', '牛肉', '進口']\n",
      "top query num 5\n",
      "Total: 248, AP: 0.02, recall: 0.18\n",
      "16 贊同課綱微調 ['贊同', '課綱', '微調']\n",
      "top query num 5\n",
      "Total: 300, AP: 0.12, recall: 0.28\n",
      "17 贊成文林苑都更案可依法拆除王家 ['贊成', '文林苑', '都更案', '依法', '拆除', '王家']\n",
      "top query num 5\n",
      "Total: 65, AP: 0.16, recall: 0.71\n",
      "18 贊成流浪動物零撲殺 ['贊成', '流浪', '動物', '零', '撲殺']\n",
      "top query num 5\n",
      "Total: 270, AP: 0.37, recall: 0.63\n",
      "19 遠雄大巨蛋工程應停工或拆除 ['遠', '雄大', '巨蛋', '工程', '應', '停工', '拆除']\n",
      "top query num 5\n",
      "Total: 240, AP: 0.18, recall: 0.44\n",
      "MAP: 0.18, avg recall: 0.52\n"
     ]
    }
   ],
   "source": [
    "# 原本的\n",
    "total_scores = []\n",
    "results = []\n",
    "total_recall = []\n",
    "\n",
    "for test_id , text_q in enumerate(train_query):\n",
    "    text = jieba.lcut(text_q)\n",
    "    text = [ t for t in text if t not in stop_words]\n",
    "    \n",
    "    scores = np.array(bm25Model.get_scores(text))\n",
    "    scores = histogram_equalization(scores)\n",
    "    \n",
    "    top_query = np.argsort(scores)[::-1][:5]\n",
    "    irrelevant_query = np.argsort(scores)[:5]\n",
    "    \n",
    "    print(test_id ,text_q, text)\n",
    "    \n",
    "    for query in top_query:\n",
    "        all_words = [ (sentence_arr[query][cnt], bm25Model.get_score(sentence_arr[query], cnt))\n",
    "                     for cnt, i in enumerate(sentence_arr[query])]\n",
    "\n",
    "        all_words = sorted(all_words,key=lambda x:(x[1]))[::-1]\n",
    "        top_words = all_words[:200]\n",
    "        top_words = [x[0] for x in top_words]\n",
    "        text += top_words\n",
    "        \n",
    "    expansion_score = np.array(bm25Model.get_scores(text))\n",
    "    expansion_score = histogram_equalization(expansion_score)\n",
    "    scores += 5 * expansion_score\n",
    "    \n",
    "    # scoring\n",
    "    top_num = 300\n",
    "    keys = pd.DataFrame(np.argsort(scores)[::-1][:top_num])\n",
    "    ans = keys[0].apply(lambda x: 'news_{:06d}'.format(x+1))\n",
    "    results.append(keys[0])\n",
    "    \n",
    "    count = 0\n",
    "    ap = 0\n",
    "    total = min([top_num, len(y_train[test_id])])\n",
    "    for i in range(top_num):\n",
    "        r = ans[i] in y_train[test_id]\n",
    "        count += r\n",
    "        p = count / (i+1)\n",
    "        ap += p * r\n",
    "    ap /= total\n",
    "    rc = count / total\n",
    "    print(\"Total: {}, AP: {:.2f}, recall: {:.2f}\".format(total, ap, rc))\n",
    "    total_scores += [ap]\n",
    "    total_recall += [rc]\n",
    "            \n",
    "print('MAP: {:.2f}, avg recall: {:.2f}'.format(np.average(total_scores), np.average(total_recall)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 十二年國教高中職「免學費補助」適用對象增加是不對的 ['十二年', '國教', '高中', '職', '免', '學費', '補助', '適用', '對象', '增加', '不對']\n",
      "top query num 50\n",
      "Total: 93, AP: 0.15, recall: 0.63\n",
      "1 反對二代健保規定 ['反對', '二代', '健保', '規定']\n",
      "top query num 50\n",
      "Total: 232, AP: 0.27, recall: 0.57\n",
      "2 反對旺旺中時併購中嘉 ['反對', '旺旺', '中時', '併購', '中嘉']\n",
      "top query num 50\n",
      "Total: 109, AP: 0.18, recall: 0.68\n",
      "3 反對無圍牆校園 ['反對', '圍牆', '校園']\n",
      "top query num 50\n",
      "Total: 35, AP: 0.41, recall: 0.89\n",
      "4 另立專法保障同婚是正確的 ['另立', '專法', '保障', '同婚', '正確的']\n",
      "top query num 50\n",
      "Total: 135, AP: 0.11, recall: 0.44\n",
      "5 同意動物實驗 ['同意', '動物', '實驗']\n",
      "top query num 50\n",
      "Total: 20, AP: 0.24, recall: 0.95\n",
      "6 國際賽事會場內應該可以持中華民國國旗 ['國際', '賽事', '會場', '應該', '持', '中華民國', '國旗']\n",
      "top query num 50\n",
      "Total: 300, AP: 0.09, recall: 0.31\n",
      "7 堅決反對政府舉債發展前瞻建設計畫 ['堅決', '反對', '政府', '舉債', '發展', '前瞻建設', '計畫']\n",
      "top query num 50\n",
      "Total: 300, AP: 0.21, recall: 0.42\n",
      "8 年金改革應取消或應調降軍公教月退之優存利率十八趴 ['年金', '改革', '應', '取消', '或應', '調降', '軍公教', '月退', '優存', '利率', '十八趴']\n",
      "top query num 50\n",
      "Total: 200, AP: 0.07, recall: 0.32\n",
      "9 應該提高酒駕罰責以有效遏制酒駕 ['應該', '提高', '酒駕', '罰責以', '有效', '遏制', '酒駕']\n",
      "top query num 50\n",
      "Total: 300, AP: 0.47, recall: 0.66\n",
      "10 拒絕公投通過門檻下修 ['拒絕', '公投', '通過', '門檻', '下修']\n",
      "top query num 50\n",
      "Total: 52, AP: 0.04, recall: 0.50\n",
      "11 支持正名「臺灣」參與國際運動賽事 ['支持', '正名', '臺灣', '參與', '國際', '運動', '賽事']\n",
      "top query num 50\n",
      "Total: 110, AP: 0.15, recall: 0.64\n",
      "12 支持陳前總統保外就醫 ['支持', '陳', '前', '總統', '保外就醫']\n",
      "top query num 50\n",
      "Total: 200, AP: 0.07, recall: 0.32\n",
      "13 核四應該啟用 ['核四', '應該', '啟用']\n",
      "top query num 50\n",
      "Total: 205, AP: 0.02, recall: 0.17\n",
      "14 油價應該凍漲或緩漲 ['油價', '應該', '凍漲', '緩漲']\n",
      "top query num 50\n",
      "Total: 68, AP: 0.13, recall: 0.59\n",
      "15 臺灣應開放含瘦肉精(萊克多巴胺)之美國牛肉進口 ['臺灣', '應', '開放', '含', '瘦肉精', '萊克', '多巴胺', '美國', '牛肉', '進口']\n",
      "top query num 50\n",
      "Total: 248, AP: 0.04, recall: 0.23\n",
      "16 贊同課綱微調 ['贊同', '課綱', '微調']\n",
      "top query num 50\n",
      "Total: 300, AP: 0.05, recall: 0.21\n",
      "17 贊成文林苑都更案可依法拆除王家 ['贊成', '文林苑', '都更案', '依法', '拆除', '王家']\n",
      "top query num 50\n",
      "Total: 65, AP: 0.27, recall: 0.72\n",
      "18 贊成流浪動物零撲殺 ['贊成', '流浪', '動物', '零', '撲殺']\n",
      "top query num 50\n",
      "Total: 270, AP: 0.36, recall: 0.61\n",
      "19 遠雄大巨蛋工程應停工或拆除 ['遠', '雄大', '巨蛋', '工程', '應', '停工', '拆除']\n",
      "top query num 50\n",
      "Total: 240, AP: 0.24, recall: 0.46\n",
      "MAP: 0.18, avg recall: 0.52\n"
     ]
    }
   ],
   "source": [
    "# 原本的\n",
    "total_scores = []\n",
    "results = []\n",
    "total_recall = []\n",
    "\n",
    "for test_id , text_q in enumerate(train_query):\n",
    "    text = jieba.lcut(text_q)\n",
    "    text = [ t for t in text if t not in stop_words]\n",
    "    \n",
    "    scores = np.array(bm25Model.get_scores(text))\n",
    "    scores = histogram_equalization(scores)\n",
    "    \n",
    "    top_query = np.argsort(scores)[::-1][:50]\n",
    "    \n",
    "    print(test_id ,text_q, text)\n",
    "    \n",
    "    for query in top_query:\n",
    "        all_words = [ (sentence_arr[query][cnt], bm25Model.get_score(sentence_arr[query], cnt))\n",
    "                     for cnt, i in enumerate(sentence_arr[query])]\n",
    "\n",
    "        all_words = sorted(all_words,key=lambda x:(x[1]))[::-1]\n",
    "        top_words = all_words[:200]\n",
    "        top_words = [x[0] for x in top_words]\n",
    "        text += top_words\n",
    "        \n",
    "    expansion_score = np.array(bm25Model.get_scores(text))\n",
    "    expansion_score = histogram_equalization(expansion_score)\n",
    "    scores += 5 * expansion_score\n",
    "    \n",
    "    # scoring\n",
    "    top_num = 300\n",
    "    keys = pd.DataFrame(np.argsort(scores)[::-1][:top_num])\n",
    "    ans = keys[0].apply(lambda x: 'news_{:06d}'.format(x+1))\n",
    "    results.append(keys[0])\n",
    "    \n",
    "    count = 0\n",
    "    ap = 0\n",
    "    total = min([top_num, len(y_train[test_id])])\n",
    "    for i in range(top_num):\n",
    "        r = ans[i] in y_train[test_id]\n",
    "        count += r\n",
    "        p = count / (i+1)\n",
    "        ap += p * r\n",
    "    ap /= total\n",
    "    rc = count / total\n",
    "    print(\"Total: {}, AP: {:.2f}, recall: {:.2f}\".format(total, ap, rc))\n",
    "    total_scores += [ap]\n",
    "    total_recall += [rc]\n",
    "            \n",
    "print('MAP: {:.2f}, avg recall: {:.2f}'.format(np.average(total_scores), np.average(total_recall)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 通姦在刑法上應該除罪化 ['通姦', '刑法', '應該', '除罪', '化'] -1\n",
      "top query num 80\n",
      "1 應該取消機車強制二段式左轉(待轉) ['應該', '取消', '機車', '強制', '二段式', '左轉', '待轉'] -3\n",
      "top query num 80\n",
      "2 支持博弈特區在台灣合法化 ['支持', '博弈', '特區', '台灣', '合法化'] 0\n",
      "top query num 80\n",
      "3 中華航空空服員罷工是合理的 ['中華', '航空', '空服員', '罷工', '合理的'] 0\n",
      "top query num 80\n"
     ]
    }
   ],
   "source": [
    "total_scores = list()\n",
    "for test_id , text_q in enumerate(test_query):\n",
    "    text = jieba.lcut(text_q)\n",
    "    text = [ t for t in text if t not in stop_words]\n",
    "    scores = bm25Model.get_scores(text)\n",
    "    top_query = np.argsort(scores)[::-1][:80]\n",
    "    \n",
    "    sentiment_alpha = find_sentiment(text)\n",
    "    \n",
    "    print(test_id ,text_q, text, sentiment_alpha)\n",
    "    print(\"top query num {}\".format(len(top_query)))\n",
    "    \n",
    "    # query expension\n",
    "    for query in top_query:\n",
    "        all_words = [ (sentence_arr[query][cnt], bm25Model.get_score(sentence_arr[query], cnt))\n",
    "                     for cnt, i in enumerate(sentence_arr[query])]\n",
    "\n",
    "        all_words = sorted(all_words,key=lambda x:(x[1]))[::-1]\n",
    "        top_words = all_words[:20]\n",
    "        top_words = [x[0] for x in top_words]\n",
    "        text += top_words\n",
    "    scores += 0.3 * np.array(bm25Model.get_scores(text))\n",
    "    \n",
    "    if sentiment_alpha != 0:\n",
    "        sentiment = sentiment_alpha * np.sign(sentiment_arr)\n",
    "        scores += sentiment / np.amax(np.abs(sentiment)) * np.amax(scores) * .5\n",
    "\n",
    "    # add train data\n",
    "    delta = np.max(scores) \n",
    "    if text_q in y_index.keys():\n",
    "        for idx, rel in (y_index[text_q][0]):\n",
    "            scores[idx] += delta * rel\n",
    "            \n",
    "    total_scores.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_num = 300\n",
    "search_result = np.zeros((20,top_num))\n",
    "for cnt,i in enumerate(total_scores):\n",
    "    keys = np.argsort(i)[::-1][:top_num]\n",
    "    search_result[cnt] += keys\n",
    "    \n",
    "search_result = search_result.astype(np.int)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Query_Index'] = ['q_{:02d}'.format(i+1) for i in range(20)]\n",
    "\n",
    "for i in range(top_num):\n",
    "    df['Rank_{:03d}'.format(i+1)] = search_result[:, i] + 1\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    df.iloc[i, 1:] = df.iloc[i, 1:].apply(lambda x: 'news_{:06d}'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = '0625-2.csv'\n",
    "df.to_csv('output/' + fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2019",
   "language": "python",
   "name": "ml2019"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
