{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.497 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from procDataSet import TrainingQuery\n",
    "\n",
    "\n",
    "top_num = 300\n",
    "\n",
    "jieba.load_userdict(os.path.join('data', 'dict.txt.big'))\n",
    "[jieba.add_word(i, freq=None, tag=None) for i in ['不支持','文林苑', '都更案','十八趴','證所稅','前瞻建設', '月退']]\n",
    "\n",
    "# add NTUSD\n",
    "positive_words = np.squeeze(pd.read_csv('data/NTUSD/ntusd-positive_zhtw.txt', header=None).values).tolist()\n",
    "negative_words = np.squeeze(pd.read_csv('data/NTUSD/ntusd-negative_zhtw.txt', header=None).values).tolist()\n",
    "for w in positive_words + negative_words:\n",
    "    jieba.add_word(w)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/stop_word_zhtw.txt') as file:\n",
    "    data = file.read()\n",
    "    \n",
    "stop_words = data.split('\\n')\n",
    "stop_words += ['「', '」', '，', '\\n', '）', '（', ')', '(']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'data/'\n",
    "raw_training_data = pd.read_csv(os.path.join(folder,'TD.csv'))\n",
    "news_urls = pd.read_csv(os.path.join(folder,'NC_1.csv'))\n",
    "contents = pd.read_json(os.path.join(folder,'url2content.json'), typ=pd.Series)\n",
    "test_query = np.array(pd.read_csv('./data/QS_1.csv').Query)\n",
    "\n",
    "## sort the contents by index\n",
    "keys, content_list = contents.keys(), contents.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add back some words\n",
    "stop_words = list(set(stop_words) - set(negative_words) - set(positive_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['通姦', '刑法', '應該', '除罪', '化']\n",
      "-1\n",
      "['應該', '取消', '機車', '強制', '二段式', '左轉', '待轉']\n",
      "-3\n",
      "['支持', '博弈', '特區', '台灣', '合法化']\n",
      "0\n",
      "['中華', '航空', '空服員', '罷工', '合理的']\n",
      "0\n",
      "['性交易', '應該', '合法化']\n",
      "0\n",
      "['ECFA', '早收', '清單', '達到', '預期', '成效']\n",
      "1\n",
      "['應該', '減免', '證所稅']\n",
      "0\n",
      "['贊成', '中油', '觀塘', '興建', '第三', '天然氣', '接收站']\n",
      "1\n",
      "['支持', '中國', '學生', '納入', '健保']\n",
      "0\n",
      "['支持', '臺灣', '中小學', '含', '高職', '專科', '服儀', '規定', '含', '髮', '襪', '鞋', '給予', '學生', '自主']\n",
      "2\n",
      "['不支持', '使用', '加密', '貨幣']\n",
      "0\n",
      "['不支持', '學雜費', '調漲']\n",
      "0\n",
      "['同意', '政府', '舉債', '發展', '前瞻建設', '計畫']\n",
      "2\n",
      "['支持', '電競', '列入', '體育競技']\n",
      "0\n",
      "['反對', '台鐵', '東移', '徵收', '案']\n",
      "-2\n",
      "['支持', '陳', '前', '總統', '保外就醫']\n",
      "0\n",
      "['年金', '改革', '應', '取消', '或應', '調降', '軍公教', '月退', '優存', '利率', '十八趴']\n",
      "0\n",
      "['同意', '動物', '實驗']\n",
      "1\n",
      "['油價', '應該', '凍漲', '緩漲']\n",
      "0\n",
      "['反對', '旺旺', '中時', '併購', '中嘉']\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "def find_sentiment(cuts):\n",
    "    p = n = 0\n",
    "    for c in cuts:\n",
    "        p += c in positive_words\n",
    "        n += c in negative_words\n",
    "    return p - n\n",
    "\n",
    "for q in test_query:\n",
    "    cuts = jieba.lcut(q)\n",
    "    cuts = [c for c in cuts if c not in stop_words]\n",
    "    print(cuts)\n",
    "    print(find_sentiment(cuts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "NumberCPU = multiprocessing.cpu_count()\n",
    "jieba.initialize()\n",
    "\n",
    "def jbcut(x):\n",
    "    if x is not None:\n",
    "        sen = jieba.lcut(x, cut_all=False)\n",
    "        sen = [i for i in sen if i not in stop_words]\n",
    "        return sen\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def psegcut(x):\n",
    "    if x is not None:\n",
    "        sen = pseg.lcut(x)\n",
    "        sen = [i for i in sen if i not in stop_words]\n",
    "        return sen\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "pool = multiprocessing.Pool(processes=NumberCPU)\n",
    "sentence_arr = pool.map(jbcut,content_list)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_arr = np.load('data/sentence_arr.npy')\n",
    "# np.save('data/sentence_arr', sentence_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add NTUSD scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要跑超久\n",
    "pool = multiprocessing.Pool(processes=NumberCPU)\n",
    "sentiment_arr = pool.map(find_sentiment, sentence_arr)\n",
    "pool.close()\n",
    "pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('data/sentiment_arr', sentiment_arr)\n",
    "sentiment_arr = np.load('data/sentiment_arr.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## okapi model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import bm25\n",
    "bm25Model = bm25.BM25(sentence_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "十二年國教高中職「免學費補助」適用對象增加是不對的 93\n",
      "反對二代健保規定 232\n",
      "反對旺旺中時併購中嘉 109\n",
      "反對無圍牆校園 35\n",
      "另立專法保障同婚是正確的 135\n",
      "同意動物實驗 20\n",
      "國際賽事會場內應該可以持中華民國國旗 766\n",
      "堅決反對政府舉債發展前瞻建設計畫 388\n",
      "年金改革應取消或應調降軍公教月退之優存利率十八趴 200\n",
      "應該提高酒駕罰責以有效遏制酒駕 441\n",
      "拒絕公投通過門檻下修 52\n",
      "支持正名「臺灣」參與國際運動賽事 110\n",
      "支持陳前總統保外就醫 200\n",
      "核四應該啟用 205\n",
      "油價應該凍漲或緩漲 68\n",
      "臺灣應開放含瘦肉精(萊克多巴胺)之美國牛肉進口 248\n",
      "贊同課綱微調 310\n",
      "贊成文林苑都更案可依法拆除王家 65\n",
      "贊成流浪動物零撲殺 270\n",
      "遠雄大巨蛋工程應停工或拆除 240\n"
     ]
    }
   ],
   "source": [
    "train_query = sorted(set(raw_training_data.Query))\n",
    "\n",
    "y_train = []\n",
    "y_index = {}\n",
    "for i  in train_query:\n",
    "    index = np.where(raw_training_data.Query==i)\n",
    "    data = raw_training_data.iloc[index]\n",
    "\n",
    "    # sort by relvanance\n",
    "    y = np.squeeze(data[data.Relevance!=0].sort_values('Relevance', ascending=False).News_Index.values).tolist()\n",
    "    y_idx = [ (int(idx.split('_')[1])-1, rel )  for idx, rel in zip(data.News_Index, data.Relevance)]\n",
    "    y_train.append(y)\n",
    "    y_index[i] = [y_idx]\n",
    "    \n",
    "    print(i, len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 十二年國教高中職「免學費補助」適用對象增加是不對的 ['十二年', '國教', '高中', '職', '免', '學費', '補助', '適用', '對象', '增加', '不對']\n",
      "0.61\n",
      "1 反對二代健保規定 ['反對', '二代', '健保', '規定']\n",
      "0.56\n",
      "2 反對旺旺中時併購中嘉 ['反對', '旺旺', '中時', '併購', '中嘉']\n",
      "0.64\n",
      "3 反對無圍牆校園 ['反對', '圍牆', '校園']\n",
      "0.83\n",
      "4 另立專法保障同婚是正確的 ['另立', '專法', '保障', '同婚', '正確的']\n",
      "0.49\n",
      "5 同意動物實驗 ['同意', '動物', '實驗']\n",
      "1.00\n",
      "6 國際賽事會場內應該可以持中華民國國旗 ['國際', '賽事', '會場', '應該', '持', '中華民國', '國旗']\n",
      "0.33\n",
      "7 堅決反對政府舉債發展前瞻建設計畫 ['堅決', '反對', '政府', '舉債', '發展', '前瞻建設', '計畫']\n",
      "0.42\n",
      "8 年金改革應取消或應調降軍公教月退之優存利率十八趴 ['年金', '改革', '應', '取消', '或應', '調降', '軍公教', '月退', '優存', '利率', '十八趴']\n",
      "0.33\n",
      "9 應該提高酒駕罰責以有效遏制酒駕 ['應該', '提高', '酒駕', '罰責以', '有效', '遏制', '酒駕']\n",
      "0.65\n",
      "10 拒絕公投通過門檻下修 ['拒絕', '公投', '通過', '門檻', '下修']\n",
      "0.50\n",
      "11 支持正名「臺灣」參與國際運動賽事 ['支持', '正名', '臺灣', '參與', '國際', '運動', '賽事']\n",
      "0.61\n",
      "12 支持陳前總統保外就醫 ['支持', '陳', '前', '總統', '保外就醫']\n",
      "0.30\n",
      "13 核四應該啟用 ['核四', '應該', '啟用']\n",
      "0.18\n",
      "14 油價應該凍漲或緩漲 ['油價', '應該', '凍漲', '緩漲']\n",
      "0.54\n",
      "15 臺灣應開放含瘦肉精(萊克多巴胺)之美國牛肉進口 ['臺灣', '應', '開放', '含', '瘦肉精', '萊克', '多巴胺', '美國', '牛肉', '進口']\n",
      "0.24\n",
      "16 贊同課綱微調 ['贊同', '課綱', '微調']\n",
      "0.22\n",
      "17 贊成文林苑都更案可依法拆除王家 ['贊成', '文林苑', '都更案', '依法', '拆除', '王家']\n",
      "0.75\n",
      "18 贊成流浪動物零撲殺 ['贊成', '流浪', '動物', '零', '撲殺']\n",
      "0.66\n",
      "19 遠雄大巨蛋工程應停工或拆除 ['遠', '雄大', '巨蛋', '工程', '應', '停工', '拆除']\n",
      "0.45\n",
      "MAP: 0.52\n"
     ]
    }
   ],
   "source": [
    "total_scores = list()\n",
    "\n",
    "for test_id , text_q in enumerate(train_query):\n",
    "    text = jieba.lcut(text_q)\n",
    "    text = [ t for t in text if t not in stop_words]\n",
    "    scores = np.array(bm25Model.get_scores(text))\n",
    "    scores = scores / np.amax(scores)\n",
    "    top_query = np.argsort(scores)[::-1][:50]\n",
    "      \n",
    "    print(test_id ,text_q, text)\n",
    "    \n",
    "    # query expension\n",
    "    for query in top_query:\n",
    "        all_words = [ (sentence_arr[query][cnt], bm25Model.get_score(sentence_arr[query], cnt))\n",
    "                     for cnt, i in enumerate(sentence_arr[query])]\n",
    "\n",
    "        all_words = sorted(all_words,key=lambda x:(x[1]))[::-1]\n",
    "        top_words = all_words[:20]\n",
    "        top_words = [x[0] for x in top_words]\n",
    "        text += top_words\n",
    "\n",
    "    expended_score = np.array(bm25Model.get_scores(text))\n",
    "    scores += expended_score / np.amax(expended_score)\n",
    "    \n",
    "    # scoring\n",
    "    top_num = 300\n",
    "    keys = pd.DataFrame(np.argsort(scores)[::-1][:top_num])\n",
    "    ans = keys[0].apply(lambda x: 'news_{:06d}'.format(x+1))\n",
    "\n",
    "    ap = sum([1 if a in y_train[test_id] else 0 for a in ans]) / min([300, len(y_train[test_id])])\n",
    "    print(\"{:.2f}\".format(ap))\n",
    "    total_scores += [ap]\n",
    "\n",
    "print('MAP: {:.2f}'.format(np.average(total_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 十二年國教高中職「免學費補助」適用對象增加是不對的 ['十二年', '國教', '高中', '職', '免', '學費', '補助', '適用', '對象', '增加', '不對'] 0\n",
      "0.61\n",
      "1 反對二代健保規定 ['反對', '二代', '健保', '規定'] -1\n",
      "0.60\n",
      "2 反對旺旺中時併購中嘉 ['反對', '旺旺', '中時', '併購', '中嘉'] -1\n",
      "0.60\n",
      "3 反對無圍牆校園 ['反對', '圍牆', '校園'] -1\n",
      "0.43\n",
      "4 另立專法保障同婚是正確的 ['另立', '專法', '保障', '同婚', '正確的'] 1\n",
      "0.44\n",
      "5 同意動物實驗 ['同意', '動物', '實驗'] 1\n",
      "0.85\n",
      "6 國際賽事會場內應該可以持中華民國國旗 ['國際', '賽事', '會場', '應該', '持', '中華民國', '國旗'] 0\n",
      "0.33\n",
      "7 堅決反對政府舉債發展前瞻建設計畫 ['堅決', '反對', '政府', '舉債', '發展', '前瞻建設', '計畫'] 1\n",
      "0.32\n",
      "8 年金改革應取消或應調降軍公教月退之優存利率十八趴 ['年金', '改革', '應', '取消', '或應', '調降', '軍公教', '月退', '優存', '利率', '十八趴'] 0\n",
      "0.33\n",
      "9 應該提高酒駕罰責以有效遏制酒駕 ['應該', '提高', '酒駕', '罰責以', '有效', '遏制', '酒駕'] 1\n",
      "0.64\n",
      "10 拒絕公投通過門檻下修 ['拒絕', '公投', '通過', '門檻', '下修'] 0\n",
      "0.50\n",
      "11 支持正名「臺灣」參與國際運動賽事 ['支持', '正名', '臺灣', '參與', '國際', '運動', '賽事'] 0\n",
      "0.61\n",
      "12 支持陳前總統保外就醫 ['支持', '陳', '前', '總統', '保外就醫'] 0\n",
      "0.30\n",
      "13 核四應該啟用 ['核四', '應該', '啟用'] 0\n",
      "0.18\n",
      "14 油價應該凍漲或緩漲 ['油價', '應該', '凍漲', '緩漲'] 0\n",
      "0.54\n",
      "15 臺灣應開放含瘦肉精(萊克多巴胺)之美國牛肉進口 ['臺灣', '應', '開放', '含', '瘦肉精', '萊克', '多巴胺', '美國', '牛肉', '進口'] 1\n",
      "0.26\n",
      "16 贊同課綱微調 ['贊同', '課綱', '微調'] 1\n",
      "0.18\n",
      "17 贊成文林苑都更案可依法拆除王家 ['贊成', '文林苑', '都更案', '依法', '拆除', '王家'] 0\n",
      "0.75\n",
      "18 贊成流浪動物零撲殺 ['贊成', '流浪', '動物', '零', '撲殺'] 0\n",
      "0.66\n",
      "19 遠雄大巨蛋工程應停工或拆除 ['遠', '雄大', '巨蛋', '工程', '應', '停工', '拆除'] -1\n",
      "0.34\n",
      "MAP: 0.47\n"
     ]
    }
   ],
   "source": [
    "total_scores = list()\n",
    "\n",
    "for test_id , text_q in enumerate(train_query):\n",
    "\n",
    "    text = jieba.lcut(text_q)\n",
    "    text = [ t for t in text if t not in stop_words]\n",
    "    scores = np.array(bm25Model.get_scores(text))\n",
    "    scores = scores / np.amax(scores)\n",
    "    top_query = np.argsort(scores)[::-1][:50]\n",
    "    \n",
    "    sentiment_alpha = find_sentiment(text)\n",
    "    \n",
    "    print(test_id ,text_q, text, sentiment_alpha)\n",
    "    \n",
    "    # query expension\n",
    "    for query in top_query:\n",
    "        all_words = [ (sentence_arr[query][cnt], bm25Model.get_score(sentence_arr[query], cnt))\n",
    "                     for cnt, i in enumerate(sentence_arr[query])]\n",
    "\n",
    "        all_words = sorted(all_words,key=lambda x:(x[1]))[::-1]\n",
    "        top_words = all_words[:20]\n",
    "        top_words = [x[0] for x in top_words]\n",
    "        text += top_words\n",
    "\n",
    "    expended_score = np.array(bm25Model.get_scores(text))\n",
    "    scores += expended_score / np.amax(expended_score)\n",
    "        \n",
    "    if sentiment_alpha != 0:\n",
    "        sentiment = sentiment_alpha * np.sign(sentiment_arr)\n",
    "        scores += sentiment / np.amax(np.abs(sentiment)) * .7\n",
    "    \n",
    "    # scoring\n",
    "    top_num = 300\n",
    "    keys = pd.DataFrame(np.argsort(scores)[::-1][:top_num])\n",
    "    ans = keys[0].apply(lambda x: 'news_{:06d}'.format(x+1))\n",
    "\n",
    "    ap = sum([1 if a in y_train[test_id] else 0 for a in ans]) / min([300, len(y_train[test_id])])\n",
    "    print(\"{:.2f}\".format(ap))\n",
    "    total_scores += [ap]\n",
    "\n",
    "print('MAP: {:.2f}'.format(np.average(total_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 通姦在刑法上應該除罪化 ['通姦', '刑法', '應該', '除罪', '化'] -1\n",
      "top query num 80\n",
      "1 應該取消機車強制二段式左轉(待轉) ['應該', '取消', '機車', '強制', '二段式', '左轉', '待轉'] -3\n",
      "top query num 80\n",
      "2 支持博弈特區在台灣合法化 ['支持', '博弈', '特區', '台灣', '合法化'] 0\n",
      "top query num 80\n",
      "3 中華航空空服員罷工是合理的 ['中華', '航空', '空服員', '罷工', '合理的'] 0\n",
      "top query num 80\n"
     ]
    }
   ],
   "source": [
    "total_scores = list()\n",
    "for test_id , text_q in enumerate(test_query):\n",
    "    text = jieba.lcut(text_q)\n",
    "    text = [ t for t in text if t not in stop_words]\n",
    "    scores = bm25Model.get_scores(text)\n",
    "    top_query = np.argsort(scores)[::-1][:80]\n",
    "    \n",
    "    sentiment_alpha = find_sentiment(text)\n",
    "    \n",
    "    print(test_id ,text_q, text, sentiment_alpha)\n",
    "    print(\"top query num {}\".format(len(top_query)))\n",
    "    \n",
    "    # query expension\n",
    "    for query in top_query:\n",
    "        all_words = [ (sentence_arr[query][cnt], bm25Model.get_score(sentence_arr[query], cnt))\n",
    "                     for cnt, i in enumerate(sentence_arr[query])]\n",
    "\n",
    "        all_words = sorted(all_words,key=lambda x:(x[1]))[::-1]\n",
    "        top_words = all_words[:20]\n",
    "        top_words = [x[0] for x in top_words]\n",
    "        text += top_words\n",
    "    scores += 0.3 * np.array(bm25Model.get_scores(text))\n",
    "    \n",
    "#     if sentiment_alpha != 0:\n",
    "#         sentiment = sentiment_alpha * np.sign(sentiment_arr)\n",
    "#         scores += sentiment / np.amax(np.abs(sentiment)) * np.amax(scores) * .5\n",
    "\n",
    "    # add train data\n",
    "    delta = np.max(scores) \n",
    "    if text_q in y_index.keys():\n",
    "        for idx, rel in (y_index[text_q][0]):\n",
    "            scores[idx] += delta * idx\n",
    "            \n",
    "    total_scores.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_num = 300\n",
    "search_result = np.zeros((20,top_num))\n",
    "for cnt,i in enumerate(total_scores):\n",
    "    keys = np.argsort(i)[::-1][:top_num]\n",
    "    search_result[cnt] += keys\n",
    "    \n",
    "search_result = search_result.astype(np.int)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Query_Index'] = ['q_{:02d}'.format(i+1) for i in range(20)]\n",
    "\n",
    "for i in range(top_num):\n",
    "    df['Rank_{:03d}'.format(i+1)] = search_result[:, i] + 1\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    df.iloc[i, 1:] = df.iloc[i, 1:].apply(lambda x: 'news_{:06d}'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = '0625-2.csv'\n",
    "df.to_csv('output/' + fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2019",
   "language": "python",
   "name": "ml2019"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
